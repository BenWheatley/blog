<h1>How would you know whether an A.I. was a person or not?</h1>
<p>I did an A-level in Philosophy. (For non UK people, A-levels are a 2-year course that happens after highschool and before university).</p>

<p>I did it for fun rather than good grades — I had enough good grades to get into university, and when the other A-levels required my focus, I was fine putting zero further effort into the Philosophy course. (Something which was very clear when my final results came in).</p>

<p>What I didn't expect at the time was that the rapid development of artificial intelligence in my lifetime would make it <em>absolutely vital</em> that humanity develops a concrete and testable understanding of what counts as a mind, as consciousness, as self-awareness, and as capability to suffer. Yes, we already have that as a problem in the form of animal suffering and whether meat can ever be ethical, but the problem which already exists, exists only for our consciences — the animals can't take over the world and treat us the way we treat them, but an artificial mind would be almost totally pointless if it was as limited as an animal, and the general aim is quite a lot higher than that.</p>

<p>Some fear that we will replace ourselves with machines which may be very effective at what they do, but don't have anything "that it's like to be". One of my fears is that we'll make machines that do "have something that it's like to be", but who suffer greatly because humanity fails to recognise their personhood. (A paperclip optimiser doesn't need to hate us to kill us, but I'm more interested in the sort of mind that can feel what we can feel).</p>

<p>I don't have a good description of what I mean by any of the normal words. Personhood, consciousness, self awareness, suffering… they all seem to skirt around the core idea, but to the extent that they're correct, they're not clearly testable; and to the extent that they're testable, they're not clearly correct. A little like the maths-vs.-physics dichotomy.</p>

<strong>Consciousness?</strong> Versus what, subconscious decision making? Isn't this distinction merely <a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow" target="_blank">system 1 vs. system 2 thinking</a>? Even then, the word doesn't tell us what it means to have it objectively, only subjectively. In some ways, some forms of A.I. looks like system 1 — fast, but error prone, based on heuristics; while other forms of A.I. look like system 2 — slow, careful, deliberative weighing all the options.

<strong>Self-awareness?</strong> What do we even mean by that? It's absolutely trivial to make an A.I. aware of it's own internal states, even necessary for anything more than a perceptron. Do we mean a mirror test? (Or non-visual equivalent for non-visual entities, including both blind people and also smell-focused animals such as dogs). That at least can be tested.

<strong>Capability to suffer?</strong> What does that even mean in an objective sense? Is suffering equal to negative reinforcement? If you have only positive reinforcement, is the absence of reward itself a form of suffering?

<strong>Introspection?</strong> As I understand it, the human psychology of this is that we don't really introspect, we use system 2 thinking to confabulate justifications for what system 1 thinking made us feel.

<strong>Qualia?</strong> Sure, but what is one of these as an objective, measurable, detectable state within a neural network, be it artificial or natural?

<strong>Empathy or mirror neurons?</strong> I can't decide how I feel about this one. At first glance, if one mind can feel the same as another mind, that seems like it should have the general ill-defined concept I'm after… but then I realised, I don't see <em>why</em> that would follow and had the temporarily disturbing mental concept of <em>an A.I. which can perfectly mimic the behaviour corresponding to the emotional state of someone they're observing, without actually feeling anything itself</em>.

<p>And then the disturbance went away as I realised this is obviously trivially possible, because even a video recording fits that definition… or, hey, a mirror. A video recording somehow feels like it's fine, it isn't "smart" enough to be imitating, merely accurately reproducing. (Now I think about it, is there an equivalent issue with the mirror test?)</p>

<p>So, no, mirror neurons are not enough to be… to have the qualia of being consciously aware, or whatever you want to call it.</p>

<p>I'm still not closer to having answers, but sometimes it's good to write down the questions.</p>

<p><a href="https://kitsunesoftware.wordpress.com/2018/06/17/how-would-you-know-whether-an-a-i-was-a-person-or-not/">Original post: https://kitsunesoftware.wordpress.com/2018/06/17/how-would-you-know-whether-an-a-i-was-a-person-or-not/</a></p>

<p>Original post timestamp: Sun, 17 Jun 2018 23:09:28 +0000</a></p>

<p>Tags: <a href='https://benwheatley.github.io/blog/tags/AI'>AI</a>, <a href='https://benwheatley.github.io/blog/tags/consciousness'>consciousness</a>, <a href='https://benwheatley.github.io/blog/tags/theory of mind'>theory of mind</a></p>

<p>Categories: <a href='https://benwheatley.github.io/blog/categories/Futurology'>Futurology</a>, <a href='https://benwheatley.github.io/blog/categories/Philosophy'>Philosophy</a>, <a href='https://benwheatley.github.io/blog/categories/Psychology'>Psychology</a>, <a href='https://benwheatley.github.io/blog/categories/Science'>Science</a></p>
