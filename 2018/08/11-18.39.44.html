<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="../../css.css"/ ></head><body>

<div class="content"><h1>A.I. safety with Democracy?</h1>
<p>Common path of discussion:</p>
<p style="font-family:sans-serif;"><strong>Alice:</strong> A.I. can already be dangerous, even though it's currently narrow intelligence only. How do we make it safe before it's general intelligence?</p>
<p style="font-family:sans-serif;"><strong>Bob:</strong> <a href="https://rationalwiki.org/wiki/Thought-terminating_clich%C3%A9" target="_blank" rel="noopener">Democracy</a>!</p>
<p style="font-family:sans-serif;"><strong>Alice:</strong> That's a sentence fragment, not an answer. What do you mean?</p>
<p style="font-family:sans-serif;"><strong>Bob:</strong> Vote for what you want the A.I. to do :)</p>
<p style="font-family:sans-serif;"><strong>Alice:</strong> But people ask for what they think they want instead of what they really want — this leads to <a href="https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/" target="_blank" rel="noopener">misaligned incentives</a>/<a href="https://wiki.lesswrong.com/wiki/Paperclip_maximizer" target="_blank" rel="noopener">paperclip optimisers</a>, or pathological focus on universal instrumental goals like <a href="https://en.wikipedia.org/wiki/Hyperinflation" target="_blank" rel="noopener">money</a> or <a href="https://en.wikipedia.org/wiki/Dictatorship" target="_blank" rel="noopener">power</a>.</p>
<p style="font-family:sans-serif;"><strong>Bob:</strong> <a href="https://en.wikipedia.org/w/index.php?title=OpenAI&amp;oldid=853340659#Strategy" target="_blank" rel="noopener">Then let's give the A.I. to everyone</a>, so we're all equal and anyone who tells their A.I. to do something daft can be countered by everyone else.</p>
<p style="font-family:sans-serif;"><strong>Alice:</strong> But that assumes the machines operate on the same speed we do. If we assume that an <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank" rel="noopener">A.G.I.</a> can be made by duplicating a human brain's connectome in silicon — mapping synapses to transistors — then <em>even with no more Moore's Law</em> <a href="https://kitsunesoftware.wordpress.com/2017/11/26/you-wont-believe-how-fast-transistors-are/" target="_blank" rel="noopener">an A.G.I. would be out-pacing our thoughts by the same margin a pack of wolves outpaces continental drift</a> (and the volume of <a href="http://www.wolframalpha.com/input/?i=human+brain+volume+%2F+(1%C2%B5%2F11nm)%5E3" target="_blank" rel="noopener">a few dozen grains of sand</a>).</p>
<p style="font-family:sans-serif;"><em>Because</em> we're much too slow to respond to threats ourselves, any helpful A.G.I. working to stop a harmful A.G.I. would have to know what to do before we told it; yet if we knew how to make them work like that, then we wouldn't need to, as all A.G.I. would stop themselves from doing anything harmful in the first place.</p>
<p style="font-family:sans-serif;"><strong>Bob:</strong> <a href="http://lawsandsausagescomic.com/comic/202" target="_blank" rel="noopener">Balance of powers, just like governments</a> — no single A.G.I can get too big, because all the other A.G.I. want the same limited resource.</p>
<p style="font-family:sans-serif;"><strong>Alice:</strong> Keep reading that educational webcomic. Even in the human case (and we can't trust our intuition about the nature of an arbitrary A.G.I.), separation of powers only works if you can <em>guarantee</em> that those who seek power don't collude. As humans collude, an A.G.I. (even one which seeks power only as an instrumental goal for some other cause) can be expected to collude with other similar A.G.I. ("A.G.I.s"? How do you pluralise an initialism?)</p>


<hr />

<p>There's probably something that <em>should</em> follow this, but I don't know what as real conversations usually go stale well before my final Alice response (and even that might have been too harsh and conversation-stopping, I'd like to dig deeper and find out what happens next).</p>

<p>I still think we ultimately want "<a href="https://kitsunesoftware.wordpress.com/2017/08/09/would-this-be-a-solution-to-the-problem-of-literal-genie-omniscient-ais/" target="_blank" rel="noopener">do what I meant not what I said</a>", but at the very least that's really hard to specify and at worst I'm starting to worry that some (too many?) people may be unable to cope with the possibility that some of the things they want are incoherent or self-contradictory.</p>

<p>Whatever the solution, I suspect that politics and economics both have a lot of lessons available to help the development of safe A.I. — both limited A.I. that currently exists and also potential future tech such as human-level general A.I. (perhaps even super-intelligence, but don't count on that).</p>

<p><a href="https://kitsunesoftware.wordpress.com/2018/08/11/a-i-safety-with-democracy/">Original post: https://kitsunesoftware.wordpress.com/2018/08/11/a-i-safety-with-democracy/</a></p>

<p>Original post timestamp: Sat, 11 Aug 2018 18:39:44 +0000</a></p>

<p>Tags: <a href='https://benwheatley.github.io/blog/tags/AGI'>AGI</a>, <a href='https://benwheatley.github.io/blog/tags/AI'>AI</a>, <a href='https://benwheatley.github.io/blog/tags/Artificial intelligence'>Artificial intelligence</a>, <a href='https://benwheatley.github.io/blog/tags/Democracy'>Democracy</a>, <a href='https://benwheatley.github.io/blog/tags/misaligned incentives'>misaligned incentives</a>, <a href='https://benwheatley.github.io/blog/tags/paperclip optimiser'>paperclip optimiser</a></p>

<p>Categories: <a href='https://benwheatley.github.io/blog/categories/Minds'>Minds</a>, <a href='https://benwheatley.github.io/blog/categories/Philosophy'>Philosophy</a>, <a href='https://benwheatley.github.io/blog/categories/Politics'>Politics</a></p>
</div>

</body>
</html>
