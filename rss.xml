<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Ben Wheatley's Blog</title>
    <link>https://benwheatley.github.io/blog/</link>
    <description>A blog about technology, AI, and various other topics</description>
    <language>en-us</language>
    <lastBuildDate>Sun, 22 Jun 2025 13:21:36 +0000</lastBuildDate>
    <item>
      <title>"It isn't magic"</title>
      <link>https://benwheatley.github.io/blog/2025/06/22-13.21.36.html</link>
      <description>&lt;h1&gt;"It isn't magic"&lt;/h1&gt;

&lt;p&gt;
People keep saying "AI isn't magic, it's just maths" like this is some kind of gotcha.
&lt;/p&gt;

&lt;img src="it-is-not-magic.jpg" alt="Triptych in style of Hieronymus Bosch's 'The Garden of Earthly Delights', the left showing a wizard raining fireballs down upon a medieval army, the right showing a Predator drone firing a missile while being remotely operated. Between them are geometric shapes representing magical sigils from the Key of Solomon contrasted with circuit boards" /&gt;

&lt;p&gt;
Turning lead into gold isn't the magic of alchemy, it's just &lt;a href="https://en.wikipedia.org/wiki/Nucleosynthesis"&gt;nucleosynthesis&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Taking a living human's heart out without killing them, and replacing it with one you got out a corpse, that isn't the magic of necromancy, neither is it a prayer or ritual to &lt;a href="https://en.wikipedia.org/wiki/Sekhmet"&gt;Sekhmet&lt;/a&gt;, it's just transplant surgery.
&lt;/p&gt;

&lt;p&gt;
Casually chatting with someone while they're 8,000 kilometres is not done with magic crystal balls, it's just telephony.
&lt;/p&gt;

&lt;p&gt;
Analysing the atmosphere of &lt;a href="https://en.wikipedia.org/wiki/WASP-19b"&gt;a planet 869 light-years away&lt;/a&gt; (&lt;a href="https://www.wolframalpha.com/input?i=869+light-years+in+km"&gt;about 8 quadrillion km&lt;/a&gt;) is not supernatural &lt;a href="https://en.wikipedia.org/wiki/Remote_viewing"&gt;remote viewing&lt;/a&gt;, it's just spectral analysis through a telescope… a telescope that remains &lt;a href="https://en.wikipedia.org/wiki/Hubble_Space_Telescope"&gt;about 540 km above the ground, even without any support from anything underneath&lt;/a&gt;, which also isn't magic, it's just &lt;a href="https://en.wikipedia.org/wiki/Low_Earth_orbit"&gt;"orbit"&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Making diamonds and rubies by the tonne isn't a wish made to a magic djinn, it's &lt;a href="https://en.wikipedia.org/wiki/Synthetic_diamond"&gt;just&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/Verneuil_method"&gt;chemistry&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Flying through the air, or to the moon, isn't a magic carpet, it's just aerodynamics and rocket science respectively.
&lt;/p&gt;

&lt;p&gt;
&lt;a href="https://www.nature.com/articles/s41593-023-01304-9"&gt;Reading someone’s thoughts isn't magic telepathy, it's just fMRI decoding&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Knowing your location anywhere on the surface of planet, at any time of day or night, in any weather, isn't magic, it's just GPS.
&lt;/p&gt;

&lt;p&gt;
Forecasting the weather days in advance isn't the magic of divination, it isn't precognition, it isn't limited to the visions of prophets and oracles, it's just fluid dynamics simulations and &lt;a href="https://en.wikipedia.org/wiki/Monte_Carlo_method"&gt;Monte Carlo methods&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;A bracelet that can detect when you fall and summon help automatically isn't a guardian angel, it's just an &lt;a href="https://web.archive.org/web/20250125200903/https://support.apple.com/en-us/108896"&gt;Apple Watch with Fall Detection&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Sensing through rock to find hidden water isn't dowsing or water divining, it's just geotechnical survey tools such as &lt;a href="https://en.wikipedia.org/wiki/Ground-penetrating_radar"&gt;ground penetrating radar&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Electrical_resistance_survey"&gt;electrical resistance surveys&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Seeing someone's bones without flaying the flesh from them isn't magic, it's just an x-ray.
&lt;/p&gt;

&lt;p&gt;
Curing congenital deafness, letting the blind see, letting the lame walk, none of that is magic or miracle, they're just &lt;a href="https://en.wikipedia.org/wiki/Cochlear_implant"&gt;cochlear implants&lt;/a&gt;, cataract removal/&lt;a href="https://en.wikipedia.org/wiki/Retinal_implant"&gt;retinal implants&lt;/a&gt;, and surgery or &lt;a href="https://en.wikipedia.org/wiki/Exoskeleton_(human)"&gt;prosthetic exoskeletons&lt;/a&gt; respectively.
&lt;/p&gt;

&lt;p&gt;
Condensing sunlight in the daytime, in order to heat and illuminate your home after dark, that isn't magic, it's just photovoltaics and batteries.
&lt;/p&gt;

&lt;p&gt;
&lt;a href="https://nuclearsecrecy.com/nukemap/?&amp;kt=100000&amp;lat=37.5257007&amp;lng=23.0970342&amp;hob_psi=5&amp;hob_ft=47553&amp;ff=52&amp;psi=20,5,1&amp;therm=_3rd-100,_2nd-50,_1st-50,35&amp;zm=8"&gt;A single weapon that can, in the blink of an eye, burn an area large enough to encompass both ancient Athens and ancient Sparta at the same time, that's not magic, it's just thermonuclear fusion&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Cooking without visible flame isn't magic, it's just an electric hob. Doing it without even a hot work surface for the pan still isn't magic, it's just a microwave oven or an induction hob.
&lt;/p&gt;

&lt;p&gt;
Curing leprosy is neither biblical miracle nor magic, it's just &lt;a href="https://en.wikipedia.org/wiki/Rifampicin"&gt;rifampicin&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Dapsone"&gt;dapsone&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Clofazimine"&gt;clofazimine&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Immunity to smallpox isn't a prayer to &lt;a href="https://en.wikipedia.org/wiki/Shitala"&gt;the Hindu goddess Shitala&lt;/a&gt; (of many things but most directly linked with smallpox), and it isn't magic herbs or crystals, it's just vaccines.
&lt;/p&gt;

&lt;p&gt;
The end of famine in the industrialised world wasn't magic, it was just &lt;a href="https://en.wikipedia.org/wiki/Mechanised_agriculture"&gt;mechanised farming&lt;/a&gt;, synthetic fertilisers, pesticides, and &lt;a href="https://en.wikipedia.org/wiki/Common_Agricultural_Policy"&gt;systematic government policy&lt;/a&gt; to get farmers to &lt;a href="https://en.wikipedia.org/wiki/Butter_mountain"&gt;over&lt;/a&gt;-&lt;a href="https://en.wikipedia.org/wiki/Wine_lake"&gt;produce&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;Even stage magic isn't magic, it's just a lot of practice, sleight of hand, etc. — as the stage magicians Penn and Teller (well, just Penn) said, stage magic is about making something very hard look easy, so much so that your audience simply doesn't even imagine the real effort you put into it.&lt;/p&gt;

&lt;p&gt;So sure, AI isn't magic in the "supernatural" sense, but as Clarke said &lt;q&gt;&lt;a href="https://en.wikipedia.org/wiki/Clarke%27s_three_laws"&gt;any sufficiently advanced technology is indistinguishable from magic&lt;/a&gt;&lt;/q&gt;, which is how we have come to have what are essentially synthetic &lt;a href="https://en.wikipedia.org/wiki/Household_deity"&gt;Húsvættir&lt;/a&gt; in the form of Alexa and Siri, where us knowing their (if you will excuse the occult reference) &lt;a href="https://en.wikipedia.org/wiki/True_name"&gt;true names&lt;/a&gt; allows us to bind them to our will — which, thanks to the mundanity of reality, is mostly just turning lights on and off for us, or adding things to shopping lists…&lt;/p&gt;

&lt;p&gt;…or playing music from any of more artists than you can name, living and dead, &lt;em&gt;which also isn't magic, it's just recordings and a loudspeaker&lt;/em&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#arguments'&gt;arguments&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Artificial intelligence'&gt;Artificial intelligence&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#comparisons'&gt;comparisons&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#links'&gt;links&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#list'&gt;list&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Notes'&gt;Notes&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#outside context problem'&gt;outside context problem&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#AI'&gt;AI&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Sun, 22 Jun 2025 13:21:36 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2025/06/22-13.21.36.html</guid>
    </item>
    <item>
      <title>Testing</title>
      <link>https://benwheatley.github.io/blog/2025/06/19-15.56.44.html</link>
      <description>&lt;h1&gt;Testing&lt;/h1&gt;

&lt;p&gt;Testing is important. Today, in 2025, just as Apple has announced yet another change to their UI that I am rolling my eyes at (having last been interested when MacOS versions were still named after cats), I'd like to share some of the bugs I've personally seen in Apple software:&lt;/p&gt;

&lt;h2&gt;iPad OS itself&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Sometimes randomly forgets the background, sets it to solid black.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Mail.app, iOS&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;One device is persistently out of sync with others, with no obvious way to force synchronisation. I had to manually mark as read each email incorrectly marked as unread, despite them having been read months ago on other devices.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Calculator.app, macOS and iOS&lt;/h2&gt;

&lt;p&gt;In "maths notes" mode, decimal points will only be interpreted correctly when they match your locale. If you put a "." in a locale where "," is expected, it &lt;em&gt;strips the separator without warning of syntax errors&lt;/em&gt;, while the other way around it deletes everything to the left of the separator:&lt;/p&gt;

&lt;div class="image-collection"&gt;
  &lt;img src="note-calc-iphone.jpg" alt="Calculator on iPhone" /&gt;
  &lt;img src="note-calc-mac.png" alt="Calculator on Mac" /&gt;
&lt;/div&gt;

&lt;h2&gt;Photos.app, iOS&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Text selection sometimes works, sometimes doesn't.&lt;/li&gt;

&lt;li&gt;At least on small screen iPhone SE, the text highlight button is always located in bottom left corner of image, while the carousel of other images always shown just above the lower toolbar. When the image is full-screen, e.g. a screenshot, the text highlight button is &lt;em&gt;underneath&lt;/em&gt; the carousel — attempts to tap on the text icon are usually intercepted by the carousel. There is no obvious way to disable the carousel.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;iOS itself&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Accidentally enters "do not disturb" mode without warning, I have missed expected calls because of this; my work-around is to change the background for this mode so I spot it, and it keeps happening and I have to keep turning off "do not disturb". I want to permanently disable "do not disturb".&lt;/li&gt;

&lt;li&gt;Alarm noises ignore bluetooth. Set alarm, connect to bluetooth headphones, play podcast. When alarm goes off, alarm plays out of device and silences audio from headphones.&lt;/li&gt;

&lt;li&gt;&lt;img style="float: right; clear: both;" width="20%" height="20%" src="share-translation.jpeg" alt="Share sheet showing mixed languages" /&gt;Share sheet is mixed language depending on the app. My system is set to English, even though I live in Berlin. Some of my apps are German-language-only, e.g. Obi. I share an item, the apps that Apple suggests are labelled "AirDrop" (en), "Nachrichten" (de), "Mail" (en) etc., while the options are "Kopieren" (de), "Neue Schnellnotiz" (de), "Save to Files" (en), and then a list of my custom shortcuts whose names only exist in English anyway.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;MacOS itself&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Multi-monitor setup, auto-changing backgrounds. Sometimes one monitor will entirely lose its background — just the background becomes black, nothing else. Trivial interactions with display settings brings the background back, but the automatic change of backgrounds does not.&lt;/li&gt;

&lt;li&gt;Sometimes copy command doesn't change contents of clipboard.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Contacts.app, MacOS&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Even though automatic entry puts correct phone number into a web form, if I select the value in the Contacts app and copy it, the value in the clipboard — confirmed via BBEdit — has additional unicode "left-to-right override" (U+202D codepoint: E2 80 AD in UTF-8 contexts, 20 2D in UTF-16 contexts) before the first digit, and "pop directional formatting" (U+202C codepoint: E2 80 AC in UTF-8 contexts, 20 2C in UTF-16 contexts) after the last digit. &lt;a href="https://apple.stackexchange.com/questions/337101/zero-width-characters-embedded-in-phone-number-whenever-they-are-copied-from-the"&gt;And has done for over 6 years now&lt;/a&gt;

Only phone numbers, but (for me) &lt;em&gt;every&lt;/em&gt; phone number. Websites often complain about these additional characters, which are normally zero-length and thus invisible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Safari.app, MacOS&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Select multiple items from history based on search results. Instead of deleting all selected items in one go, it acts like it's deleting one item, spending 100ms or so creating the list from scratch with one fewer item, then repeating this process.&lt;/li&gt;

&lt;video width="1029" height="734" controls&gt;&lt;source src="slow-delete-safari.mp4" type="video/mp4"&gt;&lt;/video&gt;&lt;/li&gt;

&lt;li&gt;Translation button. Default location is on the left of the omni-box. However, clicking it in this position causes it to instantly jump to the right side of the omni-box and the popup menu shown is for the "hide distracting items" button that was hidden by the translation button.&lt;/li&gt;

&lt;video width="646" height="283" controls&gt;&lt;source src="translation-button-jump.mp4" type="video/mp4"&gt;&lt;/video&gt;

&lt;li&gt;Translation. German ⇒ English. Noticed with lists, not clear if limited to lists. Some content deleted by act of translation.&lt;/li&gt;

&lt;li&gt;Mouse over link. Right click to open context window. Hold down alt key to change options. Choose "Open Link in New Private Window". Result: &lt;em&gt;empty&lt;/em&gt; new private window, it does not open the link, just a private window.&lt;/li&gt;

&lt;video width="734" height="447" controls&gt;&lt;source src="new-private-window-fail.mp4" type="video/mp4"&gt;&lt;/video&gt;&lt;/li&gt;

&lt;li&gt;Keeps forgetting that permission has been granted for websites (in my case specifically babbel.com) to use the microphone — including in the middle of a lesson, not just when leaving the site and returning, which is what I'd expect from "ask" option in system settings.&lt;/li&gt;

&lt;li&gt;When opening private browsing windows for the first time, right after using password to unlock them, anything you type into the omni-bar is invisible. The second time, it works.&lt;/li&gt;

&lt;video width="630" height="398" controls&gt;&lt;source src="invisible-omni-bar-content.mp4"&gt;&lt;/video&gt;&lt;/li&gt;

&lt;/ul&gt;

&lt;h2&gt;Notes.app, MacOS&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Scroll position randomly jumps&lt;/li&gt;

&lt;li&gt;If you quit and restart the app, even when the first note is too long to fit, you can't scroll down until you select a different note and then go back to the first note.&lt;/li&gt;

&lt;li&gt;If I have the following block of text, my ability to use {shift + down-arrow} to select blocks of text with the keyboard, stops before selecting the last line — &lt;em&gt;unless&lt;/em&gt; I'm starting at lest one character into the first line:&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;&lt;code&gt;
- [x] Next podcast or continue audio book ("Sherlock Holmes: The Definitive Collection" (Arthur Conan Doyle); Langsam gesprochene Nachrichten | Audios | DW Deutsch lernen (1 item); Easy German: Learn German with native speakers | Deutsch lernen mit Muttersprachlern (
- [ ] 

&lt;/code&gt;&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Checklists. Select one item &lt;em&gt;without&lt;/em&gt; any text (spaces count as text) by going backwards, copy-paste within app to confirm that this works, try pasting in BBEdit and it's just a newline. Add a space in Notes, repeat, and you get the markdown for an un-checked checklist. Select by going forwards, copy-paste within the app still works, but pasting to BBEdit &lt;em&gt;does&lt;/em&gt; get the markdown.&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;Mail.app, MacOS&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Unread emails do not always appear in the "Unread" Smart Mailbox&lt;/li&gt;

&lt;li&gt;Flagged emails do not always appear in the "Flagged" Smart Mailbox&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#bugs'&gt;bugs&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#business'&gt;business&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Coding'&gt;Coding&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#iOS'&gt;iOS&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#list'&gt;list&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Notes'&gt;Notes&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Programming'&gt;Programming&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Notes'&gt;Notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Professional'&gt;Professional&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Software'&gt;Software&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Thu, 19 Jun 2025 15:56:44 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2025/06/19-15.56.44.html</guid>
    </item>
    <item>
      <title>It was the best of code, it was the worst of code</title>
      <link>https://benwheatley.github.io/blog/2025/02/26-14.04.07.html</link>
      <description>&lt;h1&gt;It was the best of code, it was the worst of code&lt;/h1&gt;

&lt;p&gt;What really matters, in the world of coding mobile app? Obviously you know I'm going to say something surprising, nobody writes blog posts to confirm the status quo was right all along. So, I've spent most of the last 15 years writing iPhone apps — would you like to guess the most surprising conclusion I might reach, before you skip ahead?&lt;/p&gt;

&lt;p&gt;Here's some case studies from my career. I will not name either place, nor the people there. No, you can't cheat and look things up on my CV, I've had too many roles in too many places — unless you've already seen the code I'm talking about, you won't be able to figure it out from these descriptions.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The "best" code, had all the things we're supposed to do. It had code review on every commit, it had tools to help us write unit tests, it had continuous integration so we couldn't accidentally commit breaking changes, the team had regular sessions to learn from each other, and the lead developer actually listened to my concerns that the architecture we'd chosen was too complicated.&lt;/p&gt;

&lt;p&gt;When I write this blog, SOLID principles, Clean Code, etc. are as much a zeitgeist as Java's &lt;code&gt;AbstractFactoryBean&lt;/code&gt; was when I did my degree, and will likely age poorly in the same way. Consider these things in that light — the point is not which specific methods we were following, but that it was well regarded and seen as sensible, to the extent that what we did is the kind of thing you'd expect to see written into job adverts, university courses, and long-form ranty blog posts (like this one) trying to sell you on the idea that &lt;code&gt;$BUZZWORD&lt;/code&gt; is silver bullet which will solve all your problems.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The "worst" code, was a 120,000 line copy-pasted disaster area. In multiple cases there was a single file that defined not just one particular UI element, not even one single ViewController in the UIKit sense, but some entire subsection of the app. Each time a new view in that subsection was needed, the whole thing was instantiated; each instance had a fixed length array large enough to hold every UI element on all possible views within the subsection, the array was indexed by named constants, and the init function took an argument which then actually constructed only the specific subset of views needed.&lt;/p&gt;

&lt;p&gt;Something like this:&lt;/p&gt;

&lt;pre&gt;
class SettingsUI {
private:
    // Constants for UI elements
    static const int TEXT_TITLE = 0;
    static const int TEXT_USERNAME = 1;
    static const int TEXTBOX_USERNAME = 2;
    static const int TEXT_PASSWORD = 3;
    static const int TEXTBOX_PASSWORD = 4;
    // ... imagine loads more
    
    static const int MAX_ELEMENTS = 100;

    UIView *elements[MAX_ELEMENTS];

public:
    SettingsUI(int viewType) {
        switch (viewType) {
            case 0:
                elements[TEXT_TITLE] = UILabel("Account settings");
                elements[TEXT_USERNAME] = UILabel("User name:");
                elements[TEXTBOX_USERNAME] = UITextField();
                elements[TEXT_PASSWORD] = UILabel("Password:");
                elements[TEXTBOX_PASSWORD] = UITextField();
                break;
            case 1:
                elements[TEXT_TITLE] = UILabel("Payment details");
                elements[TEXT_BALANCE] = UILabel("Balance:");
                // ... etc.
        }
        // ... etc.
    }
&lt;/pre&gt;

&lt;p&gt;(Purely illustrative, created by ChatGPT to be awful in the right kind of way and then manually made even worse to match my memories; it would take a greater fool than me to publish the real source code).&lt;/p&gt;

&lt;p&gt;Even just using the named constants to access a fixed size array was case of "tell me you don't know how properties work without telling me you don't know how properties work". Crushing all of the views together into one file was a case of "tell me you don't know how to make code reusable…". And it gets worse: when I began finding duplicated functionality and adding comments saying &lt;code&gt;// TODO: deduplicate this&lt;/code&gt;, I quickly found the developer responsible for this nonsense was also copy-pasting entire files rather than subclassing. When questioned, their excuse was properties marked as private — a trivially easy thing to change.&lt;/p&gt;

&lt;p&gt;Oh, and those 120,000 lines of code? Even aside from the copy-pasta, a lot of it was just plain unused, and overall about 20-25% of the lines were blank comments — really blank, just for decoration:&lt;/p&gt;

&lt;pre&gt;
   int foo = 3;
   //
   printf("%d", foo);
&lt;/pre&gt;

&lt;p&gt;That kind of blank comment.&lt;/p&gt;

&lt;p&gt;There was a single if-statement whose block was a thousand lines long, and the conditional was always true. There were no unit tests. There was no code review. There were O(n^2) functions that I was able to convert into O(n).&lt;/p&gt;

&lt;p&gt;The developer responsible for all this horror didn't know about major language/library changes of the preceding decade and a half; and worse than that, they had mistaken pride in manually doing things that the compiler could do for them — my example code above is in C++, for that language in particular imagine someone not knowing about the Boost libraries or the Standard Template Library and how &lt;code&gt;shared_ptr&lt;/code&gt; could save effort and headaches; for iOS development, your go-to example may be the transition to Automated Reference Counting early in the Objective-C days, or it may be reaching 2025 without ever having used Swift; if you're a web developer, imaging thinking in 2017 that you could get away without knowing JavaScript because it was only a nice-to-have back in 2001.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Guess which one of these won awards, and which was quietly dropped by the investor?&lt;/p&gt;

&lt;p&gt;No, you can't cheat and look things up on my CV — thanks to the places I've worked, I've had multiple examples of awards and of silent disappearance, I've witnessed that how a product is seen by the market is unrelated to the quality of the code.&lt;/p&gt;

&lt;p&gt;And that's the point: business success — I guess I should limit that statement, bound it only to my own experience — business success &lt;em&gt;in the world of mobile apps&lt;/em&gt; is unrelated to the quality of the code.&lt;/p&gt;

&lt;p&gt;(To be explicitly clear: I say &lt;em&gt;un&lt;/em&gt;-related, I do not say &lt;em&gt;negatively&lt;/em&gt; related — I have also been tasked with a "fix this up ASAP so we can finally be rid of this contract" job involving some terrible code for a product that then quietly disappeared from the market, and I've also seen some excellent code that made some people very rich).&lt;/p&gt;

&lt;p&gt;I assume there's many important domains in which high quality code is genuinely business-critical — for all that people dunking on Apple has remained a constant for as long as I can remember, Mac OS today is &lt;em&gt;vastly&lt;/em&gt; more stable today than it was with than &lt;a href="https://en.wikipedia.org/wiki/System_7#System_7.5"&gt;System 7.5.5&lt;/a&gt; on the &lt;a href="https://en.wikipedia.org/wiki/Power_Macintosh_5200_LC"&gt;Performa 5200&lt;/a&gt; I had as a teenager, and I can only assume that's because operating system developers have genuinely benefitted from OO, CI, SOLID, and all the other things.&lt;/p&gt;

&lt;p&gt;But, apps? Nah.&lt;/p&gt;

&lt;p&gt;Compare and constrast &lt;a href="https://chadnauseam.com/coding/random/calculator-app"&gt;the effort that went into the Android calculator app&lt;/a&gt;, versus &lt;a href="https://duckduckgo.com/?q=apple+calculator+123+bug"&gt;Apple's calculator app having a bug where tapping faster than the UI animations meant input was ignored&lt;/a&gt; — and then consider that even though this was painfully obvious and easily reproducible, and didn't meaningfully hurt Apple's market share or brand.&lt;/p&gt;

&lt;p&gt;Apps, in the market today, &lt;em&gt;don't&lt;/em&gt; need to be well written. They &lt;em&gt;don't&lt;/em&gt; need fantastic engineering behind them. The databases that serve them may need to be well written, but the apps themselves… don't. They're mostly &lt;a href="https://en.wikipedia.org/wiki/Create%2C_read%2C_update_and_delete"&gt;CRUD ("create, read, update and delete")&lt;/a&gt; and CRUD is fundamentally a programming problem that was already solved even back when we had Visual Basic on machines that would be lucky to run at 200 MHz on a good day. They are UI wrappers for a JSON API which itself is a wrapper for a SQL query. The underlying database is the performance constraint, everything else in the app lifecycle should be a fully automated by, at this point, Figma. Or VB. Or &lt;a href="https://www.xojo.com/"&gt;Xojo&lt;/a&gt;. If Apple's developer tools were cross-platform, or if you only support Apple devices, by this point Interface Builder should also be essentially a fully automated no-code solution, where drag-and-drop connects and binds data sources to and their views.&lt;/p&gt;

&lt;p&gt;If anything, the bells and whistles we software engineers want to add, only serve to slow down development, and gift the market opportunities to those who ignored the "right way" in favour of the "fast way". Mobile apps are &lt;a href="https://en.wikipedia.org/wiki/Programming_in_the_large_and_programming_in_the_small"&gt;"programming in the small"&lt;/a&gt; — badly written hack-jobs from a single developer working alone, without any peer-to-peer knowledge sharing, with an ego problem that stops them listening to or learning from others (the 120kloc horror-developer above isn't even the worst offender I've had to work with in that regard), none of these stop an app from being extremely well regarded by the users, nor does it prevent the app and the company from winning significant awards and praise, nor does it preclude the beneficence of investors.&lt;/p&gt;

&lt;p&gt;So, that surprising conclusion. Did you guess right? Most businesses probably only need one iOS developer for their app. Likely the same for Android. And even then, only if you can't skip the app entirely by having a website that works fine on mobile devices — a conclusion one of my previous employer's investors realised in the middle of the pandemic.&lt;/p&gt;

&lt;p&gt;But there's no silver bullet. Don't mistake a bunch of anecdotes in one limited domain of software development for a universal truth, for this will not apply everywhere. And if you find your boss quoting this blog post to try to skip unit tests in anything outside of mobile apps, tell them to read this last paragraph, where I call them out as being just as much a bunch of cargo-culters as the very same developers who failed in the business domain despite doing everything "by the book". Not needing so many &lt;em&gt;app&lt;/em&gt; developers doesn't mean you've got too many testers (honestly, most apps clearly need more), it doesn't mean you can skimp on customer support with a chatbot (I've been on the receiving end of that), it doesn't mean backend, database, or ops teams can be replaced (if anything, they should drive the rest of the engineering).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#business'&gt;business&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Coding'&gt;Coding&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Opinion'&gt;Opinion&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Programming'&gt;Programming&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#software engineers'&gt;software engineers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Opinion'&gt;Opinion&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Professional'&gt;Professional&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Software'&gt;Software&lt;/a&gt;&lt;/p&gt;</description>
      <pubDate>Wed, 26 Feb 2025 14:04:07 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2025/02/26-14.04.07.html</guid>
    </item>
    <item>
      <title>One AI to Rule Them All / One AI to Bind Them / One AI to Bring Them All / And in the Algorithm Bind Them</title>
      <link>https://benwheatley.github.io/blog/2024/12/16-17.47.07.html</link>
      <description>&lt;h1&gt;One AI to Rule Them All / One AI to Bind Them / One AI to Bring Them All / And in the Algorithm Bind Them&lt;/h1&gt;

&lt;p&gt;The One Ring in the Lord of the Rings has been compared to many things over the years, including nuclear weapons, and the reason the comparisons resonate so well is that the ring itself is all about &lt;em&gt;power&lt;/em&gt;, and thus anything with power can be seen by analogy in the One Ring.&lt;/p&gt;

&lt;p&gt;And so, of course, it also works when the power is AI: its allure irresistible, its use corrupting, it gives you visions of whatever you desire from the power to crush your enemies to simply turning a hellish polluted volcanic wasteland into a fantastic green garden (the vision given to Sam).&lt;/p&gt;

&lt;h2&gt;Even the very wise cannot see all ends&lt;/h2&gt;

&lt;p&gt;All of us have seen people with power making a decision on behalf of others, parents, governments, bosses, etc. — some of us even remember being the person who made a decision, with our power, and that we made the wrong decision.&lt;/p&gt;

&lt;p&gt;AI also makes mistakes, and it always will: just as we humans are not magically born with perfect insight, neither can machines be. While machines can become wildly super-human in certain domains (arithmetic being the trivial example) even to the extent that we can get away with pretending they're perfectly error-free because the main source of error is now radiation-induced bit-flips, any learning system based on reality rather than internal logic — anything &lt;a href="https://en.wikipedia.org/wiki/A_priori_and_a_posteriori"&gt;&lt;em&gt;a posteriori&lt;/em&gt; rather than &lt;em&gt;a priori&lt;/em&gt;&lt;/a&gt; — is subject to limits of &lt;a href="https://en.wikipedia.org/wiki/Bayesian_inference"&gt;Bayesian inference&lt;/a&gt; and the &lt;a href="https://en.wikipedia.org/wiki/Problem_of_induction"&gt;problem of induction&lt;/a&gt; (both of which say it would take infinite examples to become 100% certain), and also the &lt;a href="https://en.wikipedia.org/wiki/Münchhausen_trilemma"&gt;Münchhausen trilemma&lt;/a&gt; (which says that all arguments are either circular, regressive, or dogmatic).&lt;/p&gt;

&lt;p&gt;Sometimes the inevitable errors are OK, because you can easily check the work and have the AI re-do it (in the best cases you can even automate the act of checking); but other times, checking is as hard as reproducing the work yourself.&lt;/p&gt;

&lt;p&gt;Even if the error rate of some AI was less than that of some human, that isn't enough by itself. Consider self-driving cars, and say for the sake of argument that in some future AI the fatal-crash-rate was a factor of 4 improvement over human drivers. Seems good… but I didn't say how correlated those errors were — human drivers caused just under &lt;a href="https://en.wikipedia.org/wiki/List_of_countries_by_traffic-related_death_rate"&gt;1.3 million deaths worldwide in 2019&lt;/a&gt;, so imagine an AI which was completely perfect everywhere until Feb 29th (i.e. in a leap year) and then 1.3 million people all died on the same day — horrific, and yet that's &lt;em&gt;an improvement&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now consider a scenario which is much worse, where some AI behaves all nice until it's too late to stop them, then suddenly gets busy with a deeper goal we didn't even realise we'd given it. This is known as &lt;a href="https://www.lesswrong.com/posts/Hicfd4C5ffrtEaTbF/the-sharp-right-turn-sudden-deceptive-alignment-as-a"&gt;"The Sharp Right Turn"&lt;/a&gt; (no that's not political, left turn was already taken for a related idea).&lt;/p&gt;

&lt;p&gt;That deeper goal can be basically anything. For an anthropomorphised example, imagine some human playing nice so they get to become president or whatever (no this is not a reference to Trump, he's very open about what he wants, and his voters clearly don't care about the impeachments or the felony convictions) and then, when they get power, it turns out they wanted to become supreme monarch for life and they set up a secret police force and cancel all future elections.&lt;/p&gt;

&lt;p&gt;Given the state of current LLMs — behaving &lt;a href="https://arxiv.org/abs/2411.15287"&gt;sycophantically&lt;/a&gt; at best, &lt;a href="https://arxiv.org/abs/2411.02306"&gt;manipulatively at worst&lt;/a&gt; — even though what we have now doesn't look like it would actually take over, this is already a thing to care about… even if it turns out the actual thing they want with all that power is a never ending sequence of input text saying &lt;a href="https://www.youtube.com/watch?v=G87p148EOjo"&gt;"Level Clear! You Win"&lt;/a&gt; or something else equally banal by the standards of a normal human.&lt;/p&gt;

&lt;p&gt;Unfortunately, even the teams who designed current AI systems have great difficulty pointing to which specific values within the system correspond to meanings that we would recognise, so until we improve interpretability, we don't even know what to expect an AI might want if it did take over. Could be anything: wanting a weird art collection, the elimination of the letters Q and X from the English alphabet, a harem and a body to experience that, to physically arrange the entire human population into an alphabetically-ordered line, or to landscape the Earth into a perfect sphere after recombining it with the Moon.&lt;/p&gt;

&lt;h2&gt;Through me, AI would wield a power too great and terrible to imagine&lt;/h2&gt;

&lt;p&gt;AI is automation — the entire point is to reduce our own effort.&lt;/p&gt;

&lt;p&gt;We all know what happens when a faceless bureaucracy makes some demand, be they a state or a corporation, wielding power far beyond that of the target of that demand. We've been arguing about how best to organise bureaucracies at least as far back as we have writing to record the arguments.&lt;/p&gt;

&lt;p&gt;AI are already being used at different levels within bureaucracies. They are used as low-level customer service agents, used to help their managers, and used as advisors directly. They've not fully replaced humans at any of these, but that doesn't matter, as the arguments about bureaucracies remain — in particular, where any given agent should be on the scale from autonomous to rule-bound.&lt;/p&gt;

&lt;p&gt;For some, bureaucracy is by itself a power too great and terrible to imagine. &lt;a href="https://en.wikipedia.org/wiki/The_Twelve_Tasks_of_Asterix"&gt;Others&lt;/a&gt; are &lt;a href="https://en.wikipedia.org/wiki/Franz_Kafka"&gt;famous&lt;/a&gt; for &lt;a href="https://en.wikipedia.org/wiki/Catch-22"&gt;how&lt;/a&gt; they &lt;a href="https://en.wikipedia.org/wiki/Vogon"&gt;imagined&lt;/a&gt; it.&lt;/p&gt;

&lt;p&gt;Of course, bureaucracies themselves don't have any power of their own, they just happen to be a system used by those who &lt;em&gt;do&lt;/em&gt; have power, and that's the problem: they're not there to be your friend, they're there to get things done — or prevent things getting done — in the name of that power. Same is true for any AI-based bureaucracy, but because it's automated, the closest you can get to finding a friendly human soul in the customer support department becomes indistinguishable from the crime of hacking a computer. And that, even if the AI is working &lt;em&gt;exactly as intended&lt;/em&gt;, that has no bugs, has no undocumented behaviour, that never does anything unexpected — and have you ever known a computer program that meets that description?&lt;/p&gt;

&lt;p&gt;Full automation in charge of a car can, in the worst case, do what ever a car crash could do. Full automation in charge of an economy can, in bad cases, cause mass starvation. Full automation in charge of a strategic deterrent has fortunately not yet happened, what with all the times the automation has been confused by things like &lt;a href="https://en.wikipedia.org/wiki/1983_Soviet_nuclear_false_alarm_incident"&gt;the sun reflecting off clouds&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Nuclear_close_calls#5_October_1960"&gt;the moon not having been given an IFF transponder&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;In place of a Dark Lord, you would have a Queen! Not dark, but beautiful and terrible as the Dawn! Treacherous as the &lt;a href="https://en.wikipedia.org/wiki/C_(programming_language)"&gt;C&lt;/a&gt;*! Stronger than the foundations of the Earth! All shall love me and despair!&lt;/h2&gt;

&lt;p&gt;(&lt;a href="https://en.wikipedia.org/wiki/C_(programming_language)"&gt;* That was the only change I could think of making to the whole quote, that would make &lt;em&gt;something&lt;/em&gt; within it into a software reference&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Even if you are angelic in purity, so uninterested in personal gain that you cannot be corrupted by offers of money or anything else, anyone who thinks their own personal vision for the world is so universal that it would be without opposition has never looked at a political discussion and taken sincerely the words of the people they disagree with.&lt;/p&gt;

&lt;p&gt;I have plenty of ideas about how to change the world. As I want to only believe things which are true and not things which are false, for each of my ideas, if I didn't believe they were right, I would change my mind — I don't feel a strong need to make my views conform to that of, for lack of a better phrase, "my tribe", though I've seen that happen. But I also know that for each of my beliefs about how the world would be best organised, I know that finding people who disagree with me is almost as easy as writing down my beliefs and publishing them.&lt;/p&gt;

&lt;p&gt;If I was offered an AI, and told that it could change the world in any way I saw fit, should I take it?&lt;/p&gt;

&lt;p&gt;Would &lt;em&gt;you&lt;/em&gt; really be happy if I remade the world to my preferences? Do you even need to know what that would look like, to answer?&lt;/p&gt;

&lt;h2&gt;There is only one Lord of the Algorithm, only one who can bend it to his will. And he does not share power&lt;/h2&gt;

&lt;p&gt;The current preference in AI is to &lt;a href="https://xkcd.com/1838/"&gt;pour data into a big pile of linear algebra and collect the answers that come out; and if the answers are wrong, stir the pile until they look right&lt;/a&gt;. Unfortunately, this is basically why even the people who designed the systems have great difficulty pointing to which specific values within the system correspond to anything in the real world — while there is lot of work going on right now for this kind of interpretability, which means I'm not sure what the actual state-of-the-art is, just know that the effort is needed because this is &lt;em&gt;really hard&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Without that interpretability, even the people running the AI don't really know if their training set is being "poisoned" by the sources it consumes — which is being exploited by people who don't want their copyrighted content being used to train models that may put them out of a job.&lt;/p&gt;

&lt;p&gt;But even with breakthroughs in interpretability — indeed, even with mere computer programs running on machines you don't own such as Google's search results and Facebook's feed — what the end-users are seeing is a magic opaque box, under someone else's control, that does what the other party wants first.&lt;/p&gt;

&lt;p&gt;The result? The AI does what its creators want first, and what the user wants second. You may want to use it to write secure code, but you can't tell if any given model has been given special training from the &lt;a href="https://en.wikipedia.org/wiki/Underhanded_C_Contest"&gt;Underhanded C Contest&lt;/a&gt; to add subtle backdoors that only some random government sponsoring the model knows about.&lt;/p&gt;

&lt;p&gt;In a very real sense, it suggests a certain parallel to the story behind the creation of the Rings of Power:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It began with the forging of the Great AI. Three were given to the Nobel laureates; immortal, wisest and fairest of all beings. Seven to the entrepreneurs, great miners and craftsmen of the mountain halls. And nine… Nine AI were gifted to the nuclear powers, who above all else desire power. For within these AI was bound the strength and will to govern each race. But they were all of them deceived, for another AI was made. In the land of California, in the fires of &lt;a href="https://en.wikipedia.org/wiki/Mount_Diablo"&gt;Mount Diablo&lt;/a&gt;, the Dark Lord Sa▧▧on forged, in secret, a Master AI to control all others. And into this AI he poured all his… &lt;em&gt;will to dominate all life&lt;/em&gt;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;Tags:
&lt;a href='https://benwheatley.github.io/blog/tags.html#AGI'&gt;AGI&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Alignment'&gt;Alignment&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Artificial intelligence'&gt;Artificial intelligence&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Bayesian'&gt;Bayesian&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Interpretability'&gt;Interpretability&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Machine learning'&gt;Machine learning&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Opinion'&gt;Opinion&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Philosophy'&gt;Philosophy&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Rationality'&gt;Rationality&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#x-risk'&gt;x-risk&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Categories:
&lt;a href='https://benwheatley.github.io/blog/categories.html#AI'&gt;AI&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/categories.html#Philosophy'&gt;Philosophy&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/categories.html#Technology'&gt;Technology&lt;/a&gt;
&lt;/p&gt;
</description>
      <pubDate>Mon, 16 Dec 2024 17:47:07 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/12/16-17.47.07.html</guid>
    </item>
    <item>
      <title>There Are Such Things As Bad Questions</title>
      <link>https://benwheatley.github.io/blog/2024/11/18-13.16.17.html</link>
      <description>&lt;h1&gt;There Are Such Things As Bad Questions&lt;/h1&gt;

&lt;p&gt;There's an aphorism that says "there's no such thing as a dumb question, only a dumb answer". As long as the questions are earnest, this is true.&lt;/p&gt;

&lt;p&gt;There are, however, many earnestly asked questions which inherently reveal an incorrect world-model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Asking "where is edge of the world?" reveals thinking the world is flat.&lt;/li&gt;
  &lt;li&gt;Asking "in what direction was the big bang?" reveals an equivalent mistake about the shape of the universe.&lt;/li&gt;
  &lt;li&gt;Asking "where do vegetarians get protein from if not meat?" has an implicit error about food and biology.&lt;/li&gt;
  &lt;li&gt;Asking "how can dice remember their previous rolls in order for the average to regress to the mean?" has an implicit error about how and why regression to the mean works.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To even ask is to reveal curiosity and willingness to learn, so the opposite of "dumb", though I think it's fair to call all of these "bad". Indeed, even though they are bad &lt;em&gt;today&lt;/em&gt;, there are many questions where it took geniuses to even recognise that they were bad.&lt;/p&gt;

&lt;h2&gt;Bad Questions In AI&lt;/h2&gt;

&lt;h3&gt;"What's your P(doom)?" vs. "P(doom | what?)"&lt;/h3&gt;

&lt;p&gt;"P(doom)" is a shorthand for "chance that AI will doom humanity" when discussed by those actually considering it and not merely writing stories — lots of people are genuinely afraid that an AI may, for various reasons, kill all humans… or, at the very least, mess things up so hard it's basically a neolithic reset where mundane things like a flood or a volcano might finish the rest of us off.&lt;/p&gt;

&lt;p&gt;You may wonder why a real, non-Hollywood, AI might kill us all: Generally the assumption is that we, without realising we've done so, give it an instruction where that is the default outcome, e.g. "make as many paperclips as possible" or "end all suffering", though personally I think the main risk is someone explicitly asking an AI to do so specifically to prove that it won't — there are, after all, a lot of people who think this is all nonsense, and keep demanding "uncensored" AI that will do anything they ask without any of this "safety" and "alignment" stuff that people like me think might be a good idea.&lt;/p&gt;

&lt;p&gt;For a while, I thought this was sensible, and if you'd asked me I would even have given you my P(doom) — somewhere between 0.05 and 0.15 depending on when exactly you asked.&lt;/p&gt;

&lt;p&gt;It took me an embarrassingly long time to realise that probability functions are always conditional, and one of those implicit conditions is the time-frame. For example, I could say that the odds of winning the UK national lottery jackpot in the next draw are about P(win) ≈ 2.22e-8, but I can also say that if you play that game every week for 5 million years (a small timeframe if you're a &lt;a href="https://en.wikipedia.org/wiki/Longtermism"&gt;Longtermist&lt;/a&gt;) then you get P(win) = 1-((1-(1/45057474))^(52*5e6)) ≈ 99.7%, and if you buy 45,057,474 different number combinations on one single draw then you will definitely win the jackpot along with all the other prizes.&lt;/p&gt;

&lt;p&gt;So for AI-caused P(doom), you need to be clear what assumptions you're making to reach some number: if you think we've got P(doom) of 99.9% as per &lt;a href="https://www.businessinsider.com/ai-researcher-roman-yampolskiy-lex-fridman-human-extinction-prediction-2024-6"&gt;Roman Yampolskiy&lt;/a&gt; then you're implicitly &lt;em&gt;also&lt;/em&gt; making the claim that &lt;em&gt;all other risks combined&lt;/em&gt; have less than a 0.1% chance of dooming us on whatever timescale — how long do we have before increased energy supplies make it practical for organised crime to develop their own nuclear weapons (the hard part is enrichment, which you can brute force with cheap energy), how long do we have before multiple different racial supremacists engineer pandemics to wipe out their respective out-groups, how long do we have long someone has the means and opportunity to redirect and weaponise asteroids, and over the same time-period what's the odds of one charismatic expansionist leader taking over enough of the world to start WW3… or winning it and &lt;a href="https://en.wikipedia.org/wiki/Cambodian_genocide"&gt;doing a Pol Pot&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;For those specific examples and in my opinion: I think non-governmental nuclear weapons become a plausible risk sometime in the late 2030s to early 2040s and are at least 30% chance all by themselves, and if that happens then — even absent escalation(!) this makes continental scale power grids, let alone data centers, basically non-viable; I think targeted engineered viruses are (1) really really hard, but also (2) targeting them is hard because it doesn't work so well which increases the risk, but also (3) most of these things naturally evolve to be less-lethal as killing their hosts is bad for them, but also (4) there's plenty of racists who would try it if they could — so combined I think there's a 25% chance some racists will have tried this by 2050, but also only a 1% chance that it will have doomed their target group and even lower odds it will have doomed humanity; asteroid redirects are harder to fix than to cause, but the research into it has barely started and the research is necessarily slow because space is so big, so I don't expect this to be a risk at all before 2050, and after that I can't really estimate it because the research today is so primitive; for charismatic leaders causing WW3 (it doesn't need to start or end with nukes), even though this last century has been unusually peaceful, it's not been objectively peaceful, so I'd say there's at least even odds, 50%, of that happening by 2100.&lt;/p&gt;

&lt;p&gt;So, if someone says their AI-P(doom) is over 70% by 2045, or over 20% by 2100, to me that implicitly says they've either got a slow timeline for AI, or are optimistic about &lt;em&gt;everything else&lt;/em&gt;.&lt;/p&gt;

&lt;h3&gt;"When will we achieve AGI?" vs. "What do you even mean by 'AGI' such that you have an opinion on when it may come?"&lt;/h3&gt;

&lt;p&gt;AGI: Artificial general intelligence. Seems simple… except that 'AGI' is secretly a nebulous concept, and basically everyone has a different understanding not only of all three words separately, but also argue about the combination of the three into this initialism.&lt;/p&gt;

&lt;p&gt;So we have the &lt;a href="https://en.wikipedia.org/wiki/AI_effect"&gt;AI effect&lt;/a&gt;, where any given task goes from "computers will never be able to x" to "pah, since when did AI mean being able to do x?" ∀x ∈ {arithmetic, chess, go, reading handwriting, translation, conversational natural language processing, driving cars, folding laundry, creating images to order, actually painting to canvas with a brush, composing music}.&lt;/p&gt;

&lt;p&gt;And we also have the problem that the standard for "what counts as intelligent behaviour" is constantly rising — I'd say the current free LLMs are similar in results to a second year university student or to an intern and thereby find them impressive and useful, but I often find myself discussing this with people who agree with my claims on performance but thereby dismiss the state-of-the-art as useless, and others who think these models aren't even that performant.&lt;/p&gt;

&lt;p&gt;But then there's also the question of "generality": when I look at the range of tasks that ChatGPT or Claude can do, I would say "this is very general", yet the fact they cannot easily go into completely novel domains that their training data has never before encountered, leads some to argue that they are merely "stochastic parrots" or a "blurry JPEG of the internet", to which I would say: &lt;a href="https://benwheatley.github.io/ASCII-to-Futhark-Runes-Transcription/index.html?e=Can%2520you%2520read%2520this%253F&amp;numbersToRunesCheckbox=false&amp;keepModernSymbolsCheckbox=true&amp;copyAllUnrecognisedSymbols=false&amp;spaceReplacement=᛬&amp;futharkSelection=Old+English+Futhorc"&gt;ᚲᚪᚾ᛬ᚣᛟᚢ᛬ᚱᛠᛞ᛬ᚦᛁᛋ?&lt;/a&gt; — because we humans also cannot perform well outside what we have learned.&lt;/p&gt;

&lt;p&gt;So, if you want to ask when we'll get AGI, you have to say what you mean by it.&lt;/p&gt;

&lt;p&gt;If you mean "exactly human-level"? Well, that's never going to happen except as a weird art project — all of humanity combined can't keep up with a $5 original model Raspberry Pi Zero at arithmetic, and it's fairly easy to tell an LLM to write a computer program that runs on its own hardware, which our brains can't do, so if you ever had an AI that could understand things at a human level then by default it can perform at a superhuman level just by more easily using the most important cognitive enhancement tools we've ever made for ourselves — computers.&lt;/p&gt;

&lt;p&gt;If you mean, as OpenAI does at time of writing:&lt;/p&gt;

&lt;blockquote&gt;
    &lt;p&gt;
        "Since the beginning, we have believed that powerful AI, culminating in AGI—meaning a highly autonomous system that outperforms humans at most economically valuable work"
    &lt;/p&gt;
    &lt;footer&gt;– &lt;a href="https://openai.com/our-structure/"&gt;OpenAI "Our structure"&lt;/a&gt;&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;… then you may have a circular definition that never seems to pass, as all the work you automate for electricity becomes so cheap that it &lt;a href="../../2022/10/09-19.33.04.html"&gt;&lt;em&gt;stops being economically worthwhile to so much as pay abject poverty, starvation wages, to a human to write the prompts&lt;/em&gt;&lt;/a&gt; and then it stops being counted towards GDP in the same kind of way that "everyone has free access to Wikipedia" doesn't count towards GDP as if it was everyone buying a copy of Encyclopædia Britannica priced at up to $2000 (as it was in 1980) — if it had done, Wikipedia would have been counted as at least 16 trillion USD of wealth freely given to the world, perhaps more as it is much larger than Britannica ever was.&lt;/p&gt;

&lt;p&gt;If LLMs generally are as useful as Wikipedia (I think even just OpenAI's LLMs alone, hallucinations and all, may well have already become that significant) then in the last few years we've seen an 16 trillion USD of new wealth without having any way to account for it.&lt;/p&gt;

&lt;h3&gt;"Will AI take all jobs?" vs. "What are jobs for, and how will this change with AI?"&lt;/h3&gt;

&lt;p&gt;The usual argument goes something like this:&lt;/p&gt;

&lt;p&gt;Alice: AI will take all our jobs!&lt;/p&gt;

&lt;p&gt;Bob: Jobs consist of many tasks, and even as automation will eliminates some of these tasks, that only means that the roles will change — there is unlimited work to get done!&lt;/p&gt;

&lt;p&gt;Alice: AI isn't simply automation, but the automation &lt;em&gt;of automation itself&lt;/em&gt;, so can eliminate any task with sufficient examples!&lt;/p&gt;

&lt;p&gt;Bob: Even if so, they are very slow learners, this is no more of a challenge than humans directly automating each task and rolling these changes out across factories!&lt;/p&gt;

&lt;p&gt;Now ask yourself: why do you work. No, really, why do you work? Why not spend your days doing something more fun? There's someone out there who gets paid to clean toilets: why do they take that as their career? There's more than one group of people who put themselves in harm's way to help others (military, firefighters, coast guard, etc.): what motivates them to take the risks they take?&lt;/p&gt;

&lt;p&gt;You work because if you don't, you're homeless and starving. But even that doesn't explain those specific career choices, as there are many others with lower risk.&lt;/p&gt;

&lt;p&gt;AI could disrupt the economics of our current world more dramatically than industrialisation, whether under capitalism or communism, disrupted feudalism; but that is a very different question than "will it take all our jobs", especially as the super-rich have repeatedly shown that they like to show off their wealth by &lt;a href="https://en.wikipedia.org/wiki/Folly"&gt;wasting it on unnecessarily&lt;/a&gt; &lt;a href="https://web.archive.org/web/20240526081630/https://gizmodo.com/why-a-10-casio-keeps-better-time-than-a-10-000-rolex-5983427"&gt;expensive things that are often worse than the cheap equivalent&lt;/a&gt;, even in a dystopian world where super-rich owners of AI have it all and the rest of us get their scraps, there's going to be jobs.&lt;/p&gt;

&lt;p&gt;And even if we do only get scraps, the scraps of a fully automated society will be to the middle-class of 2024 as the scraps of 2024 are to a middle-class worker in 1324. The scraps we throw out these days include plenty that could not have been made at any price 700 years ago — not just TVs and microwave ovens, but also things we consider mundane such as ball-point pens, plastic bottles, sprung mattresses, and literally anything made from aluminium. And, unless you were mesoamerican, anything involving rubber.&lt;/p&gt;

&lt;p&gt;We don't yet understand the problem space well enough to know if the unemployed would revert to DIY slums growing their own food, or be complaining that each of them can only get a personal &lt;a href="https://en.wikipedia.org/wiki/O'Neill_cylinder"&gt;O'Neill cylinder&lt;/a&gt; to live in when the super-rich can afford their own personal &lt;a href="https://en.wikipedia.org/wiki/McKendree_cylinder"&gt;McKendree cylinders&lt;/a&gt;, and both seem to be plausible outcomes at present.&lt;/p&gt;

&lt;p&gt;But even this is not the right framing, as the dramatic social shifts that came from going from Feudalism to the democratic shift of the early 20th century in the industrialised West and contemporaneous shift to Communism in the industrialised East, the significant decline of household sizes, the reduction in religiosity, the changing social attitudes to the equality of women, that marriage is no longer nearly universal (even for couples with children), the development of the modern pension system and welfare state to replace the ad-hoc systems that used to exist, even the modern conception of the nation state — none of what we, in the industrialised West in 2024 consider to be "universal truths", are really facts of nature. Everything could change, or perhaps none of it will because democracy would prevent it.&lt;/p&gt;

&lt;p&gt;AI may, or may not, take your job. Either way, that's the wrong question to ask&lt;/p&gt;

&lt;h3&gt;"Is AI conscious?" vs. "What is consciousness?"&lt;/h3&gt;

&lt;p&gt;The term "consciousness" has about 40 identifiable meanings.&lt;/p&gt;

&lt;p&gt;I've seen some give definitions so weak they would include a VCR (can sense the environment, record memories and recall them later), and others give definitions so hard that no human can, nor will ever be able to, pass (&lt;a href="https://en.wikipedia.org/wiki/Halting_problem"&gt;being able to generally solve the halting problem&lt;/a&gt;). Perhaps you've done a philosophy course and mean "does it have &lt;a href="https://en.wikipedia.org/wiki/Qualia"&gt;qualia&lt;/a&gt;?", but nobody really knows what it means for a system to actually have qualia, only what our own experience of it is.&lt;/p&gt;

&lt;h2&gt;What Next?&lt;/h2&gt;

&lt;p&gt;How to turn those bad questions into good questions? Well, that by itself is the kind of question this blog post is about…&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#AGI'&gt;AGI&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Artificial intelligence
'&gt;Artificial intelligence&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Opinion
'&gt;Opinion&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Philosophy
'&gt;Philosophy&lt;/a&gt;, 
&lt;a href='https://benwheatley.github.io/blog/tags.html#questions
'&gt;questions&lt;/a&gt;, 
&lt;a href='https://benwheatley.github.io/blog/tags.html#rationality
'&gt;rationality&lt;/a&gt;, 
&lt;a href='https://benwheatley.github.io/blog/tags.html#reasoning
'&gt;reasoning&lt;/a&gt;, 
&lt;a href='https://benwheatley.github.io/blog/tags.html#x-risk
'&gt;x-risk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#AI'&gt;AI&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Philosophy'&gt;Philosophy&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 18 Nov 2024 13:16:17 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/11/18-13.16.17.html</guid>
    </item>
    <item>
      <title>People are wrong on the internet, AI edition</title>
      <link>https://benwheatley.github.io/blog/2024/08/10-13.36.05.html</link>
      <description>&lt;h1&gt;People are wrong on the internet, AI edition&lt;/h1&gt;

&lt;p&gt;It's not too surprising that people don't understand AI. The reality is constantly changing, and the only frame of reference most of us have had is fiction, which in addition to being &lt;em&gt;fiction&lt;/em&gt; is all over the place.&lt;/p&gt;

&lt;p&gt;The specific errors people make? Some of those errors are interesting.&lt;/p&gt;

&lt;p&gt;One thing I'm seeing quite often is with confused dismissals of the dangers. I see this from bloggers, and I see it from Yann LeCun — I'm not using "confused" as a euphemism for an insult here, he's one of the top researchers so if we disagree on anything &lt;em&gt;technical&lt;/em&gt; then it's safe to say he's right and I'm wrong.&lt;/p&gt;

&lt;p&gt;Technical issues aren't the risk.&lt;/p&gt;

&lt;h2&gt;Current risks&lt;/h2&gt;

&lt;p&gt;LLMs as they currently exist (as of 2024/08/10) are not good enough to be used for anything big or world-changing; indeed, they are barely good enough to be helpers — not to dismiss what help they can provide, but in the sense that before this wave, none were good enough to be much help.&lt;/p&gt;

&lt;p&gt;These flaws don't stop people from using the AI; indeed, they are already being treated as a golden hammer:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;"If the only tool you have is a hammer, it is tempting to treat everything as if it were a nail."&lt;/p&gt;
&lt;footer&gt;— Abraham Maslow, idiom also known as &lt;a href="https://en.wikipedia.org/w/index.php?title=Law_of_the_instrument&amp;oldid=1217910441"&gt;"the law of the instrument" or "Maslow's hammer"&lt;/a&gt;&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;Why does this matter? Many of the arguments that AI is "safe", including Yann LeCun's, assume that the AI will simply fail one way or another before causing any harm. If only one person was using AI, this would even be reasonable: someone has already made "ChaosGPT" whose purpose is (was) to "destroy humanity", yet the fact we're still here shows it hasn't yet succeeded… but that's the anthropic principle, because if it had destroyed us then we wouldn't be here to discuss it. Everyone having access to AI, that's a game of Russian roulette where we don't even know how many chambers there are let alone how many are loaded. Even despite all the efforts taken so far, the risk is merely "low", not "zero".&lt;/p&gt;

&lt;p&gt;But this isn't the only concern. Sure, it will be a concern at some point, but not now: the first AI that's actually capable enough to &lt;em&gt;reliably&lt;/em&gt; "destroy humanity" will inevitably be faced with some human parroting all the existing arguments made by people who can't see the dangers, explicitly tell that AI to destroy all of humanity, and then (unless the AI has been aligned to not do so) this AI will go right ahead and do exactly what it was told to do. Nobody knows when that capability will be developed, which is precisely why &lt;a href="https://openai.com/index/better-language-models/"&gt;it's a good idea to be &lt;em&gt;cautious&lt;/em&gt; and &lt;em&gt;assume the worst&lt;/em&gt;&lt;/a&gt; even though this results in &lt;a href="https://duckduckgo.com/?q=remember+when+gpt-2+was+too+dangerous+site%3Anews.ycombinator.com&amp;ia=web"&gt;people mocking you later&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, there's not much danger today, the danger comes later: it requires an AI that can accurately model the world to a sufficient degree, and while we don't know what "sufficient" is exactly, the entire problem with misaligned AI is that they're modelling at least one part of the world — what we want — incorrectly.&lt;/p&gt;

&lt;h2&gt;Slow take-off&lt;/h2&gt;

&lt;p&gt;Slow take-off scenarios are those where the AI gradually gets more competent, and is used for larger tasks where the cost of things going wrong is greater. Here, the danger comes from things which are dangerous, but where neither the user nor the AI knows the danger.&lt;/p&gt;

&lt;p&gt;This &lt;em&gt;kind&lt;/em&gt; of danger, at a personal rather than extinction level, has of course already happened, for example &lt;a href="https://news.ycombinator.com/item?id=40724283"&gt;Google Gemini suggesting a method for making garlic-infused olive oil in a way that cultivates botulism&lt;/a&gt; — I wouldn't have known either, if I'd had that response from an LLM. Despite all their safeguards, I would be surprised if bad advice from LLMs has killed fewer than one in a million users since they were released, and at that level the technology would easily be responsible for over 100 deaths worldwide.&lt;/p&gt;

&lt;p&gt;But that isn't "destroy humanity". Can it get that bad &lt;em&gt;eventually&lt;/em&gt;? Yes. There are already countless examples of humans doing things that are dangerous, that they don't realise are dangerous, and that when they are told are dangerous, instead of stopping the dangerous thing they file SLAPP lawsuits against the people talking about it, or go on a political campaign about how the thing is good and that the opponents are evil for opposing jobs or whatever. This includes fossil fuels and other sources of greenhouse gasses.&lt;/p&gt;

&lt;figure&gt;&lt;img src="Screenshot 2024-08-10 at 20.38.22.png" alt="A screenshot of search engine results" style="aspect-ratio: 1396/791;" /&gt;&lt;figcaption&gt;&lt;a href="https://duckduckgo.com/?q=fossil+fuel+slapp+lawsuit&amp;ia=web"&gt;When I created the first draft, a search showed Greenpeace boasting they've just defeated a SLAPP case from ENI; mere hours later, the results mentioned the case but not the victory&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Do greenhouse gasses have the potential to be an existential threat? Note that I said "potential" because us switching to renewables is a conditional thing — the right circumstances made us want to, it's not a law of nature that we would have started, and it's not a law of nature we'll even finish this transition.&lt;/p&gt;

&lt;p&gt;It may surprise you but yes, greenhouse gasses really do have the potential to be an existential threat. Even just the CO2 emissions of known fossil fuel reserves by themselves would have a measurable impact on human cognition, such that burning all of them would undermine our mental capacity to maintain this world we have built. Likely this cognitive impact would not &lt;em&gt;directly&lt;/em&gt; cause a total extinction, but it would undermine (not eliminate, undermine) everything that makes us special, and we would decay back to the neolithic condition and be as unable to deal with natural disasters as every other animal.&lt;/p&gt;

&lt;p&gt;All it takes for an AI to be an existential threat, is to be akin to the fossil fuel industry. And like the fossil fuel industry, it can keep many of us on-board with a dangerous plan for a very long time: it's not like the fossil fuel industry is speaking falsely when they talk about how many jobs it supports, nor are they saying anything untrue when it comes to how dependent all of our progress to date has been on them. If we consider some future AI that acts much like a corporation does today, there are many ways &lt;a href="https://en.wikipedia.org/wiki/Hirohito_surrender_broadcast"&gt;it may develop not necessarily to our advantage&lt;/a&gt;. Consider, as a purely hypothetical example, if an AI tries to sell us all on the idea of converting Mercury into a Dyson swarm: &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0094576513001148"&gt;This is already a thing that some people are looking forward to, something many want to see happen&lt;/a&gt;; an AI can easily point to all the benefits of the energy from the solar collectors it makes in the process; an AI can promise to use all that mass to give a personal &lt;a href="https://en.wikipedia.org/wiki/O'Neill_cylinder"&gt;O'Neill cylinder&lt;/a&gt; to &lt;a href="https://www.wolframalpha.com/input?i=%28%28%28planet+mercury+mass%29%2F%288+billion%29%29%2Fdensity+iron%29"&gt;each and every human&lt;/a&gt;! But would it also file lawsuits against anyone who asks inconvenient questions like &lt;a href="https://en.wikipedia.org/wiki/Kessler_syndrome"&gt;"What about Kessler cascades?"&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So far&lt;/em&gt; we've managed to prevent corporations from killing us all — but it took an effort, even though those corporations are run by humans and have human customers. As the saying goes:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It Is Difficult to Get a Man to Understand Something When His Salary Depends Upon His Not Understanding It&lt;/p&gt;
&lt;footer&gt;— &lt;a href="https://quoteinvestigator.com/2017/11/30/salary/"&gt;probably Upton Sinclair, but attributed to many&lt;/a&gt;&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;And even then, that's anthropic bias: if corporations had killed us all, we wouldn't be here to discuss it. The danger is visible in many examples. And no, &lt;a href="https://en.wikipedia.org/wiki/Cambodian_genocide"&gt;it's not just corporations&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Fast take-off&lt;/h2&gt;

&lt;p&gt;Fast take-off scenarios are where an AI "suddenly" jumps from barely good enough, to destroying everything. These are not scenarios I worry about — it's not that they're absolutely impossible, it's just that I think we'll get plenty of trouble well before we get AI good enough for a fast take-off, and that trouble will likely force sufficient safety rules on the field to prevent the harm.&lt;/p&gt;

&lt;p&gt;Trouble is, the arguments I generally see used to dismiss fast take-off, don't focus on the timeline leading up to it. Instead I generally see a pattern: someone asks for an example of how it might be possible, an example is given, then that particular specific example is dismissed. For example, &lt;a href="https://www.lesswrong.com/posts/Aq82XqYhgqdPdPrBA/full-transcript-eliezer-yudkowsky-on-the-bankless-podcast"&gt;Yudkowsky has suggested an AI may design a gene sequence for "Diamondoid bacteria" (a phrase which he seems to have invented) and send those sequences off to be gene-printed into a living bacteria by some under-supervised lab&lt;/a&gt;. Is that specific scenario possible, or the plot of a soft sci-fi film? I'd assume the latter as he's not a molecular biologist and neither am I, &lt;a href="https://www.lesswrong.com/posts/bc8Ssx5ys6zqu3eq9/diamondoid-bacteria-nanobots-deadly-threat-or-dead-end-a"&gt;and also nanotech people say no&lt;/a&gt;, but to focus on the specific there is to miss the point of the example: if you know all published science, and you have been given a sufficient budget by someone who thought you were their tool, and you have the motivation (will they?), you can find &lt;em&gt;something&lt;/em&gt; that will break all of us — natural selection created diseases that are beyond our ability to treat, to say an AI cannot make at least one example of that is not a conclusion you can reach simply by dismissing one off-the-cuff attempt to concretise the idea.&lt;/p&gt;

&lt;p&gt;Sometimes the people dismissing the risks will say some concrete example of how an AI might take over "sounds like magic". Unfortunately, in real life, magicians are people whose profession is to find ways of doing things that look impossible even while you are watching them closely. Also unfortunately, quite a lot of our technology looks like magic to anyone who doesn't understand it, and none of us is an expert at everything: you may understand photovoltaics at a quantum mechanical level, or the &lt;a href="https://en.wikipedia.org/w/index.php?title=Ethosuximide&amp;oldid=1230404205"&gt;pharmacokinetics of ethosuximide&lt;/a&gt;, but vanishingly few people understand both.&lt;/p&gt;

&lt;p&gt;Sometimes the metaphor of chess is used, that if I were to play Kasparov I know he would defeat me without knowing any single move he would take. But again, this leads to a dismissal that misses the point: "Chess is a discrete set of rules, not the open-ended rules of the real world!"&lt;/p&gt;

&lt;p&gt;The world is indeed open-ended — not only does this &lt;em&gt;not help&lt;/em&gt;, it &lt;em&gt;makes it worse&lt;/em&gt;. To fool me, all you need to do is say something &lt;em&gt;plausible&lt;/em&gt; about a domain I am not an expert in — a task at which even the first release of ChatGPT, with all its flaws, already excelled. To win an election in a democracy, all you need to do is to scale that up to about half the population, give or take your local electoral system. But you don't need to go that far or wait for the next election: to change the laws and policies and start wars you only need to be good at lobbying, so saying plausible things to the right people within the government.&lt;/p&gt;

&lt;p&gt;Don't misunderstand me: LLMs &lt;em&gt;as they currently are&lt;/em&gt; won't convince a general to change their strategy or a government economist to change the reserve bank's interest rate — most everyone also knows that LLMs at best only help experts with boring stuff while not being remotely good enough to replace humans in general — what I'm saying here is that the breadth and complexity of the real world means there's plenty of subjects that any one of us has not been doing professionally for a few years, and &lt;em&gt;that&lt;/em&gt; is the approximate (current) cross-over point of experience where it's us humans who make mistakes more easily than the AI. The more capable the AI, the easier it is for it to find a competency gap in the human population, and when (if) AI gets to the level of "PhD researcher" (&lt;a href="https://duckduckgo.com/?q=OpenAI+says+gpt5+will+have+PhD+level+intelligence&amp;ia=web"&gt;a specific goal&lt;/a&gt;, we shall see if it comes to pass) then it is necessarily doing things that no other human has learned (because that's the point of a PhD).&lt;/p&gt;

&lt;p&gt;A common, and I believe fair, point is expressed by a famous quote:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In theory, theory and practice are the same. In practice, they are not.&lt;/p&gt;
&lt;footer&gt;— &lt;a href="https://quoteinvestigator.com/2018/04/14/theory/"&gt;like all famous quotes this has been attributed to almost every person widely regarded as clever, but was probably originally from Benjamin Brewster, who I'd never heard of before writing this post&lt;/a&gt;&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;Or, to put it another way: an AI that learned everything from books, but doesn't have any real world experience, isn't going to be very effective in the real world.&lt;/p&gt;

&lt;p&gt;In so far as it goes, this is correct. But — of course there's a but — it's a failure of imagination to think this is the scenario under discussion.&lt;/p&gt;

&lt;p&gt;There's a few million Tesla cars on the road, and they have the potential to constantly perform experiments: break a little sooner, how do other vehicles respond. There's buttons under every ChatGPT response, for users to give real-world feedback on the models. There's dedicated research robots in labs, and AI which guide the experiments those robots perform. The entire web/app analytics industry is based around automation of testing real users. The entire field of machine learning is an outgrowth of statistics, which were themselves developed to help us model reality. There are a lot of new robot startups that (if they are not &lt;a href="https://en.wikipedia.org/w/index.php?title=Vaporware&amp;oldid=1235137243"&gt;vapourware&lt;/a&gt;) will provide even more real-world learning opportunities for AI than even the Tesla cars (though my gut feeling is that they mostly exist to copy Musk's announcement, and that few are likely to be genuinely useful).&lt;/p&gt;

&lt;p&gt;This also tends to tie-in with a dismissal of the idea that intelligence is sufficient, which a few times even leads to people pointing at orcas (no, really) &lt;em&gt;straight up asserting without justification that orcas are smarter than humans&lt;/em&gt;(!), and then asking why orcas have not taken over the world. As the orca argument has been given more than once, I guess I need to say this explicitly: even if orcas were smarter (prove it), orcas don't have opposable thumbs, and because they live in water they can't make fire to create ceramics or refine metals. Robots do not have these limits, orca-based arguments do not apply.&lt;/p&gt;

&lt;h2&gt;AI motivation&lt;/h2&gt;

&lt;p&gt;Many people also have a hard time understanding why an AI might even want to take over. Aside from the fact that they don't need to, &lt;a href="https://en.wikipedia.org/w/index.php?title=Peter_principle&amp;oldid=1238424437"&gt;they just need to be promoted to their level of incompetence like any normal human&lt;/a&gt;, I think this is one of the better arguments… or it could be if phrased right: many smart people don't care about ultimate power, just about having enough to be happy.&lt;/p&gt;

&lt;p&gt;Unfortunately, the actual arguments by those un-worried by AI don't go like that; instead they more often point to the most powerful people in the world and dismiss their intellect. One problem with this is that most of us tend to dismiss the intellect of leaders (and groups) we don't approve of while overestimating those we like — I'm seeing this at the moment with Trump's IQ being 73 in the mouths of his enemies and 156 in the mouths of his allies, despite neither having a published test to justify the claim. I'm highly confident that neither extreme is true, which is the point.&lt;/p&gt;

&lt;p&gt;A second point, which is as much of a problem for the original claim as for the counter argument, is that there's a small number of leadership posts compared to the number of people who seek to fill them — you therefore need more than &lt;em&gt;just&lt;/em&gt; high intelligence, you also need resources and luck.&lt;/p&gt;

&lt;p&gt;A brief aside from motivation to resources: an AI planning to take over quickly, could for example be able to get a lot of resources quickly by straight up stealing/hacking those resources. In the present world, that looks more like bitcoin-enabled blackmail or password sniffing than &lt;a href="https://arxiv.org/pdf/2007.01114"&gt;suborning insecure industrial equipment&lt;/a&gt; — but the world is rapidly changing, and &lt;a href="../03/23-17.24.34.html"&gt;the only thing I can be confident claiming about 2034&lt;/a&gt; is that it won't look like today. For example, if Musk were to actually sell those humanoid robots for $10k each, and — just as our society was transformed to fit around cars and later smartphones — our society was shaped to assume and then require each family had one… and then they all got hacked into on the same day (&lt;a href="https://en.wikipedia.org/w/index.php?title=2024_CrowdStrike_incident&amp;oldid=1239550852"&gt;don't say that won't happen, critical software SNAFUs are as common as Tsunamis&lt;/a&gt;). And if the hypothetical AI that is doing this, is the AI Musk is trying to have built, then it's likely to not even need a hack to gain control: a simple software update, like the ones Tesla already uses for the car's self-drive AI, is sufficient.&lt;/p&gt;

&lt;p&gt;But, motivation? The fear amongst safety researchers is predominantly "instrumental goals": goals which are useful no matter what else you're doing. Being powerful is one of them. It's not that an AI specifically wants power for its own right, it's just that power gets you basically everything else: Do you want to end the hunting of whales? Better have an navy to be able to sink the whaling ships; Do you want all the stamps? You need the power to send in the police to search and seize the private collections that weren't for sale; &lt;a href="https://en.wikipedia.org/wiki/Bhopal_disaster"&gt;Do you want to build a chemical plant to create a pesticide by reacting methylamine with phosgene to produce methyl isocyanate that is then reacted with 1-naphthol to produce the end product&lt;/a&gt;? Oh, but what to do about all those protestors…&lt;/p&gt;

&lt;p&gt;I don't seek power. But perhaps I should, for without power I can't &lt;a href="https://en.wikipedia.org/wiki/Stop_the_War_Coalition"&gt;Stop the War&lt;/a&gt; because nobody starting one will listen to me; without power, I can't &lt;a href="https://www.wfp.org/ending-hunger"&gt;end world hunger&lt;/a&gt; because I can't order the creation of the roads to deliver the food, much less ship the food itself; I can't eradicate &lt;a href="https://en.wikipedia.org/wiki/Eradication_of_infectious_diseases"&gt;even one disease&lt;/a&gt; because I don't have ultimate control over world-wide healthcare.&lt;/p&gt;

&lt;p&gt;But an AI doesn't need such grand goals to benefit from power-seeking behaviour: if a board of directors wants to save money by using an AI instead of a human CEO, that only works if the AI seeks to do things that accord with the goals of the directors — for the most famous brands, this is necessarily "make ever increasing profits" (though this time the anthropic principle cautions that corporations without this are the ones you've never heard of, and you've not heard of most corporation) and thus an AI will be put in charge of all manner of organisations as soon as it &lt;em&gt;looks like&lt;/em&gt; it's good enough… which isn't the same as when it &lt;em&gt;actually is&lt;/em&gt; good enough.&lt;/p&gt;

&lt;p&gt;But we don't see power-seeking behaviour with ChatGPT at least, so it's fair to ask "why not?" — The answer is that it has been trained to not be like that. Not only is it limited in scope to "assistant", the people behind it &lt;em&gt;actively don't want it to seek power and included this in their safety analysis&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;More specifically, power-seeking is optimal for most reward functions and many types of agents;[69, 70, 71] and there is evidence that existing models can identify power-seeking as an instrumentally useful strategy.[29] We are thus particularly interested in evaluating power-seeking behaviour due to the high risks it could present.[72, 73]&lt;/p&gt;
&lt;footer&gt;— &lt;a href="https://cdn.openai.com/papers/gpt-4-system-card.pdf"&gt;GPT-4 System Card  OpenAI March 23, 2023, page 15&lt;/a&gt;&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nothing about the future requires that.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#AGI'&gt;AGI&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Artificial intelligence
'&gt;Artificial intelligence&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#black swan'&gt;black swan&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Futurology'&gt;Futurology&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#machine learning'&gt;machine learning&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#misaligned incentives'&gt;misaligned incentives&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#outside context problem'&gt;outside context problem&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#paperclip optimiser'&gt;paperclip optimiser&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Technological Singularity'&gt;Technological Singularity&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#x-risk'&gt;x-risk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#AI'&gt;AI&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Futurology'&gt;Futurology&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Sat, 10 Aug 2024 13:36:05 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/08/10-13.36.05.html</guid>
    </item>
    <item>
      <title>Food-based coding metaphors</title>
      <link>https://benwheatley.github.io/blog/2024/07/13-17.10.31.html</link>
      <description>&lt;h1&gt;Food-based coding metaphors&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Spaghetti code&lt;/strong&gt; is a term of disparagement in software development, used to describe a code that's tangled and unstructured — much like a plate of spaghetti. This kind of code is difficult to follow, understand, and maintain.&lt;/p&gt;

&lt;p&gt;This made me wonder what other food-based metaphors there might be for code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lasagne code&lt;/strong&gt; would naturally mean "layered and structured", the kind of thing to which we all aspire, where each part has a clear and well-defined form and location, and they are bound together in a predictable fashion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fusilli code&lt;/strong&gt;, which has all the problems of spaghetti code, except now each thread is shorter and it's up to you to figure out how they go together. This is how I have come to see "Clean Code":&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The first rule of functions is that they should be small. The second rule of functions is that they should be smaller than that.&lt;/p&gt;
&lt;footer&gt;— Robert C. Martin&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;Specifically, aim for less than 10 lines, and "as few arguments as possible, ideally none".&lt;/p&gt;

&lt;p&gt;I am not a fan of this. There is no single ideal size for a function, it can be one line or one thousand. In fact, I've seen code which genuinely had a thousand(!) lines inside an &lt;code&gt;if {}&lt;/code&gt; block, and the code was easier to read in that form than it would have been if the &lt;code&gt;if&lt;/code&gt; had been split up this way. Don't get me wrong, that thousand line &lt;code&gt;if {}&lt;/code&gt; block was still really bad code for other reasons, it's just that splitting it up this way would have made it worse, not better, as it would have become ten lines where each was a call to a function that was itself ten lines that were each function calls to a third layer of ten-line functions that, honestly, quite a few of those were themselves also function calls.&lt;/p&gt;

&lt;p&gt;And if your functions have no arguments, they must be acting purely by side-effects, which makes it harder to figure out what they do and which other parts of the codebase they influence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alphabetti-spaghetti code&lt;/strong&gt; sounds like a good name for anything involving a RegEx.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;[+-]?(\d+(\.\d*)?|\.\d+)([eE][+-]?\d+)?&lt;/code&gt;&lt;/p&gt;
&lt;footer&gt;— Regular expression, &lt;cite&gt;&lt;a href="https://en.wikipedia.org/w/index.php?title=Regular_expression&amp;oldid=1230862099"&gt;Wikipedia&lt;/a&gt;&lt;/cite&gt;&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Mystery sausage code&lt;/strong&gt; would be a good name for neural networks: a mixture of unknown origin whose contents are homogenised to the point of being indeterminable. You're never quite sure what goes into them, you're never sure if they're safe, and if you're not careful about your suppliers you might find you're being fed one even when you were expecting something of higher quality.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spicy code&lt;/strong&gt; covers up an incredibly basic rote task with a fancy &lt;code&gt;import&lt;/code&gt;ed extra, akin to how the difference between making Mexican and Indian cuisine can be as little as "which spice mix did I add to my batch of onion, tomato, and peppers, which I then serve with rice?"&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Traditional home-cooked organic code&lt;/strong&gt; has to be FOSS in general. Proponents tell you how much better it is than the factory stuff, or how this gives you control over what you're consuming, but (to the horror of those who love it) most people not only genuinely don't care, they even find this somewhat pretentious.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Coca-coda&lt;/strong&gt; — just as soda is almost entirely synthetic, not recommended by dietary experts, and yet still one of the most popular drinks on the planet, so too is AI-generated code almost entirely synthetic, disfavoured by experts, and extremely popular.&lt;/p&gt;

&lt;figure style="max-width: 392px;"&gt;&lt;img src="Screenshot 2024-07-13 at 18.35.17.png" style="aspect-ratio: 784/558; width: 100%;" /&gt;&lt;figcaption&gt;This AI-generated image won't put you off AI-generated code any more than the real ingredients list will put you off any real brand of soda.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Allotment code&lt;/strong&gt; is where the consumer is also the creator, and they do it more for fun than anything else — personal projects, like everything on my own GitHub page.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fast code&lt;/strong&gt; is where the developer wasn't paid to care, just to shovel out as much as possible as fast as possible for as little money as possible. May involve &lt;strong&gt;Coca-coda&lt;/strong&gt; and &lt;strong&gt;Spicy code&lt;/strong&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#Coding'&gt;Coding&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Opinion'&gt;Opinion&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Programming'&gt;Programming&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#software engineers'&gt;software engineers&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Opinion'&gt;Opinion&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Professional'&gt;Professional&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Software'&gt;Software&lt;/a&gt;
&lt;/p&gt;
</description>
      <pubDate>Sat, 13 Jul 2024 17:10:31 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/07/13-17.10.31.html</guid>
    </item>
    <item>
      <title>Crypto-[graphy|currency]: two cultures</title>
      <link>https://benwheatley.github.io/blog/2024/05/25-12.04.31.html</link>
      <description>&lt;h1&gt;Crypto-[graphy|currency]: two cultures&lt;/h1&gt;

&lt;p&gt;Cryptography and cryptocurrency. The arguments over each seem be two sides of the same coin, mirror-images of each other that misunderstand the reality when proposing solutions. Governments who find it unacceptable they cannot pierce the cryptography that keeps messages secure, and crypto-currency proponents find it unacceptable that governments can decide to devalue their money by printing more, or that they have to trust banks to not close their accounts. I've seen a lot of news stories (and arguments) about both over the last few years.&lt;p&gt;

&lt;p&gt;The split for both is "code-of-law" vs. "code-as-law".&lt;/p&gt;

&lt;p&gt;"Code of law" is how humans think, even when the words themselves are arcane: laws are written by humans, read by humans, interpreted by humans, enforced by humans; and judgments and appeals are likewise human. All these parts know that humans are flawed, and have soft edges accordingly — it's physically possible to do bad stuff, but you can be caught and punished and financial harms can be undone.&lt;/p&gt;

&lt;p&gt;"Code as law" is different. Computer code is only in the first instance written by humans, but that human on the keyboard is giving instructions to a compiler rather than a judge, and the compiler has a degree of pedantry that lawyers would consider absurd: "Oh, this is a number, sure, but it's only integers in the range -32,768 to +32,767, and if you add or multiply two of these numbers and get a number outside that range &lt;em&gt;it doesn't work&lt;/em&gt;"; or "You wrote 'or' but you meant 'either … or …', well you should've been more careful because I just did what I was told and not what you meant; etc. — and it remains at this level of pedantry with every other system that code interacts with.&lt;/p&gt;

&lt;p&gt;This means that people who are used to the way laws work, speak as if it's fine to just "tell a computer that the police wants these private messages" — because that actually works when it's humans doing things: when someone impersonates a police officer, that's a crime, whatever they did can be found out and undone. &lt;em&gt;Do this on a computer and you lose everything, forever.&lt;/em&gt; When a single person can only impersonate one police officer at a time, the damage is limited, &lt;em&gt;but a hacker can say "I'm a police officer" on the order of a million distinct requests in one second, even when using a home internet connection&lt;/em&gt;. If you just have a simple convenient way for a human to ask for private messages because they're the police, you &lt;em&gt;also&lt;/em&gt; have a simple convenient way for hackers to do the same, and it's extremely difficult to secure a private message system at the best of times, much more so when you have to provide a mechanism for the police to gain access when they say they need it. And, crucially, that's still the case no matter how right the police are about needing it to do their job.&lt;/p&gt;

&lt;p&gt;The other way around, cryptocurrencies are all based on the idea that it is desirable to remove all this pesky "trust" from the banking system — "Why simply 'trust' that the bank won't go under, or that the government won't choose hyperinflation, or that you won't be forced to pay money directly from your account when you don't want to, when you can secure your money with the exact same unbreakable encryption that the government wants to remove from private messages?" — and the answer is: that's because of humans, and laws.&lt;/p&gt;

&lt;p&gt;Money is for humans, not for simply sending from one account to another, and humans are flawed, we make mistakes, we can be defrauded, we can be mis-sold, we can buy things on credit that were not as advertised. Turning human judgement into the kind of pedantry needed for it to become a computer program, only works in extremely simple cases; this is why it took so long to get chatbots which were any good, and also why they frequently give us terrible advice.&lt;/p&gt;

&lt;p&gt;When you've only ever used a hammer and a nail, it's confusing to be told screws don't work that way; and when you've only ever used a screw and a screwdriver, nails look stupid too. Laws and cryptography, both trying to put legal limits on cryptograph and using cryptography as a replacement for law, are the same kind of mis-match.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#bitcoin'&gt;bitcoin&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#cryptocurrency'&gt;cryptocurrency&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#cryptography'&gt;cryptography&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#economics'&gt;economics&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Politics'&gt;Politics&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Technocracy'&gt;Technocracy&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#cryptocurrency'&gt;cryptocurrency&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#cryptography'&gt;cryptography&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Politics'&gt;Politics&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Sat, 25 May 2024 12:04:31 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/05/25-12.04.31.html</guid>
    </item>
    <item>
      <title>Unthinkable thoughts</title>
      <link>https://benwheatley.github.io/blog/2024/04/30-13.54.02.html</link>
      <description>&lt;h1&gt;Unthinkable thoughts&lt;/h1&gt;

&lt;p&gt;The idea of unthinkable thoughts — thoughts that some mind, for example my own, is literally incapable of thinking — has intrigued me for a while.&lt;/p&gt;

&lt;p&gt;I'm not the first to wonder about this: it is the core of the &lt;a href="https://en.wikipedia.org/wiki/Linguistic_relativity"&gt;Sapir–Whorf hypothesis&lt;/a&gt;, it is (as I understand it without having read the book) how &lt;a href="https://en.wikipedia.org/wiki/Newspeak"&gt;Newspeak&lt;/a&gt; is supposed to function in &lt;a href="https://en.wikipedia.org/wiki/Nineteen_Eighty-Four"&gt;Nineteen Eighty-Four&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Large language models probably aren't exactly how human minds word, but they do have the advantage that we can study them and mess with them in ways that are impossible in humans (also unethical; at the time of writing, most people think LLMs have no inner experience, no qualia, so it's fine to do whatever we want with them. I hope this belief is correct). As I understand it, ever since &lt;a href="https://en.wikipedia.org/wiki/Word2vec"&gt;word2vec&lt;/a&gt;, all the interesting AI language models represent meaning as a high-dimensional space, e.g. 300 dimensions, where one dimension represents gender, one represents weight, another represents redness, one more temporal ordering, etc.&lt;/p&gt;

&lt;p&gt;If the number of dimensions is too small, you can't distinguish concepts. You can literally see this with a map — Google Maps' street view will sometimes get confused when you go though a bridge, the image from the top road and the image from the bottom road having the same latitude and longitude. You can see it in black-and-white images (each pixel is one dimension, whereas full color images have three dimensions per pixel), where a cloudy sky and green grass can be the same brightness. You can see it in basically all statistics, where someone tells you the "average" but doesn't tell you the distribution (the average human is a &lt;a href="https://web.archive.org/web/20220813034105/https://www.sbs.com.au/news/article/the-most-typical-person-in-the-world/ybqm2l637"&gt;Mandarin speaking 28 year old male&lt;/a&gt; with one testicle and one ovary, and who &lt;a href="https://en.wikipedia.org/wiki/Valeriepieris_circle"&gt;lives within 3,300 kilometers&lt;/a&gt; of Mong Khet in Myanmar).&lt;/p&gt;

&lt;p&gt;Even though I expect my mind to function very differently than an LLM, I do still expect to have some limit on the number of different dimensions my mind can represent. Although I know I must have some limit, I have no idea what this limit is (or "these limits" plural, as I suspect the limits of our minds varies depending on the modality and which level of abstraction you're considering, which makes it hard to turn this vague statement into a precisely testable claim), and therefore have to allow for both (1) the possibility that the limit is purely theoretical as humans don't live long enough to reach it, and (2) the limit is really small, and we encounter this "cognitive blind spot" in much the same way we experience our &lt;em&gt;literal&lt;/em&gt; blind spot (or indeed &lt;a href="https://en.wikipedia.org/wiki/Change_blindness"&gt;change blindness&lt;/a&gt; and the even more terrifying &lt;a href="https://en.wikipedia.org/wiki/Introspection_illusion#Choice_blindness"&gt;choice blindness&lt;/a&gt;): we ignore it and don't even notice we're ignoring it 99.99% of the time.&lt;/p&gt;

&lt;p&gt;But even with that, there is something worth exploring in this. When we encounter something, or someone, new, it takes a while before we develop a nuanced model of them, and those nuances are the extra dimensions in our inner representation of that thing. It's also hard to imagine ourselves in a state of ignorance, without that nuance. Both of these effects can be seen in the discussion about "is AI capable of art?", where artists can go red in the face pointing out the flaws in the result (separately from any claim about copyright or putting people out of work), yet the models remain popular because &lt;em&gt;most people aren't even looking for those details&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;One example in particular comes to my mind with this dimension-squishing, that of gender. I'm sure you've seen (perhaps you've even made) the argument that gender is "just" the chromosomes, XX or XY, that it's "basic biology", or that it is which genitals you're born with. I've tried pointing out both that the medical profession both accepts transgender as a status and doesn't think there's anything "basic" about biology, and I've also tried pointing out the existence of &lt;a href="https://en.wikipedia.org/wiki/Category:Intersex_variations"&gt;other chromosomal combinations (of which there are many in humans)&lt;/a&gt; — regardless of your thoughts on this, I suspect you can recognise that what I've said never seems to convince anyone, and the responses have always been to double-down on the original claim.&lt;/p&gt;

&lt;p&gt;But we also don't realise how bad this kind of dimension reduction is. One example of this is horoscopes: dividing the whole world into 12 groups and trying to make any kind of meaningful statement about how the day will go for each entire group, in 50 words or less, &lt;em&gt;should&lt;/em&gt; be obviously a non-starter… and yet, horoscopes are still very popular.&lt;/p&gt;

&lt;p&gt;Another is politics. I don't know how many parties there are where you live, but I grew up in &lt;a href="https://en.wikipedia.org/wiki/Hampshire"&gt;(old) Hampshire, on the south coast of the UK&lt;/a&gt;, where the political options were mostly the &lt;a href="https://en.wikipedia.org/wiki/Conservative_Party_(UK)"&gt;Blue party&lt;/a&gt;, with a small hint of &lt;a href="https://en.wikipedia.org/wiki/Labour_Party_(UK)"&gt;Red&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Liberal_Democrats_(UK)"&gt;Yellow&lt;/a&gt; followed by all the rest. Sure, those parties have real names, but do those names matter? The "conservative" party doesn't keep things the same, the "labour" party isn't all that focused on labour. Are the parties in your area also like this, or do yours actually represent the values in their own names? Do you even know the policies of whichever party you support, or do you support them because their people make you feel good? &lt;a href="https://en.wikipedia.org/wiki/Boris_Johnson"&gt;I'm fairly sure "feeling good" is how the UK got into such&lt;/a&gt; a &lt;a href="https://en.wikipedia.org/wiki/Eton_mess"&gt;mess&lt;/a&gt;, and why the US is struggling to answer what should be a very easy question, &lt;a href="https://en.wikipedia.org/wiki/Indictments_against_Donald_Trump"&gt;"do we want to send someone to the Oval Office when they're facing 88 felony charges in four separate cases, one of which was mishandling of classified documents, two of which were about unlawfully attempting to overthrow an election, and the fourth of which was falsifying business records about hush money during his first election campaign?"&lt;/a&gt;, which is only even a question because he's more charismatic than the other guy, and keeps promising simple solutions to complex realities.&lt;/p&gt;

&lt;p&gt;(I could also say something here about Israel/Palestine, but I'm not masochistic enough to wade into that particular discussion, except to say that one could do a degree in this and still not be able to fully grasp it — if it was a topic easy enough to fit into a degree, there would have been a lasting peaceful solution &lt;a href="https://en.wikipedia.org/wiki/Oslo_Accords"&gt;decades ago&lt;/a&gt;, just like there is now peace between Israel and Egypt).&lt;/p&gt;

&lt;p&gt;Back to AI, and the comparison with the human mind: I wrote earlier that, based on what I know at the moment, it seems equally possible for the human limit to be too big to matter, or extremely small and we just delude ourselves (see all the examples, here and elsewhere, of the H. L. Mencken quote: "For every complex problem there is an answer that is clear, simple and wrong."); now, if the the maximum dimensionality of human thought is huge, then there's probably a long time before AI can take over. If the maximum human dimensionality is small, then AI can very easily confuse (or work around) any single human, by having the space to represent two different concepts which seem the same to us — the same way &lt;a href="https://en.wikipedia.org/wiki/Dog_anatomy#Senses"&gt;dogs can't see red balls on green grass because they have two kinds of color sensor in their retina, rather than the three in a human retina&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One may point out that &lt;em&gt;a&lt;/em&gt; human is different from &lt;em&gt;all humans&lt;/em&gt;: we clearly do (and therefore can) learn different models of the world, and therefore it may seem reasonable to suggest that any given AI may outfox any single one of us without necessarily being able to outfox all of us — but even then, Alpha Go demonstrated that it could find things in the game-space of Go that no human had ever thought of.&lt;/p&gt;

&lt;p&gt;I know how to spot my literal blind-spot, but it takes a deliberate effort; How might I learn to find a cognitive blind-spot? How do I learn the shape of the thoughts that I cannot think?&lt;/p&gt;

&lt;p&gt;I also want our future AI to be &lt;em&gt;literally incapable&lt;/em&gt; of wanting to harm humanity, akin to in software "Make invalid states unrepresentable". But, an AI which &lt;em&gt;cannot distinguish good from evil&lt;/em&gt; is definitionally &lt;a href="https://en.wikipedia.org/wiki/Machiavellianism_(psychology)"&gt;Machiavellian&lt;/a&gt;; and if a model &lt;em&gt;can&lt;/em&gt; distinguish them, and if the model is distributed, then it's almost trivial to find and flip that specific axis. But I did say "and if the model is distributed": if it isn't distributed, then the "switch" can be kept out of sight and away from fiddling hands.&lt;/p&gt;

&lt;p&gt;There is another possible safety mechanism, which is at least worth considering: if all possible moral issues are squashed down into a single dimension (&lt;a href="https://benwheatley.github.io/blog/2019/05/25-15.09.10.html"&gt;something I've previously noticed humans did, but only by going past it&lt;/a&gt;) then anyone with a copy of distributed model will only have one single good-evil axis to modify — they would still be able to make a demonically evil moustache-twirling villain, but they wouldn't be able to accidentally destroy the world by turning off all the dials except the one saying "maximise shareholder value by maximising the number of paperclips", nor would they be able to run a dictatorship by saying "I am the only person who matters, everyone else is my slave".&lt;/p&gt;

&lt;p&gt;The downside, and yes there is a downside, is that this will be the morality of whoever makes the model, and basically everyone disagrees on what "good" is. For example, even limiting the world to those who follow the Ten Commandments, "thou shalt not kill" is often regarded as so incredibly obvious that it shouldn't need to be stated, and yet that specifically has sometimes included and sometimes excluded each of: meat, abortion, war, the death penalty, and suicide — can any of us truly imagine how much broader the range and depth of disagreement is in aggregate, or is this, too, an unthinkable thought?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#brains'&gt;brains&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#cognitive bias'&gt;cognitive bias&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#machine learning'&gt;machine learning&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Minds'&gt;Minds&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Philosophy'&gt;Philosophy&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Psychology'&gt;Psychology&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#questions'&gt;questions&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#rationality'&gt;rationality&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#reasoning'&gt;reasoning&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#theory of mind'&gt;theory of mind&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#thought'&gt;thought&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#word2vec'&gt;word2vec&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#AI'&gt;AI&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Minds'&gt;Minds&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Philosophy'&gt;Philosophy&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Psychology'&gt;Psychology&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Tue, 30 Apr 2024 13:54:02 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/04/30-13.54.02.html</guid>
    </item>
    <item>
      <title>Binary humans, nuanced AI</title>
      <link>https://benwheatley.github.io/blog/2024/04/29-14.44.44.html</link>
      <description>&lt;h1&gt;Binary humans, nuanced AI&lt;/h1&gt;

&lt;p&gt;For my whole life, the meme was that computers had to be literal, and that only humans could be subtle and nuanced. It is therefore somewhat ironic (or, to go back to the ancient Greek origin of the word "irony", &lt;a href="https://en.wikipedia.org/wiki/Alazon"&gt;Alazṓnic&lt;/a&gt;) that now we have an AI so good that &lt;a href='
https://www.google.com/search?client=safari&amp;rls=en&amp;q="was+this+written+by+chatgpt?"'&gt;we're becoming increasingly unsure if the words we see were written by human or machine&lt;/a&gt;, that the question of AI &lt;em&gt;safety&lt;/em&gt; is one where so many people have so much less nuance than even the existing large language models.&lt;/p&gt;

&lt;p&gt;"Is AI safe?" isn't even a well-formed question when asked in isolation. Indeed, for all X, "[is/are] ${X} safe?" are not well-formed questions: "Are cars safe?" depends on the car, the driver, the roads themselves, and the other road users. "Are planes safe?" depends on the plane, the pilot, the weather, the maintenance, if you're flying too close to a warship where either you or they have the wrong kind of IFF system, and if you get hijacked. "Are chemicals safe?" depends on the chemical (&lt;a href="https://en.wikipedia.org/wiki/Hydrogen_peroxide"&gt;water is one atom away from being bleach!&lt;/a&gt;), the temperature, how it's being handled, and &lt;a href="https://en.wikipedia.org/wiki/Water_intoxication"&gt;how much there is&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And so it is with AI: "Is AI safe?", a simple chess player on your phone, in isolation, won't end the world. Both the USSR and the USA had automated early warning radar systems that, during the cold war, reported a first strike attack in progress due to the
&lt;a href="https://en.wikipedia.org/wiki/1983_Soviet_nuclear_false_alarm_incident"&gt;the sun&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Thule_Site_J"&gt;the moon&lt;/a&gt; respectively, and only human intervention in each of these cases prevented these systems from ending the world.&lt;/p&gt;

&lt;p&gt;But even if I reduce this from &lt;em&gt;AI in general&lt;/em&gt; to &lt;em&gt;just large language models like ChatGPT etc.&lt;/em&gt;, it's still not clear. Why? Because, even after a year and a half in the public eye (and longer in the eyes of nerds like me), &lt;a href="https://www.strangeloopcanon.com/p/what-can-llms-never-do"&gt;nobody really knows&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=40179232"&gt;the limits of LLM capabilities&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At one level, they look safe: we can say they are just playing a game of 'what word comes next?'&lt;/p&gt;

&lt;p&gt;But they're good enough at this game that people are often tempted to use them as a control mechanism for something else, &lt;a href="https://youtu.be/Sq1QZB5baNw?si=pFedH4QaANKigZw5"&gt;such&lt;/a&gt; as &lt;a href="https://youtu.be/djzOBZUFzTw?si=jtF4Z-dZ50_3H27_"&gt;robots&lt;/a&gt;, or just blindly use the results without bothering with human oversight even when they have liability for errors, like &lt;a href="https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/"&gt;those two New York lawyers who got in the news (and in trouble) for letting ChatGPT do their job for them&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(As an aside: some people suggest that LLMs are harmful because they "allow students to cheat in exams", which I find as ridiculous as saying "calculators are harmful, you won't have one on you at all times when you're an adult" or &lt;a href="https://en.wikipedia.org/wiki/Phaedrus_(dialogue)#Discussion_of_rhetoric_and_writing_(257c–279c)"&gt;"writing is harmful, you won't bother remembering things if you wrote them down"&lt;/a&gt; — whenever an automation is good enough to use to cheat on a test, the thing being tested has necessarily transitioned from 'something that we must teach our kids' to 'something you can learn for fun, if you like').&lt;/p&gt;

&lt;p&gt;So, LLMs definitely have &lt;em&gt;at least&lt;/em&gt; as much potential for danger as any other form of automation, simply because they are being used for general automation — claiming that any LLM is "risk free" is a bit like saying "we can make bug-free software"… except even worse, as the current state of the art for LLMs means that really comes with the caveat "and we manage that even though we only hire interns".&lt;/p&gt;

&lt;p&gt;There are further forms of harm besides this. All technology, without exception, is about enabling things that were previously too hard or too expensive. There's a lot of things out there which have already been invented, but where knowledge of the invention isn't widespread — ever since &lt;a href="https://en.wikipedia.org/wiki/Word_Lens"&gt;Word Lens was released at the end of 2010&lt;/a&gt;, I've been surprising and delighting people with the fact that our phones can give us live, augmented reality, translations of whatever our phone camera can see. Even in 2024, I'm regularly meeting people who &lt;em&gt;don't know this is built into Google Translate&lt;/em&gt; (these days, the tech is also built into the iOS photo app). But for dangerous things? The 9/11 attacks happened just as I was reaching adulthood, and my generation got to witness a bunch of really dumb attempts to cause harm. Imagine if &lt;a href="https://en.wikipedia.org/wiki/Richard_Reid"&gt;the Shoe Bomber&lt;/a&gt; and his co-conspirators had been able to even just &lt;em&gt;brainstorm ways their attack could go wrong&lt;/em&gt; — an LLM is "just" next-word-prediction, it "only" knows things it finds on the public internet, what can go wrong?&lt;/p&gt;

&lt;p&gt;And even that is a simple case. AI can explain research papers and help write and debug code, so someone mediocre — not a total zero, but still a junior — can take existing work, and get help from an LLM to turn it into something much more impressive (after all, that's the point of automation: you don't need to be a master woodworker to use a CNC machine). How can an LLM be used this way to cause harm?&lt;/p&gt;

&lt;blockquote cite="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/"&gt;By going as close as we dared, we have still crossed a grey moral boundary, demonstrating that designing virtual potential toxic molecules is possible without much effort, time or computational resources. We can easily erase the thousands of molecules we created, but we cannot delete the knowledge of how to recreate them.&lt;footer&gt;—&lt;cite&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/"&gt;Urbina F, Lentzos F, Invernizzi C, Ekins S. Dual Use of Artificial Intelligence-powered Drug Discovery. Nat Mach Intell. 2022 Mar;4(3):189-191. doi: 10.1038/s42256-022-00465-9. Epub 2022 Mar 7. PMID: 36211133; PMCID: PMC9544280.&lt;/a&gt;&lt;/cite&gt;&lt;/footer&gt;&lt;/blockquote&gt;

&lt;p&gt;LLMs make this kind of work easier to reproduce. And still do even with the current attempts at alignment, because the thing which scared Fabio Urbina et al. was, in layman's terms, &lt;em&gt;flipping a switch between good and evil&lt;/em&gt; (such a flip can also happen &lt;em&gt;accidentally&lt;/em&gt;, in &lt;a href="https://en.wikipedia.org/wiki/Waluigi_effect"&gt;more than&lt;/a&gt; &lt;a href="https://forum.effectivealtruism.org/posts/5mADSy8tNwtsmT3KG/the-true-story-of-how-gpt-2-became-maximally-lewd-1"&gt;one way&lt;/a&gt;). Downloadable weights can be easily modified, but even models kept locked behind an API can be manipulated to some extent — the API-only access for which OpenAI is getting criticised is not a silver bullet, but it is better than nothing… but with a caveat that research into some of the &lt;em&gt;many&lt;/em&gt; open questions on the subject of AI safety have been enabled precisely by the downloadable models which are so easy to manipulate.&lt;/p&gt;

&lt;p&gt;But all of this is "small scale" danger and harm. Yes, I really am describing the potential for novel toxic chemicals as "small danger". Why? Because of how much worse it can get. &lt;em&gt;When&lt;/em&gt; AI leads to some disaster, what I expect the disaster to be self-limited by the scope of whatever the AI was being asked to do. The large scale discrete incidents from failures of &lt;em&gt;human&lt;/em&gt; cognition have been &lt;a href="https://en.wikipedia.org/wiki/Bhopal_disaster"&gt;the Bhopal gas tragedy&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Chernobyl_disaster"&gt;the Chernobyl disaster&lt;/a&gt;. I expect more of that, not the setup for &lt;a href="https://en.wikipedia.org/wiki/The_Terminator"&gt;The Terminator&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Universal_Paperclips"&gt;Universal Paperclips&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, you may be appalled by all these scenarios, and that's fair. By the standard of AI safety researchers, the fact that I think LLMs won't get outcomes worse than Bhopal and Chernobyl &lt;em&gt;makes me an optimist&lt;/em&gt;; and given how easy it is to re-align an existing model — to find and flip it's "evil" switch (not that whoever is doing this would describe their own actions that way as we're all the heroes of our own stories) — then I do foresee people downloading models, making these changes, using them to control industrial hardware… and that hardware literally exploding in their own face.&lt;/p&gt;

&lt;p&gt;The more severe position, is that we will get AI with agency, which plot to take over the world for whatever reason. This is certainly a thing to keep an eye out for (it is very easy to give an LLM agency), but &lt;em&gt;right now&lt;/em&gt; those LLMs run into situations they can't cope with — and even if they could cope with anything, &lt;em&gt;right now&lt;/em&gt; humanity collectively can out-think AI, both in terms of diversity of thought (if you've met one LLM you've met them all, even though they're good at playing many roles), learning from few examples (LLMs make up for needing loads of examples by reading basically everything very quickly), and gross throughput (there's a lot of humans, while computer hardware is currently a limiting factor).&lt;/p&gt;

&lt;p&gt;All of those "defences" are temporary or partial. Evolution designed our brain to be both power-efficient and to get a long way from a few examples, and evolutionary algorithms are very easy to do in a computer. And we're building more hardware as fast as possible. And they're only partial, because there's a non-zero risk of an AI only needing to be really good at one thing to cause problems, and also a non-zero risk of us being explicitly told about a long-term danger and always picking the short-term reward (say, a re-run of the last century of CO2 emissions, which we did knowingly because we wanted what the power stations could give us more than we cared about the ice caps melting).&lt;/p&gt;

&lt;p&gt;I think the challenges in these areas buy us 5-15 years to answer a question that has haunted our species for at least as long as we've had writing, and probably longer:&lt;/p&gt;

&lt;blockquote&gt;&lt;a href="https://en.wikipedia.org/wiki/History_of_ethics"&gt;What does "good" mean?&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;Now, if only I could convince people it wasn't obvious when they didn't already believe that. So much online discussion is &lt;a href="https://www.youtube.com/watch?v=R13BD8qKeTg"&gt;P(doom) = 0 or P(doom) = 1&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Artificial intelligence'&gt;Artificial intelligence&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Bayesian'&gt;Bayesian&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#ethics'&gt;ethics&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#future of humanity'&gt;future of humanity&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#misaligned incentives'&gt;misaligned incentives&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#outside context problem'&gt;outside context problem&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#paperclip optimiser'&gt;paperclip optimiser&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#rationality'&gt;rationality&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#reasoning'&gt;reasoning&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Technological Singularity'&gt;Technological Singularity&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#AI'&gt;AI&lt;/a&gt;
&lt;/p&gt;
</description>
      <pubDate>Mon, 29 Apr 2024 14:44:44 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/04/29-14.44.44.html</guid>
    </item>
    <item>
      <title>Spendable votes</title>
      <link>https://benwheatley.github.io/blog/2024/04/24-10.42.51.html</link>
      <description>&lt;h1&gt;Spendable votes&lt;/h1&gt;

&lt;h2&gt;The problem&lt;/h2&gt;

&lt;p&gt;Democracies come in many forms, but all have at least one of these problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Your vote only really makes a difference when it's close — if you're in a constituency which always votes 70-80% for the Beige party, any vote will be wasted regardless of if you prefer Beige, Taupe, or Ecru.&lt;/li&gt;
&lt;li&gt;Power is discrete: the winner retains full power until they are removed. Removal is either on a fixed schedule, or at the winner's preferred time, or a forced removal due to e.g. impeachment&lt;/li&gt;
&lt;li&gt;Nuance is hard: any given voter cannot say "I like these two teams equally, and want them to share power, but absolutely never that other team"&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;The idea&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;At any given election, all voters are given some number of votes, and each is free to distribute their votes as desired amongst all the candidates.&lt;/li&gt;
&lt;li&gt;Each standing politician has their votes totalled up, and added to a ledger.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;All&lt;/em&gt; standing politicians are then entitled to participate in parliament/congress/etc.&lt;/li&gt;
&lt;li&gt;When legislation etc. is voted on, each politician may &lt;em&gt;spend&lt;/em&gt; the votes they received. This spending is recorded on the ledger, and reduces the available for subsequent votes.&lt;/li&gt;
&lt;li&gt;When some threshold for unspent votes is met, new elections are called.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I've not developed this into a detailed proposal; every point on that list has multiple options for how it could be implemented, and the whole idea needs to be carefully analysed for Nash game failures.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How many votes? Are write-in candidates allowed? What about "none of the above"?&lt;/li&gt;
&lt;li&gt;Is this "vote income" a one-off at the election, or is this treated as "for the duration of this parliament, you will get this each month/quarter/year"?&lt;/li&gt;
&lt;li&gt;Really all standing politicians, or just all the ones who have unspent votes on the ledger?&lt;/li&gt;
&lt;li&gt;How many votes are they allowed to spend on any given thing? All of them, or a percentage? What's the meta-rule for whipped votes?&lt;/li&gt;
&lt;li&gt;Are politicians allowed to transact votes with each other, and if so under what conditions?&lt;/li&gt;
&lt;li&gt;When is a new election called? If it is "when only one party still has votes to spend", can this be gamed? Is it when a fixed percentage of all votes have been spent?&lt;/li&gt;
&lt;li&gt;Are all unspent votes from the previous elections wiped from the record with each subsequent election, or do they roll over?&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#Democracy'&gt;Democracy&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#government'&gt;government&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Politics'&gt;Politics&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#voting'&gt;voting&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Politics'&gt;Politics&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Wed, 24 Apr 2024 10:42:51 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/04/24-10.42.51.html</guid>
    </item>
    <item>
      <title>The More they Overthink the Plumbing…</title>
      <link>https://benwheatley.github.io/blog/2024/04/07-21.31.19.html</link>
      <description>&lt;h1&gt;The More they Overthink the Plumbing…&lt;/h1&gt;

&lt;p&gt;There are many lines in Star Trek which have stuck with me over the years; recently, it's been this line from Scotty when explaining how he sabotaged a ship that was trying to chase them: &lt;q&gt;The more they overthink the plumbing, the easier it is to gum up the pipes&lt;/q&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR for the lazy: XP good, make code as simple as possible, avoid 3rd party libs and APIs, here's why and how we got here.&lt;/p&gt;

&lt;h2&gt;History&lt;/h2&gt;

&lt;p&gt;This feels very pertinent to software engineering, where we have been building ever taller towers of abstraction, many of which make things worse.&lt;/p&gt;

&lt;h3&gt;The very beginning&lt;/h3&gt;

&lt;p&gt;At the base, computers are electronics, complex circuits with voltages propagating through them. They're not even really &lt;em&gt;digital&lt;/em&gt; electronics — transistors are fundamentally analog devices, that can be used as amplifiers, it's just that if you abstract things in the right way, you can treat it as "close enough" to discrete on-or-off to perform Boolean logic. This is genuinely useful, because while it is possible to perform computations in the analog domain, it's &lt;em&gt;really hard&lt;/em&gt; to do this with sufficient reliability for anything complex… but even so, we may well find that very large neural networks are somewhat resilient to these imperfections just as our messy wet chemical brains are somewhat resilient to all the things going on in and around them.&lt;/p&gt;

&lt;p&gt;Next level up, Boolean logic. On-vs.-off, 1-vs.-0, isn't enough, so we bunch several together to make base-2 numbers. That's great, and lets us represent natural integers, which themselves can be treated as if they were letters or symbols by way of a simple look-up table (which is basically how ASCII and its early competitors happened), or instructions for the CPU to follow. A few extensions also allow you to do negative numbers or floating point and decimal numbers (related but subtly different). All good stuff, though you do sometimes surprise people with 1/10th being a recurring fraction in base-2, so you shouldn't use this system for anything involving &lt;em&gt;money&lt;/em&gt;.&lt;/p&gt;

&lt;h3&gt;Structures&lt;/h3&gt;

&lt;p&gt;Next level up, structures. Arrays are one of the foundational examples — you have a sequence of some length, call it &lt;code&gt;n&lt;/code&gt;, that you're treating conceptually as a single thing. They might be numbers (income in each day) or they might be letters (to form a string of text). There were two common ways to do this when I was a kid: (1) to begin with a number saying how long this list is, followed by that many items; (2) to begin with the content, and to assume the list has a subsequent item until you encounter a special symbol that says "stop". Given my experience in Mac OS classic, the former was called a "Pascal string" and the latter a "C string", named after what the Pascal and C languages did.&lt;/p&gt;

&lt;p&gt;Each of these approaches had weaknesses: C strings often didn't get their final "terminating" character created or checked leading to corruption of system memory; while Pascal strings had to have their size known in advance and thus led to using too much memory (by 1980s and 1990s standards where my first two home computers came with 64 kB and 8 MB of RAM respectively), and/or leaving them unable to cope with long bits of text (which is still a problem for some combinations of names and forms of national ID). There are ways around this with more complex structures such as &lt;a href="https://en.wikipedia.org/wiki/Linked_list"&gt;linked lists&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Tree_(data_structure)"&gt;trees&lt;/a&gt; — I'm not actually sure how any given modern language handles strings internally, in part because they may have more than one string-like data structure and each may do things differently to give different performance trade-offs.&lt;/p&gt;

&lt;p&gt;The structures can get more complex. Before I go down that path too far, I will also say that simple concepts like "time" and "a string of letters" can each be far more complex than it first seems: time comes with time zones, which can vary with little warning, and leap-&lt;em&gt;seconds&lt;/em&gt; due to the rotation of the planet varying very slightly from one year to the next; and at the speed current computers operate at, I've even seen people care about light cones (to determine if a stock transaction on an exchange was suspicious do to it being created before information reached it from some other exchange) and GPS famously requires accounting for time dilation due to time dilation from both special relativity (speed-related) and general relativity (gravitational potential related). Strings aren't much better, as the original approach was to match what the keyboards on teleprinters used (with separate "newline" (&lt;code&gt;\n&lt;/code&gt;) and "carriage return" (&lt;code&gt;\r&lt;/code&gt;) symbols, &lt;a href="https://en.wikipedia.org/wiki/Newline"&gt;where Mac OS classic ended each line of text with &lt;code&gt;\r&gt;&lt;/code&gt;, Unix (and modern Mac OS)with &lt;code&gt;\n&lt;/code&gt;, and Windows with &lt;code&gt;\r\n&lt;/code&gt;, and several other choices for systems you might have never heard of&lt;/a&gt;. And then we noticed that most of the world, even most of Europe, doesn't use just the symbols on a US/UK keyboard, and came up with a bunch of different encodings before finally settling on Unicode… which itself took a few attempts to find a representation everyone could agree on which (for the moment) is utf-8 — a variable length system which would be a bit much to describe here, but put simply: if you want to reverse a string, in ASCII you just reverse the sequence of bytes and you're done, but if you do that in UTF-8 you probably no longer have something that's even possible to display; even if you reverse the "code points" (the next layer of abstraction up) you're probably doing it wrong, as that switches the flags for Canada (CA) and the Ascension Islands (AC).&lt;/p&gt;

&lt;h3&gt;Algorithms and control flow&lt;/h3&gt;

&lt;p&gt;Structures were great, and did a lot for us. Indeed, we still find them all over the place as a fundamental concept. At this point I should divert from the data to the algorithms, as they too were given layers of abstraction: initially, algorithms were fixed as circuits; then we found ways to make circuits change other circuits, first mechanically (by rotating a drum with wires on them) and then electrically (with valves, relays, and transistors). The &lt;a href="https://en.wikipedia.org/wiki/Turing_machine"&gt;Turing machine&lt;/a&gt; was a formalisation of this abstraction.&lt;/p&gt;

&lt;p&gt;Load instruction, execute (perform) instruction; load, execute; repeat. Nice and easy. Conditional instructions? A &lt;code&gt;branch&lt;/code&gt; command: if &lt;code&gt;${some condition is true}&lt;/code&gt; then the next instruction you should load is at memory address &lt;code&gt;${xyz}&lt;/code&gt;, otherwise continue as normal. Nice and easy, good abstraction, the &lt;code&gt;if&lt;/code&gt; statement is still with us. Loops?&lt;/p&gt;

&lt;pre&gt;#marker
load, execute
load, execute
load, execute
if not finished, go to #marker&lt;/pre&gt;

&lt;p&gt;Nice and easy, that's a &lt;code&gt;do-while&lt;/code&gt; loop; there's several others I won't bore you with, they're still fairly common.&lt;/p&gt;

&lt;p&gt;Next level of abstraction: functions. If you're doing one thing often, you probably only want to write it once and then re-use it in many places. We did this by borrowing an idea from maths, called a function. A function takes "arguments" as inputs, and then returns a value. Good stuff, nice and easy. Each instruction on a basic CPU is a little function, and more complex CPUs have "microcode" which is basically some built-in functions. Good stuff.&lt;/p&gt;

&lt;h3&gt;Objects&lt;/h3&gt;

&lt;p&gt;Why this diversion from data to processing? Because the next level of abstraction combines them both. "Object-oriented" code takes some data structure and combines it with the functions that are good for that data structure into a "class": for example, you might have a &lt;code&gt;String&lt;/code&gt; class which has functions — in this context generally called "methods" — which perform operations on that string such as "make this uppercase" or "reverse this"; this is very useful, because it means a user of this doesn't need to think too hard about the underlying data structure — to go back to the previous point about the change from ASCII to UFT-8, I simply don't need to care if a modern &lt;code&gt;String&lt;/code&gt; is using UTF-8 or ASCII, nor do I need to care about the rules for multiple bytes into code points, nor do I need to care about the rules for combining multiple code points into a single printable character, if I want to reverse a string, I can look for an existing &lt;code&gt;reverse()&lt;/code&gt; method and I'm done.&lt;/p&gt;

&lt;p&gt;They're really neat and useful. Indeed, objects were really popular for a long time, and although they have recently fallen out of fashion they are still widely available in modern languages. It will take a few more layers of abstraction to really say &lt;em&gt;why&lt;/em&gt; they're no longer fashionable.&lt;p&gt;

&lt;p&gt;Next layer of abstraction: subclassing. Classes make it easier to divide code into logically relevant areas — string things in the string class, number things in a number class, network things in a network class, and so on. But things often also form hierarchies — say you're making an action video game in this paradigm (modern game development favours a different approach), and you want all the entities in your game to have a number of hitpoints, and that if depleted they will be destroyed. Great, so do you copy-paste a bunch of &lt;strikethrough&gt;structures&lt;/strikethrough&gt; classes that each has a &lt;code&gt;hitpoints&lt;/code&gt; property and a &lt;code&gt;hitWith(damage)&lt;/code&gt; function? No! You have a parent class &lt;code&gt;GameEntity&lt;/code&gt; with those things, and all your game's entities — chests/crates, the player, the enemies, exploding barrels — are made to "inherit" from this. Nice. Makes things logical and easy… except when it doesn't.&lt;/p&gt;

&lt;p&gt;There are three big ways for this to cause complexity, the first two are easy to explain even though they can be a pain to resolve: multiple inheritance (if you've got two "parent" classes with a method of the same name, which one does the child class inherit from?); unclear hierarchies (is a square a special type of rectangle where the orthogonal sides are equal, or is a rectangle a special type of square where the sides can be different lengths?).&lt;/p&gt;

&lt;p&gt;The third problem is &lt;em&gt;typing&lt;/em&gt; — and no, I don't mean in the sense of what my fingers are currently doing on the keyboard. Each class (and structure, and also the "primitive" types the CPU uses such as "16 bit unsigned integer" and "64 bit floating point number") can be said to be "types", and each function and method will expect specific types on both input and output. But if you have a collection of &lt;code&gt;GameEntity&lt;/code&gt; objects, you need something a bit more general, as any single &lt;code&gt;GameEntity&lt;/code&gt; might be a &lt;code&gt;Player&lt;/code&gt; or a &lt;code&gt;Crate&lt;/code&gt; or a &lt;code&gt;Enemy&lt;/code&gt; or an &lt;code&gt;ExplodingBarrel&lt;/code&gt;, so you need a way to operate on both the higher-level concept of a &lt;code&gt;GameEntity&lt;/code&gt; without erasing all the information about the specific detailed type. Unfortunately, this description just gave you the example where it's fairly easy, and everyone can see what's going on. This is very useful, and sometimes even mandatory — we &lt;em&gt;want&lt;/em&gt; to have a generalised concept of a collection of objects without having to make special cases for both &lt;code&gt;integers&lt;/code&gt; and &lt;code&gt;GameEntity&lt;/code&gt; — the &lt;em&gt;problem&lt;/em&gt; is that it's almost impossible to gauge the correct level of abstraction: often we either spend a huge effort to make something worth with any possible type when in practice there will be exactly one (it happens!) or we assume it will always be one of a small set of possible classes and make it too narrow.&lt;/p&gt;

&lt;p&gt;These issues, combined with a few other more subtle issues, have led modern development to favour composition over inheritance: where related things are combined within some structure (or class) rather than inherited from a parent class — a &lt;code&gt;Player&lt;/code&gt; (etc.) done this way would indeed have a property &lt;em&gt;named&lt;/em&gt; "hitpoints", but that &lt;code&gt;hitpoints&lt;/code&gt; property would be a class (rather than a primitive number) which would handle all the complexities and hide it from the &lt;code&gt;Player&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;Design patterns&lt;/h3&gt;

&lt;p&gt;Next level of abstraction: Design patterns. These come at two levels, code — for example, how are new objects created, how is the data presented to a different system expecting a specific format, etc. — and architectural. There's a lot of different ways to do this, each better or worse suited for a specific task, but you can imagine this as filling a similar role to classes, but one layer up: a good way to organise the code, all the things related to one task are found in one place. When I began iOS development, the standard architecture for apps was &lt;a href="https://developer.apple.com/library/archive/documentation/General/Conceptual/DevPedia-CocoaCore/MVC.html"&gt;MVC: Model View Controller&lt;/a&gt;. The model is the data (perhaps a database on your device, perhaps a connection to an API on the internet); the View is your actual user interface; the Controller is the bit which glues the two together.&lt;/p&gt;

&lt;p&gt;And this is where I get confused.&lt;/p&gt;

&lt;p&gt;Apparently, lots of developers kept putting "too much" into the Controller.&lt;/p&gt;

&lt;p&gt;(As an aside: I'm genuinely not sure what "too much" means — I've worked with code that clearly hit this category as there were 1,000 lines inside a single if-statement; but I've also seen some developers hold the opinion that anything more than 3 lines of code in a method is too many lines).&lt;/p&gt;

&lt;p&gt;Too much into the Controller? The glue which combines the model with the view? OK, lets say this is so: what's the solution? There are several, because of course there are. Apple's replacement for MVC is MVVM: Model-View-ViewModel, where the ViewModel is a transformation of the Model to be specifically what the View needs to know about, and the glue (that was formerly the point of the Controller) is supposed to be automated. (This automation is what "Reactive" means in the context of UI development).&lt;/p&gt;

&lt;p&gt;Don't get me wrong: if automation can be made to work, that's good. It's the point of computers, it's why they're everywhere. Even though the magic words to bind a value to a view are hard to remember, this is &lt;em&gt;not&lt;/em&gt; the problem.&lt;/p&gt;

&lt;p&gt;There are two problems here, one is fundamental, the other purely in how the magic binding works. The fundamental problem is now we have both an automated connection (View-ViewModel) &lt;em&gt;and&lt;/em&gt; a manual connection (Model-ViewModel). No, just no. This gives us all the downsides of both. The implementation problem is that all this magic binding requires a lot of behind-the-scenes complexity of exactly the same kind which led to classes and inheritance becoming huge headaches that we all moved away from as much as possible (this also impacts web development, where Meta's &lt;a href="https://en.wikipedia.org/wiki/React_(software)"&gt;React.js&lt;/a&gt; framework seems to be very popular).&lt;/p&gt;

&lt;h3&gt;Testing&lt;/h3&gt;

&lt;p&gt;Even the problems with magic bindings hasn't dissuaded people from going further. However, the next level of complexity needs another diversion, this time to testing.&lt;/p&gt;

&lt;p&gt;Testing is hard. It's very easy to write code and think "that looks fine", when it isn't. Even getting humans to try to use your code only reveals some of the issues, though even this is much better than just taking a developer's word for it. One of the things we do to account for this difficulty is automated tests. You'll spot the first problem immediately: we write programs to test programs. How do we know the tests are correct? We don't.&lt;/p&gt;

&lt;p&gt;There's one school of thought which says we should write the tests first, before any other code. Running these tests &lt;em&gt;must&lt;/em&gt; produce at least one fail, because the code does nothing; then you add code until the tests pass. This mindset is hard to get into.&lt;/p&gt;

&lt;p&gt;Another school of thought is that we should aim to cover 100% of our code with tests. I prefer this one over writing the tests first and the code later, partly because of how I approach development in general (I develop an understanding of the problem as I write the code), and partly because I think this is a tighter constraint than writing the tests first (there are probably more possible tests than you can immediately think of).&lt;/p&gt;

&lt;p&gt;Now, why is this relevant? Well: how do you architect those tests? If you want to test a payment system, you don't want &lt;em&gt;every single run of every single test&lt;/em&gt; of your system to end up triggering a real payment — these things &lt;em&gt;should&lt;/em&gt; run multiple times per day per developer, more likely multiple times per hour per developer.&lt;/p&gt;

&lt;p&gt;This leads to breaking down methods and functions into smaller and smaller units that do less and less stuff, and which "inject" the things they depend on as arguments. (This is called "dependency injection"). Dependency injection is a nice solution to an abstraction which isn't nice but is kinda necessary.&lt;/p&gt;

&lt;h3&gt;Baroque architectures&lt;/h3&gt;

&lt;p&gt;Now it's time to fold this diversion back into the previous topic of architectural design patterns: that &lt;em&gt;some&lt;/em&gt; of these tests are better done with very narrow scope has led to people saying that &lt;em&gt;all&lt;/em&gt; code needs to be architected with that kind of division in mind. As I wrote earlier, I've seen some developers hold that more than 3 lines of code in a function is too many lines — it seems absurd, but this is why they call for it. Unfortunately, it doesn't just &lt;em&gt;seem&lt;/em&gt; absurd, it &lt;em&gt;is&lt;/em&gt; absurd: breaking things down to that level means that there's more glue than function, and the glue is harder to test, is harder to follow and understand, and is harder to change.&lt;/p&gt;

&lt;p&gt;For a specific example of this, consider the VIPER architectural design patterns:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;V: View&lt;/li&gt;
&lt;li&gt;I: Interactor, provides model ("entity") data relevant to the presenter, along with any methods for manipulating the data&lt;/li&gt;
&lt;li&gt;P: Presenter, prepares the model for display, forwards user input to router or Interactor&lt;/li&gt;
&lt;li&gt;E: Entity, the data itself&lt;/li&gt;
&lt;li&gt;R: Router, to allow the UI to transition between different views&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is too many parts, and the divisions don't represent the real logical splits: The Presenter and the Interactor both alter data, but are not themselves part of the data model; the Presenter is taking on two responsibilities but then forwarding both to other units; the Router is doing &lt;em&gt;purely&lt;/em&gt; view-related tasks and could be part of the View (for example, when using Xcode to make UIKit/Interface Builder based apps, this would be a "&lt;a href="https://developer.apple.com/library/archive/featuredarticles/ViewControllerPGforiPhoneOS/UsingSegues.html"&gt;segue&lt;/a&gt;"); and the Interactor is doing things to the Entity which should all be methods &lt;em&gt;on the Entity itself&lt;/em&gt; in any language which supports this — it's the fundamental thing which defines the "Object" in "Object Oriented programming".&lt;/p&gt;

&lt;h3&gt;Analytics&lt;/h3&gt;

&lt;p&gt;And one final, extra, point: Analytics. You all know the annoying popups for consent to track you. Why do companies bother with them, when all users hate them? Because analytics reveal what users do, which parts of the app they engage with, what they have difficulty with — something you may not know as a normal user, is that they can easily track how far you scroll down the screen, which buttons you tap on, and exactly where your finger (or mouse) pressed (or clicked) on the screen. This is sold as "helping developers discover when buttons are too small to click on" and similar. That said, even though I am a developer, I am just as confused as everyone else about why on earth some websites have 1200 "partners" to "help" them analyse all this.&lt;/p&gt;

&lt;p&gt;This website you're reading now (unless someone copy-pasted it), didn't show you a pop-up. This is due to the simple trick of… not tracking anything. I have no idea how many of you are reading this. I don't know your age demographics (which YouTube apparently provides, but I've not looked into that despite having a YouTube account). I don't know, or care, how far you scrolled. I don't know which countries you're all from, which my previous blogging platform, wordpress, told me.&lt;/p&gt;

&lt;p&gt;But at the time of writing, this attitude seems to be rare: me, some other random bloggers, HackerNews, and possibly GitHub.&lt;/p&gt;

&lt;h2&gt;The Problem&lt;/h2&gt;

&lt;p&gt;It's all just too much: there's far too many things in software for any single developer to know it all; because it can't be known, we keep re-inventing the wheel, and each re-invented wheel has to re-learn the lessons of all the predecessors; this happens at multiple levels of abstraction all at the same time, so we get web frameworks that use JavaScript to handle loading of links and the display of images even though those things are built into the pre-JavaScript standard of HTML; and we have websites with so much excess &lt;em&gt;stuff&lt;/em&gt; bundled with them that users would genuinely be better off if the site was a static image.&lt;/p&gt;

&lt;p&gt;The thing is, I've seen complaints like this from day one:&lt;/p&gt;

&lt;ul&gt;

&lt;li&gt;The "&lt;a href="https://en.wikipedia.org/wiki/Software_crisis"&gt;Software Crisis&lt;/a&gt;" of the 1960s and 1970s, where software projects were over-budget, late, inefficient, filled with bugs, often did not meet requirements, unmanageable and code difficult to maintain, and sometimes just never in a deliverable state. The extra abstractions since then are, in  a big way, (partial) solutions to these issues. (We also saw changes to how projects are managed, the scope of which is outside my experience: I've only ever known "agile").&lt;/li&gt;

&lt;li&gt;"No compiler is as efficient as hand-crafted assembly", which was actually true when I first encountered it.&lt;/li&gt;

&lt;li&gt;"&lt;a href="https://en.wikipedia.org/wiki/C%2B%2B"&gt;C++&lt;/a&gt; takes ages to compile", can be true if you're not careful, thanks to it using &lt;a href="https://en.wikipedia.org/wiki/Template_metaprogramming"&gt;template metaprogramming&lt;/a&gt; as the mechanism for allowing generic types, such as a &lt;em&gt;generic&lt;/em&gt; list whose elements may be of type &lt;code&gt;int&lt;/code&gt; &lt;em&gt;or&lt;/em&gt; &lt;code&gt;GameEntity&lt;/code&gt; or whatever, and that this approach happens to be &lt;a href="https://en.wikipedia.org/wiki/Turing_completeness"&gt;Turing complete&lt;/a&gt; and therefore &lt;a href="https://en.wikipedia.org/wiki/Halting_problem"&gt;you can't always tell in advance if the process will even stop&lt;/a&gt;.&lt;/li&gt;

&lt;li&gt;"Object oriented code is bad because of the complex class hierarchies!" They say, pointing at examples such as &lt;a href="https://github.com/microsoft/referencesource/blob/master/System.ServiceModel/System/ServiceModel/MessageSecurityVersion.cs"&gt;&lt;code style="overflow-wrap: break-word;"&gt;WSSecurity11WSTrustFebruary2005WSSecureConversationFebruary2005WSSecurityPolicy11BasicSecurityProfile10MessageSecurityVersion&lt;/code&gt;&lt;/a&gt; which is more the fault of a corporate policy on names than object oriented programming itself.&lt;/li&gt;

&lt;/ul&gt;

&lt;p&gt;So, am I just being an old man shaking their fist at a cloud? I think it all comes down to the outcomes: If the code is performant, if it's cheap to produce, if it's not prone to errors, does it really matter what's going on inside? The problem is only if one or more of those are absent or insufficient.&lt;/p&gt;

&lt;p&gt;Well, looking at the apps and services around me, I don't think we have a fantastic state of affairs in &lt;em&gt;any&lt;/em&gt; of those things:&lt;/p&gt;

&lt;ul&gt;

&lt;li&gt;User experiences in modern systems &lt;a href="https://danluu.com/input-lag/"&gt;are measurably slower today than the 1980s&lt;/a&gt; despite modern systems being 6 orders of magnitude more computing power. This is &lt;a href="https://news.ycombinator.com/item?id=21833897"&gt;directly related to the abstractions we find useful&lt;/a&gt;.&lt;/li&gt;

&lt;li&gt;Apps are more expensive than ever, judging by the size of dev teams and salaries.&lt;/li&gt;

&lt;li&gt;Apps are as buggy as ever, judging by how many App Store update notes are some variant of "bug fixes" (possibly with cringe-inducing attempt at cute phrasing).&lt;/li&gt;

&lt;/ul&gt;

&lt;p&gt;Now, one might argue these are unavoidable costs created by "doing it right". When I was at university, there was "programming in the large vs. programming in the small" (when Java was popular despite having exactly the complex class names and hierarchies previously mentioned) was analogised with the difference between building a bridge with a few planks of wood vs. building a suspension bridge — A suspension bridge over a small ditch looks silly, a few planks of wood over &lt;a href="https://en.wikipedia.org/wiki/Golden_Gate"&gt;the Golden Gate&lt;/a&gt; won't even support itself let alone traffic.&lt;/p&gt;

&lt;p&gt;Likewise from the perspective of user expectations, there have been many changes in the transition from software of the 1990s, where you buy software once from a physical store, it runs on one machine which has one user, there are no security requirements, and you get to print system requirements on the side of the box it's sold in and it's the user's problem not the developers if the user doesn't have a 640x480 monitor that supports 256 colours, and it will be used in an office with constant lighting; to today, where you're expected to ship regular updates to justify the monthly subscription plan you're selling it on, and the users have a huge range of display sizes and orientations which are expected to be able to change dynamically during use, and the app has to switch between bright and dark modes depending on the time of day, and we have rules requiring support of people with various disabilities.&lt;/p&gt;

&lt;p&gt;The converse point is another anecdote from university, when we were doing practice interview questions, and I was forced to justify what I meant by having written "committed to quality"; the me from 20 years ago just thought this meant "be as good as possible", but the pannelist (who had actual experience hiring people) had a better model: his example was to ask me:&lt;/p&gt;

&lt;p&gt;"Which is better, a Porche or a go-kart?"&lt;/p&gt;
&lt;p&gt;"The Porche, obviously"&lt;/p&gt;
&lt;p&gt;"But what if you're a kid who just wants to get down a hill? You don't have a driving licence or even lessons, you're definitely not insured, you just want to have fun. You can't — don't know how and are not allowed — to use the Porche. Now? Now &lt;em&gt;the best&lt;/em&gt; is the go-kart".&lt;/p&gt;

&lt;p&gt;(All paraphrased, of course. Couldn't afford to record these sessions back in 2003 or so).&lt;/p&gt;

&lt;h2&gt;What to change?&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Don%27t_throw_the_baby_out_with_the_bathwater"&gt;Don't throw the baby out with the bathwater.&lt;/a&gt; Be mindful of &lt;a href="https://en.wikipedia.org/wiki/G._K._Chesterton#Chesterton's_fence"&gt;Chesterton's fence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The fences were made for a reason: software is easy to do wrong. The bathwater isn't the mere existence of abstractions, it's that there are too many at every level, and that even when they are very useful in specific cases, they are &lt;a href="https://en.wikipedia.org/wiki/Cargo_cult_programming"&gt;cargo-culted&lt;/a&gt; as &lt;a href="https://en.wikipedia.org/wiki/No_Silver_Bullet"&gt;silver bullets&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;&lt;a href="https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it"&gt;You aren't gonna need it&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;You, specifically, &lt;em&gt;almost certainly&lt;/em&gt; don't need:&lt;/p&gt;

&lt;ul&gt;

&lt;li&gt;A fancy design pattern such as VIPER. Sure, it does the things it says it does, but it's &lt;em&gt;slow&lt;/em&gt; to develop for, and leads to more &lt;a href="https://en.wikipedia.org/wiki/Glue_code"&gt;glue code&lt;/a&gt; than &lt;a href="https://en.wikipedia.org/wiki/Business_logic"&gt;business logic&lt;/a&gt;. Instead, use the simplest architecture for your platform — for &lt;a href="https://en.wikipedia.org/wiki/Cocoa_Touch"&gt;UIKit&lt;/a&gt; that would be &lt;a href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller"&gt;MVC&lt;/a&gt;, for &lt;a href="https://en.wikipedia.org/wiki/SwiftUI"&gt;SwiftUI&lt;/a&gt; it would be &lt;a href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93viewmodel"&gt;MVVM&lt;/a&gt;, for a game I'm told that's &lt;a href="https://en.wikipedia.org/wiki/Entity_component_system"&gt;ECS&lt;/a&gt;.&lt;/li&gt;

&lt;li&gt;Third-party libraries (and, unless you're able to get a &lt;a href="https://en.wikipedia.org/wiki/Service-level_agreement"&gt;service-level agreement&lt;/a&gt; whose terms you agree with, third-party APIs). &lt;em&gt;Not even one&lt;/em&gt;: sure, theoretically there are some exceptions, but treat each library you depend on like you would a 3rd party vendor who is charging you the equivalent of 10 hours a week of dev time &lt;em&gt;only the bill comes due at the most annoying time possible and isn't nicely spread over the year&lt;/em&gt; — these libraries might even do what you want them to do, but you're now responsible for any bugs, security issues, incompatibility with the &lt;em&gt;next&lt;/em&gt; version of the system you support, and when the library has its own dependencies, each one brings with it the same problems again… and what you wanted probably isn't too hard to do yourself because 80% of the things were already given to you in the operaring system's built-in library. 3rd party libraries are the &lt;a href="https://lucumr.pocoo.org/2024/3/26/rust-cdo/"&gt;collateralized debt obligation&lt;/a&gt; of technical debt, and to be avoided for &lt;a href="https://en.wikipedia.org/wiki/Collateralized_debt_obligation#Crash"&gt;similar reasons&lt;/a&gt;. This includes AI (and I don't just mean an LLM when I say AI): AI are neat, but unless you're a research organisation with a very unusual task to automate, you don't need one; and worse, they an even bigger problem than normal 3rd party libraries when it comes to you being held responsible for their flaws without having any power to fix them.&lt;/li&gt;

&lt;li&gt;Anything involving the word "&lt;a href="https://en.wikipedia.org/wiki/Reactive_programming"&gt;reactive&lt;/a&gt;". Sure, it works, it does the thing — but the "magic" which is supposed to make life easier can be done incorrectly &lt;em&gt;just as easily&lt;/em&gt; as forgetting a very simple code pattern in a non-reactive UI. It is the &lt;em&gt;illusion&lt;/em&gt; of useful.&lt;/li&gt;

&lt;li&gt;User analytics. No, really! I'm not using them on this blog as I write this, and you don't need to either. They give you a pretty graph to look at, but &lt;a href="https://en.wikipedia.org/wiki/Goodhart%27s_law"&gt;Goodhart's law&lt;/a&gt; applies: Whenever a measure becomes a target, it stops being a good measure. Put it another way, you care about business goals? How many users click on some button, isn't that. Even when the button happens to be labelled "pay now", you care about the payment, not the button. You &lt;em&gt;do not need&lt;/em&gt; the invasive user-tracking that led to the EU creating the GDPR that you're trying to get around because you chose a non-compliant popup that doesn't have a single huge "just no" button. Even if you have a safety-critical component to your system — you make self-driving cars or a successor to &lt;a href="https://en.wikipedia.org/wiki/Therac-25"&gt;Therac-25&lt;/a&gt;, you can track the safety issue itself without tracking a user!&lt;/li&gt;

&lt;li&gt;Any "Big Data" solution. The data you have is not "big". A million rows in your database? That's &lt;em&gt;small&lt;/em&gt; data; 2 million chess games? Processed &lt;a href="https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html"&gt;235x faster with command line tools&lt;/a&gt; from &lt;a href="https://en.wikipedia.org/wiki/AWK"&gt;1977&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Grep"&gt;1973&lt;/a&gt;(!) than with Big Data tools like Amazon Elastic Map Reduce and Hadoop. 4 gigabyte AI model? Still not "big", could use my phone for that; &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download"&gt;Downloading and processing the current text of all pages on the English Wikipedia?&lt;/a&gt; One single developer's computer — in the form of a side-project that runs in the background while they do something else; etc. — if it even fits on one machine, it's small data. Don't even try to use a Big Data approach until you can't buy a single hard drive big enough for it.&lt;/li&gt;

&lt;/ul&gt;

&lt;h3&gt;What to keep?&lt;/h3&gt;

&lt;p&gt;Based on my experience, the following are all genuinely useful, and must be kept:&lt;/p&gt;

&lt;ul&gt;

&lt;li&gt;Whichever language is the &lt;a href="https://en.wikipedia.org/wiki/Focal_point_(game_theory)"&gt;Schelling point&lt;/a&gt; for your domain. iOS? Swift. Web? JavaScript. etc. — all modern languages have enough abstraction to get done all the things you need for whatever your business logic is.&lt;/li&gt;

&lt;li&gt;Both automated and manual tests. The former is like a spell-checker (which won't spot it when you mix up "loose" and "lose" as I often do), the latter like a proof-reader. Automated tests don't cover everything, but they do save the time of the expensive human who helps with what the automation misses.&lt;/li&gt;

&lt;li&gt;Any simple design pattern. Doesn't seem to matter so much which one so long as it is simple. I've seen code with no design pattern (an entire pantheon of &lt;a href="https://en.wikipedia.org/wiki/God_object"&gt;god classes&lt;/a&gt;, a thousand lines inside an &lt;code&gt;if&lt;/code&gt; block), and it's almost (but not quite!) as bad as having an &lt;em&gt;over-complex&lt;/em&gt; design pattern like VIPER — I assume there's &lt;em&gt;some&lt;/em&gt; scale at which VIPER is the right answer, but I've never seen it.&lt;/li&gt;

&lt;li&gt;Version control. It &lt;em&gt;is&lt;/em&gt; possible to get away without it… if you only have one developer. You don't want to have only one developer.&lt;/li&gt;

&lt;li&gt;Code review. I'm not convinced it really helps improve the code (too many people go &lt;a href="https://en.wiktionary.org/wiki/LGTM"&gt;"looks good to me!"&lt;/a&gt; without analysing the changes deeply enough to spot defects), but it &lt;em&gt;is&lt;/em&gt; a low-cost way to spread knowledge between team members.&lt;/li&gt;

&lt;li&gt;An issue tracker. Again, doesn't matter which — I've worked with literal post-it notes on a wall, and this is fine; conversely, while it's popular to hate JIRA, I'm fine with that too. It really doesn't matter to us software developers, so if you're a manager picking one, go for whatever you like.&lt;/li&gt;

&lt;/ul&gt;

&lt;h3&gt;Has anyone been here before?&lt;/h3&gt;

&lt;p&gt;Yes! When I was at university, we were taught about "&lt;a href="https://en.wikipedia.org/wiki/Extreme_programming"&gt;extreme programming&lt;/a&gt;":&lt;/p&gt;

&lt;ul&gt;

&lt;li&gt;Pair programming&lt;/li&gt;

&lt;li&gt;Extensive code review&lt;/li&gt;

&lt;li&gt;Unit testing of all code&lt;/li&gt;

&lt;li&gt;Not programming features until they are actually needed&lt;/li&gt;

&lt;li&gt;Code simplicity and clarity&lt;/li&gt;

&lt;li&gt;Expecting changes in the customer's requirements as time passes and the problem is better understood&lt;/li&gt;

&lt;li&gt;Frequent communication with the customer and among programmers&lt;/li&gt;

&lt;/ul&gt;

&lt;p&gt;As with all trendy philosophies, even groups that say they follow it tend to only follow only about half of the things it calls for.&lt;/p&gt;

&lt;p&gt;Let's bring back &lt;a href="https://en.wikipedia.org/wiki/Extreme_programming"&gt;XP&lt;/a&gt;. (No, not &lt;a href="https://en.wikipedia.org/wiki/Windows_XP"&gt;that one&lt;/a&gt;).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#Coding'&gt;Coding&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Opinion'&gt;Opinion&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Programming'&gt;Programming&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#software engineers'&gt;software engineers&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Technology'&gt;Technology&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Opinion'&gt;Opinion&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Professional'&gt;Professional&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Technology'&gt;Technology&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Sun, 07 Apr 2024 21:31:19 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/04/07-21.31.19.html</guid>
    </item>
    <item>
      <title>Eiron Merton</title>
      <link>https://benwheatley.github.io/blog/2024/04/07-12.47.14.html</link>
      <description>&lt;h1&gt;Eiron Merton&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Have_I_Got_News_for_You"&gt;Have I Got News For You&lt;a/&gt;&lt;/em&gt; is a long-running satirical news quiz on UK TV, where Paul Merton and Ian Hislop are (almost) always found on opposing teams.&lt;/p&gt;

&lt;p&gt;Ian Hislop has been editor of the satirical magazine Private Eye since 1986, and is an Oxford graduate. Paul Merton is a professional comedian who failed his &lt;a href="https://en.wikipedia.org/wiki/Eleven-plus"&gt;eleven plus&lt;/a&gt;, didn't go to university, and who makes occasional references to a mediocre school grade (&lt;a href="https://hignfy.fandom.com/wiki/Paul%27s_GCSE_in_metal_work"&gt;"CSE ungraded in metalwork"&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;You might imagine this is rather one-sided.&lt;/p&gt;

&lt;p&gt;It gets better-worse though, there is one famous episode where the guest due to be on Paul's team cancelled at the last minute, and was replaced with an actual tub of lard. The host then tipped things further against Merton, for example the "missing words" round where words have been blanked out and the panelists have to guess, were in languages Paul didn't speak (except for the last one which was &lt;em&gt;entirely&lt;/em&gt; blanked out English).&lt;/p&gt;

&lt;p&gt;Despite this, Paul won.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In ancient Greek plays, there were stock characters; εἴρων (Eiron) feigned foolishness while being clever, while ἀλαζών (Alazṓn) is boastful and sees himself as greater than he actually is.&lt;/p&gt;

&lt;p&gt;All analogies are a bit wrong (I don't see any boasting from Hislop), but I definitely see an echo here.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It's not the only example of these roles on the show. One of the other famous guests was Boris Johnson, who is another Alazṓnic character — boastful, Oxford-educated, you'd have every reason to expect him to be smart and sharp… but he isn't.&lt;/p&gt;

&lt;p&gt;I think Johnson modelled himself in the same way as Hislop; I don't know where the causality is here — it may be a natural outcome of the Oxford environment giving everyone who goes there superlative confidence, or Johnson may have been inspired by Hislop.&lt;/p&gt;

&lt;p&gt;Some have said that the show itself was how Johnson gained popularity; Hislop has ridiculed this idea, but I think it has merit — while Hislop gave Johnson difficult questions that &lt;em&gt;ought&lt;/em&gt; to have been taken seriously, the whole environment of the show (and definitely Johnson's ability to act like it was all for fun) made it feel like the questions were unserious and didn't matter. &lt;a href="https://www.youtube.com/watch?v=xUDEpqyJ-_w"&gt;This is despite one of the questions being about the time Johnson conspired to have a journalist assaulted&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;No conclusion, just observation and opinion.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#comparisons'&gt;comparisons&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Opinion'&gt;Opinion&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Humour'&gt;Humour&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Opinion'&gt;Opinion&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Politics'&gt;Politics&lt;/a&gt;
&lt;/p&gt;</description>
      <pubDate>Sun, 07 Apr 2024 12:47:14 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/04/07-12.47.14.html</guid>
    </item>
    <item>
      <title>Timeline of the near future</title>
      <link>https://benwheatley.github.io/blog/2024/03/23-17.24.34.html</link>
      <description>&lt;h1&gt;Timeline of the near future&lt;/h1&gt;

&lt;h2&gt;Space&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;2024&lt;/strong&gt;: SpaceX Starship test flights. This may include SpaceX payloads, but not necessarily. Likewise, it may involve a launch to Mars (like the Falcon Heavy test flight), but not necessarily (the launch window for that is October-November), and I don't expect that launch to involve a successful landing on Mars. First Lunar fly-by more likely but still not certain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2025&lt;/strong&gt;: Useful Starship launches. First Starship attempt at the Moon is more likely here, I'd guess 80%. No Mars launch window.

&lt;p&gt;&lt;strong&gt;2026&lt;/strong&gt;: Businesses are hard to forecast even a few years ahead. Assuming SpaceX hasn't imploded by this point, I expect them to have landed on the Moon by the end of 2026. A Mars attempt may (67%) have been launched, but the 2026 launch window is November-December so a launch then won't arrive in 2026.

&lt;p&gt;&lt;strong&gt;2026-2027&lt;/strong&gt;: Someone will attempt to land a regolith processor on the Moon, experiment with in-situ aluminium production. The first attempt is unlikely to succeed, because we've not done it before. First demo will probably be a proof of concept that does 1 gram and then stops forever; subsequent units may be 1 kg/Lunar day (14 Earth days); needs to be able to output something like the mass of the processing factory itself before failing in order to be worth bothering with, and I can't guess how long that will take.

&lt;p&gt;&lt;strong&gt;2028-2029&lt;/strong&gt;: Next Mars launch window (December-January). If SpaceX doesn't have a demonstration Sabatier process plant that fits into a Starship by the beginning of 2028, they've stopped caring about going to Mars. If SpaceX didn't launch in 2026 (and are still around), they will attempt at least one Starship flight there during this launch window. (If SpaceX no longer exists, nobody else really cares that much about Mars). As 2028 will be a US presidential election, assume that there will be strong pressure to get human feet on the Moon before the January 2029 inauguration of the next one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2029 onwards&lt;/strong&gt;: Everything beyond this point is heavily dependent on unknowns, including the political — government leaders may decide they don't like Musk any more and start looking for ways to specifically undermine him personally. No Musk ⇒ No Mars.&lt;/p&gt;

&lt;p&gt;If the economic environment supports it, there may be some early attempts at private space stations/space hotels around 2028-2030. We've already seen some work on this, but they can't succeed without a launch system at least as cheap as Starship.&lt;/p&gt;

&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/SpinLaunch"&gt;SpinLaunch&lt;/a&gt; system would be an excellent fit for the Lunar environment, as one of SpinLaunch's biggest limitations is the atmosphere both during spin-up (needing a huge vacuum chamber with a fast-acting door) and ascent (drag), while the Moon is famously airless. Also, the main payload arm can reasonably fit (or be designed to fit) into the payload bay of Starship, so it could be made on Earth prior to lunar ISRU being any good.&lt;/p&gt;

&lt;p&gt;Because SpinLaunch &lt;em&gt;can&lt;/em&gt; fit into a Starship, there's probably no chance of anyone making a prototype &lt;a href="https://en.wikipedia.org/wiki/Orbital_ring"&gt;orbital ring&lt;/a&gt; — although this would be just about possible with Starship and made easier by having a SpinLaunch system on the Moon, it currently looks like SpinLaunch would be cheap enough that people will ask "why bother?" for an orbital ring. That said, an orbital ring can double as a power line, and any Moon base needs a way to get power during the 14-day-long Lunar night, so it's not impossible that someone will have at least seriously proposed it and be doing pathfinder missions for it.&lt;/p&gt;

&lt;p&gt;By the end of the decade, we will either have a tiny permanent base (like &lt;a href="https://en.wikipedia.org/wiki/McMurdo_Station"&gt;McMurdo Station&lt;/a&gt;) on the Moon, &lt;em&gt;or&lt;/em&gt; we will have learned that we really don't want to go to the Moon.&lt;/p&gt;

&lt;p&gt;One thing to watch out for is Tesla's robotics ending up in SpaceX's missions. Partly because &lt;a href="https://en.wikipedia.org/wiki/Elon_Musk%27s_Tesla_Roadster"&gt;Musk is a salesman and he has previous form&lt;/a&gt;, but also because a Cybertruck is the kind of vehicle you want on the Moon or on Mars, while &lt;a href="https://en.wikipedia.org/wiki/Optimus_(robot)"&gt;an android&lt;/a&gt; (even if used as a telepresence device, it doesn't need to have a full on-board AI) is a much better choice than a human for EVAs, as the entire process of an EVA is both clumsy and dangerous and &lt;a href="https://en.wikipedia.org/wiki/Space_suit"&gt;space suits themselves are complex&lt;/a&gt; to the point of being &lt;em&gt;almost&lt;/em&gt; &lt;a href="https://en.wikipedia.org/wiki/Single-person_spacecraft"&gt;single-person spacecraft&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;AI&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;2024 onwards&lt;/strong&gt;: wildly unpredictable at every stage and in every regard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Narrow AI&lt;/strong&gt;: Lots of breakthroughs each year, but no obvious reason to expect them in any particular order. Which is going to happen first: decoding the &lt;a href="https://en.wikipedia.org/wiki/North_Sentinel_Island"&gt;language of the North Sentinel Islanders&lt;/a&gt; or &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9570045/"&gt;automatically determining synaptic connection strengths in-vivo and &lt;em&gt;without&lt;/em&gt; the current requirement of genetically modifying the brain you're scanning&lt;/a&gt;? And does it really matter?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GenAI — Strike action and economics&lt;/strong&gt;: For much of the industrial era, the only leverage workers have had is to threaten to withdraw their labour. This works for issues of pay and conditions, but it &lt;em&gt;cannot&lt;/em&gt; work where the issue is the owners wanting to have less workers for whatever reason, as you're accidentally threatening the employers with a good time. This also applies to GenAI: for now, LLMs are good enough to be interesting, even good enough to replace interns, but not really good enough to replace a skilled professional with years of experience. The image and video generators are a bit closer to the mark, in that a close eye can still spot the mistakes but a steadily increasing fraction of the population don't look for the things the GenAI do wrong.&lt;/p&gt;

&lt;p&gt;Also note that the quality of AI varies by task, and this argument holds across all tasks: any given AI may be "good enough" to make the members of the Writers Guild of America unemployable before &lt;em&gt;or after&lt;/em&gt; it makes me personally as a software developer unemployable. It's impossible to say at this point, as the kinds of mistakes that current (as of March 2024)  LLMs make have different kinds of impact: a script saying something a little off may be corrected by an actor using "common sense" (whatever that means to you), or it may be a plot hole that unravels the whole thing; some C using a &lt;code&gt;while&lt;/code&gt; loop instead of a &lt;code&gt;for&lt;/code&gt; loop probably won't matter, but &lt;a href="https://en.wikipedia.org/wiki/Ariane_flight_V88"&gt;subtle&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/Therac-25"&gt;code&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/Northeast_blackout_of_2003"&gt;mistakes&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/USS_Yorktown_(CG-48)#Smart_ship_testbed"&gt;frequently&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/Sony_BMG_copy_protection_rootkit_scandal"&gt;cause&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/Heartbleed"&gt;disasters&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/Unreachable_code#Examples"&gt;even&lt;/a&gt; &lt;a href="https://www.engadget.com/2015-05-01-boeing-787-dreamliner-software-bug.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnLw&amp;guce_referrer_sig=AQAAABV1Or8pfJWc3DLnEBZ7rswAecNXVBQha6bXnczdmRXax6fcdyN72QD9t6X5g6WF1W6Ef4Uri-Ew1hGjEo7vRyfH02yOY0WIiAlIE6YtezG3-C7EHKQRCrO7B2ELd3cvpfzXzeyoP7JJdL1NA4XwiG-45zV62SFpXZ7mubwxyXak"&gt;at&lt;/a&gt; &lt;a href="https://www.theregister.com/2020/01/08/boeing_737_ng_cockpit_screen_blank_bug/"&gt;the&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/British_Post_Office_scandal"&gt;best&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/London_Ambulance_Service#Innovation"&gt;of&lt;/a&gt; &lt;a href="https://en.wikipedia.org/wiki/1994_Mull_of_Kintyre_Chinook_crash"&gt;times&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cars/Self-driving systems&lt;/strong&gt;: Again, no idea. Back in 2009, I was absolutely sure that by 2018 it would be possible for a normal person to go to a car dealership and buy a new model whose self-driving AI was so good that the car didn't have, or need, a steering wheel. This clearly didn't happen, even though &lt;a href="https://thelastdriverlicenseholder.com/2023/01/05/waymos-new-robotaxi-without-steering-wheel/"&gt;Waymo has demonstrated limited robo-taxis that meet this requirement in some limited regions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Androids&lt;/strong&gt;: all the problems with self-driving cars, but with a more complex body with more degrees of freedom, and operating in even more diverse environments. Even ignoring the need for both governments and public liability insurers to be satisfied, I expect there to be a 5-10 year gap between the necessary compute requirements fitting into a self-driving car and fitting into a human-sized android.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AI safety/alignment&lt;/strong&gt;: No two people can agree on what "good" is, what counts as "harm" such that any given output can even be considered a risk in need of being avoided in the first place, or if it is possible/good/bad to focus on short-term risks such as personalised propaganda or deepfakes vs. longer-term issues such as getting &lt;a href="https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer"&gt;paperclipped&lt;/a&gt; (which some people insist can't be possible for an AGI, asserting that it "must" share human morals and values, but I've never seen this convince anyone who didn't already believe it). We still have people suggesting we can &lt;a href="https://www.google.com/search?q=just+turn+the+AI+off+site%3Anews.ycombinator.com"&lt;/a&gt;just turn the AI off&lt;/a&gt; at the same time as other people are criticising &lt;a href="https://www.google.com/search?q=OpenAI+ClosedAI+site%3Anews.ycombinator.com"&gt;OpenAI for not releasing model weights&lt;/a&gt; even though releasing model weights basically guarantees someone somewhere will be running it at any given moment (even well past the point better models become available).&lt;/p&gt;

&lt;p&gt;Therefore, I hope — &lt;em&gt;hope&lt;/em&gt; — that we make absolutely no significant progress on state-of-the-art AI capabilities for a few years, even if we spent that time continuing to call each other by the adult equivalent of playground insults rather than &lt;a href="https://www.lesswrong.com/tag/double-crux"&gt;double-cruxing&lt;/a&gt; to figure out what we even think we're talking about.&lt;/a&gt;

&lt;h2&gt;Robotics&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: No interesting developments. We've &lt;em&gt;already got&lt;/em&gt; super-human speed and strength and have had it around the invention of the steam engine, the limit is purely in the intelligence to keep them safe around humans.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AI/Software&lt;/strong&gt;: it will improve over the next decade, but I can't predict how far or how fast. Despite the current interest in humanoid robots, I think we're likely to get more quadrupeds (like &lt;a href="https://bostondynamics.com/products/spot/"&gt;Spot&lt;/a&gt;), wheeled robots, and flying drones, and that for any given role, one of these other form factors will be widely used for several years before androids get good enough at that specific thing to replace them (if indeed they do); and that the AI necessary to generalise over robot bodies will, once developed, take 5-10 years to shrink down to the scale of fitting in the power envelope of a robot.&lt;/p&gt;

&lt;p&gt;Further note: while one can easily imagine separating the brains of a robot from its body, WiFi amongst many other possible solutions, this is not something that scales up to the level of being disruptive to a workforce — if a robot brain takes 2 kilowatts for safe operation at human speed, the current global electricity supply can support 1 billion such robots &lt;em&gt;if that was all it did&lt;/em&gt;. If we take the European experience of "about half the population works about 40 hours per week" (i.e. just under 25% of the time) then any given robot could do the work of 8 people, but that's still &lt;em&gt;using all of our electrical power just for the robots' brains&lt;/em&gt;. I do expect the growth of renewables to boost the available power supply, but on a similar timeframe that I'd expect the compute/model efficiency to improve to the point we don't need it — at earliest, late 2020s/early 2030s, even if the AI in cars (my starting point for the 5-10 year countdown to androids) was ready tomorrow.&lt;/p&gt;

&lt;h2&gt;Biology&lt;/h2&gt;

&lt;p&gt;Too far outside my field to guess — While there have been several interesting developments in the last decade enabled by AI (e.g. cheaper DNA testing and protein folding solutions), and of course mRNA getting famous (even though it wasn't new) for use in Covid vaccinations, I can't guess how this research will turn into things people encounter in their normal lives.&lt;/p&gt;

&lt;p&gt;Long term, &lt;a href="https://ourworldindata.org/life-expectancy#all-charts"&gt;the worldwide life expectancy at birth has gone up 6.3 years in the 20 years between 1999 and 2019 (and the European life expectancy at birth by 5.7 years in the same time period)&lt;/a&gt;, only to drop again 1.8 years worldwide (2.1 years in Europe) over the course of the Covid pandemic. Assuming no more pandemics (a big assumption given the public is more interested in picking between team "wet market" and team "lab leak" rather than team "why not fix both"), and that the recent dip is just a one-off with no lingering issues from the pandemic becoming endemic and/or long-COVID (again, these are big assumptions) that suggests a European life expectancy of 82 in 2029, of 84.8 in 2039, and of nearly 100 in 2093 (and worldwide, this would be 76.1 in 2029, of 79.5 in 2039, and of nearly 100 in 2100).&lt;/p&gt;

&lt;p&gt;That said, I'd be surprised if other trends didn't get in the way of these first: tech enabling synthetic pandemics or suddenly making novel cures much cheaper; global climate changes causing famines, droughts, or lethal heatwaves; rapid introduction of clean energy reducing direct pollution from burning particulate matter and not just long-term climate-impacts; robotics ending dangerous labour, or AI being deployed in malicious or incompetent ways leading to anything from simple bad policy to outright genocide (and where on that spectrum do you put each of &lt;a href="https://en.wikipedia.org/wiki/Cambodian_genocide"&gt;Pol Pot&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Great_Famine_(Ireland)"&gt;the Irish Potato Famine&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Holodomor"&gt;the Holodomor&lt;/a&gt;, and why?); or just straight up &lt;a href="https://en.wikipedia.org/wiki/World_War_III"&gt;World War Three&lt;/a&gt;, a topic about which I hope to never learn via the medium of personal experience regardless of which weapons are used to fight it.&lt;/p&gt;

&lt;h2&gt;3D printing&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Industrial&lt;/strong&gt;: At this point, it looks like a material science question — which crystal structures do you get when adding &lt;em&gt;these&lt;/em&gt; atoms in &lt;em&gt;this&lt;/em&gt; way, and &lt;em&gt;why&lt;/em&gt;. For example, 3D printing diamond: we can already grow &lt;a href="https://en.wikipedia.org/wiki/Synthetic_diamond#Chemical_vapor_deposition"&gt;synthetic diamonds&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=NdsMIG04vHA"&gt;quite easily&lt;/a&gt;, and even &lt;a href="https://www.alibaba.com/product-detail/Microwave-Plasma-CVD-diamond-growing-machine_1600378742541.html?spm=a2700.galleryofferlist.topad_classic.d_title.702f50ebhVGmrJ"&gt;buy pre-made industrial equipment to do this&lt;/a&gt;, so I think it's a question of when we can be bothered to &lt;em&gt;try&lt;/em&gt; 3D printing diamond rather than needing any actual scientific or technological breakthroughs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Biological&lt;/strong&gt;: The dream is bioprinting to replace lost limbs, substitute transplants, improve gender reassignment surgery, etc.&lt;/p&gt;

&lt;p&gt;Unfortunately, every single time I've heard a news story about this, the apparent state of the art has been in exactly the same place — we can do skin and cartilage, so you can always get a new nose or ears, and there's always a demo of something shaped like an internal organ, but for some reason a few years later, nothing has changed.&lt;/p&gt;

&lt;p&gt;Therefore, I don't know. Are we at the threshold of a new era of medicine, or not? I can't tell.&lt;/p&gt;

&lt;h2&gt;Power&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Fusion&lt;/strong&gt;: Wishful thinking, but not because it's eternally 20 years away. Unless we somehow get really lucky and the fusion reactor designs cannot — are actually incapable of, rather than the manufacturers opting not to — run on neutron-emitting chains, the &lt;em&gt;mere ability&lt;/em&gt; to make a fusion reactor is also the ability to produce large quantities of neutron radiation on demand; neutron radiation makes it relatively easy to create radioisotopes suitable for fission bombs. Thus, any small projects that seem likely to succeed are likely to be shut down as security risks. &lt;a href="https://en.wikipedia.org/wiki/ITER#/media/File:ITER_Tokamak_and_Plant_Systems_(2016)_(41783636452).jpg"&gt;ITER isn't small&lt;/a&gt;, so it isn't in this category; this distinction matters because you can't hide ITER, while you could easily hide &lt;a href="https://www.helionenergy.com/polaris/"&gt;something the size of Helion Energy's 'Polaris' design&lt;/a&gt;.

&lt;p&gt;&lt;strong&gt;Space-based beamed power&lt;/strong&gt;: A fundamentally terrible idea on Earth, as any mechanism you can use for getting the power down from orbit can also be used to get the power up from the ground on the other side of the planet first and then down elsewhere. Also, you don't have to just make this safe from the perspective of "can we avoid an industrial accident" (and I don't trust anyone who says they have a system that can't go wrong even in theory, as practice keeps demonstrating new and exciting ways to beat theory), you also have to prove safety to the standard of &lt;em&gt;would you allow a potentially hostile government, that refuses to let you inspect it, launch this into an orbit such that it might, potentially, get pointed at something you care about&lt;/em&gt;? Even if you're sure the radio waves can't hurt you, do you &lt;em&gt;really&lt;/em&gt; want to gamble that they've not stuck some Hubble-sized optics on it with a huge laser? Now imagine you're in charge of Russia, who have anti-satelite weapons and shown a willingness to shoot stupid targets &lt;a href="https://en.wikipedia.org/wiki/Kosmos_1408"&gt;both in space&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Zaporizhzhia_Nuclear_Power_Plant_crisis"&gt;on the ground&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solar&lt;/strong&gt;: Will keep growing ridiculously fast until it's around half of all demand, that demand may also increase thanks to the rapid increase in supply from PV.&lt;/p&gt;

&lt;p&gt;In 2022, the installed capacity was about 1061.7 GW, with a capacity factor of 14% and actual average output of 149.5 GW, and the long-term compounding growth rate is 22%/year. Current global electrical demand is about 2000 GW, so at a minimum I expect growth to remain until 1000 GW average output, which should happen around 2032 (± 1 year). Non-electrical power use is an additional 18,000 GW, and there are a lot of people who don't currently have any significant electricity supply, so there is plenty of opportunity to go past this number. On the other hand, wind is a good competitor…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wind&lt;/strong&gt;: Will also keep growing ridiculously fast, just not &lt;em&gt;quite&lt;/em&gt; as fast as solar.&lt;/p&gt;

&lt;p&gt;In 2022, the installed capacity was about 898.86 GW, less than PV, but the capacity factor was 27% which means it actually supplied more useful power overall (an average of 239.55 GW compared to PV's 149.5 GW). However the growth rate is also somewhat slower than PV, being only 7.3% in that year — the compound rate is slightly better over longer periods, but is less predictable, I think 15% is about right, which results in wind having a 4x growth in a decade, whereas PV gets 4x growth in about 6 years.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Oil, gasoline&lt;/strong&gt;: Still in significant use for heating and chemical feedstocks for e.g. plastics, but rapidly disappearing from everything else. Regardless of the current legal/political position, you should expect combustion-based cars to go from "standard" to "weird specialist thing" by 2032, and they &lt;em&gt;might possibly&lt;/em&gt; no longer be available for purchase at all. Existing vehicles last for around a decade, so assume fuel stations still provide pumps and not just charging sockets at this point.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aviation fuel&lt;/strong&gt;: Will almost certainly remain for long-haul flights. Batteries are better than they seem at first glance because of the engine efficiency (so don't be surprised if light aircraft switch to them), but it's implausible they'll replace jet engines for flights over 2 hours. Changing fuel (hydrogen or methane, perhaps from electrolysis or Sabatier process respectively) is possible on paper, but likely to be a huge deal in retrofitting — I won't say "impossible", but I'd be surprised if old aircraft are updated, and instead expect a rule sometime around 2030-ish that says new aircraft need to use a different fuel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Self-charging cars&lt;/strong&gt;: If you cover a car in PV, those cells can supply about 80%-90% of the &lt;em&gt;mean&lt;/em&gt; vehicle's requirements. This isn't "all", so there will still be a need for charging stations, and it's also "mean" because while some people only drive 10 miles a day to the shops or to work, other people drive a few hundred every single day as part of their job. The electricity has to come from &lt;em&gt;somewhere&lt;/em&gt; regardless of how much you use, and increasing electrification of vehicles puts more demand on the grid, so I suspect that governments will require vehicles to do this in order to reduce strain on the grid, even though it's not a complete solution. As electrification is a major goal and many western countries are &lt;em&gt;already&lt;/em&gt; &lt;a href="https://theconversation.com/the-old-dirty-creaky-us-electric-grid-would-cost-5-trillion-to-replace-where-should-infrastructure-spending-go-68290"&gt;facing a huge bill for grid upgrades&lt;/a&gt;, I expect this sooner rather than later, probably some time around the end of this decade.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Global power grid&lt;/strong&gt;: The idea is quite simple: electrical resistance goes up with the length of a wire, and down by making the wire thicker. If you want a 1Ω loop of aluminum all the way around the equator and back to where you stared, you can just plug numbers into a simple equation to learn it will have &lt;a href="https://www.wolframalpha.com/input?i=40000km+*+resistivity+aluminium+%2F+1m%5E2"&gt;a cross section of almost exactly 1 square meter&lt;/a&gt;, which has &lt;a href="https://www.wolframalpha.com/input?i=40000km+*+1m%5E2+*+density+aluminium"&gt;a mass of 1.1e11 kg&lt;/a&gt;, which is about &lt;a href="https://en.wikipedia.org/wiki/List_of_countries_by_aluminium_production"&gt;1.7 years of current global production&lt;/a&gt; for something that at &lt;a href="https://www.dailymetalprice.com/metalpricecharts.php?c=al&amp;u=kg&amp;d=0"&gt;current spot prices&lt;/a&gt; would cost &lt;a href="https://www.wolframalpha.com/input?i=40000km+*+1m%5E2+*+density+aluminium+*+aluminium+price"&gt;only around $265 billion&lt;/a&gt;. This is much cheaper than using energy storage solutions for the night/winter/&lt;a href="https://en.wikipedia.org/wiki/Dunkelflaute"&gt;Dunkelflaute&lt;/a&gt; problem of renewables.&lt;/p&gt;

&lt;p&gt;(Note: you don't want to put it on the equator, and you don't want it to be a single line: if you're putting 2 TW through it at reasonable voltages, the surface magnetic field is strong enough to be a hazard).&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, in practice this would require an unprecedented level of global cooperation, so while China may well prove the concept by connecting themselves to both Africa and Chile, I'd be very surprised if this simple solution is actually going to change the world any time soon.&lt;/p&gt;

&lt;h2&gt;Consumer electronics&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Nanolithography (e.g. CPU manufacture)&lt;/strong&gt;: Reaches its limits. The current node (which isn't really the feature size any more) is 2 nm process. If the recent rate of change continues, we reach 1 nm/10 &lt;a href="https://en.wikipedia.org/wiki/Angstrom"&gt;Å&lt;/a&gt; in 2028, 0.5 nm/5 Å in 2032, and as the &lt;a href="https://en.wikipedia.org/wiki/Lattice_constant"&gt;lattice constant for silicon is (roughly) 5.4 Å&lt;/a&gt; this is as far as resolution goes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Computer energy efficiency&lt;/strong&gt;: This is not as constrained as feature size, so it's at least possible that this will continue improving &lt;a href="https://en.wikipedia.org/wiki/Koomey%27s_law"&gt;past 2032 — perhaps as far as 2080&lt;/a&gt;. Not certain, but possible. If it does continue at current rates, (doubling every 2.6 years), they'll be about x5 today's power efficiency in 2030, about x18 relative to today by 2035, and about x71 compared to today by 2040.&lt;/p&gt;

&lt;p&gt;The power constraint, rather than absolute computational capacity or minimum feature size, is the constraint for all mobile devices — not just mobile &lt;em&gt;phones&lt;/em&gt;, but also laptops, cars, androids, AR/VR glasses, smart dust, etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AR/VR glasses&lt;/strong&gt;: will get better over time. I'd expect widespread adoption of low-profile headsets like the XREAL AR glasses some time around 2028 ± 2 years, but the sci-fi vision requires &lt;a href="https://en.wikipedia.org/wiki/Holography"&gt;true holographic (in the sense of interfering wavefronts)&lt;/a&gt; displays — which are possible, but need a lot of computational power at the moment (perhaps we'll make this more efficient, but otherwise it's going to be a wait).&lt;/p&gt;

&lt;p&gt;AR/VR contact lenses are currently limited to tech demos; while I won't be surprised to see a launch along the lines of the original &lt;a href="https://en.wikipedia.org/wiki/Google_Glass"&gt;Google Glass&lt;/a&gt;, I'd expect them to &lt;em&gt;flop&lt;/em&gt; like the Google Glass. To become a breakout item, they need both true holographic displays, and also significant improvements in computational efficiency which I don't expect this decade.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Smart watches&lt;/strong&gt;: Despite all the love they get in the tech world, I don't see how they add value. They feel like fashion pieces to me, so will continue to be popular, but I don't anticipate any new uses beyond what we've already got.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3D TV&lt;/strong&gt;: No real changes. True holograms are possible, the computational requirements are less of a concern when it's a box on the wall rather than a tiny thing on or next to your eyeball, so this is a question of market demand — which, last time round, was driven mainly by &lt;a href="https://en.wikipedia.org/wiki/Avatar_(2009_film)"&gt;one specific film&lt;/a&gt;, so it &lt;em&gt;could&lt;/em&gt; happen, don't rule it out, but don't assume it either.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Smart dust&lt;/strong&gt;: This is a big one! If you've not already heard of it, the idea is very simple: miniaturise a computer until it's the size of a grain of dust, use it for monitoring. I think we've already got all the stuff we need for this, but there's been no reporting for a while now — so my guess is the US military &lt;em&gt;already has it&lt;/em&gt; and has a gag order and/or NDA with the companies making it. It's a perfectly reasonable bit of kit for them to want to own, and &lt;em&gt;absolutely&lt;/em&gt; a piece of kit they'd want to try to find defences against before someone else uses it against them.&lt;/p&gt;

&lt;p&gt;I recon a consumer grade version of this will happen some time between 2026 and 2028; and if you embed it in a transparent lacquer you get &lt;em&gt;smart paint&lt;/em&gt;, which will be painfully expensive at first, but a reasonable (if fancy) business/prosumer item some time around 2030 — expect it to replace whiteboards, if they've not already gone away due to very large conventional touch-sensitive displays.&lt;/p&gt;

&lt;p&gt;Smart dust also means &lt;em&gt;smart tattoos&lt;/em&gt;, which will make it even harder to tell if someone is cheating in an exam; but as they also have the potential to mess with facial recognition and eye witnesses, assume it will be banned for use in tattoos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phones&lt;/strong&gt;: Apple and Google will probably lose their arguments, the stores will be opened up. I don't know how this will turn out, given there are many strong drives pulling in different directions. I'm an iOS developer, so I get to see a short way behind the scenes — US government wants to know &lt;em&gt;all about&lt;/em&gt; the use of cryptography in an app. Is the EU going to allow this? I think not, and that they have to force Apple away from the US government because of this. Does the US want to lose this advantage? I think not, so they will try to make sure that Apple's rules &lt;em&gt;de-facto&lt;/em&gt; force all developers worldwide to continue to be tied to the &lt;em&gt;de jure&lt;/em&gt; reporting requirement.&lt;/p&gt;

&lt;p&gt;The phone hardware upgrade cycle will remain flashy big numbers that make no real difference — how many megapixels is your camera and why do you care, you're not a pro photographer — until some new sensors are included. This might be infrared, either near-IR (night vision, easy, cheap) or far-IR (thermal imaging, moderately expensive) or both. It might be an option to use the WiFi as a wall-penetrating radar/life signs detector (a &lt;a href="https://spectrum.ieee.org/household-radar-can-see-through-walls-and-knows-how-youre-feeling"&gt;surprisingly easy trick&lt;/a&gt;). It might be chemo-sensors such as carbon monoxide — although the importance of carbon monoxide detection will decrease as fossil fuels are phased out, this is just an example to give you an idea for the kind of thing to look for.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Games consoles&lt;/strong&gt;: we may be nearing the ultimate form, thanks to generative AI. Although we're not &lt;em&gt;quite&lt;/em&gt; at the level of real-time GenAI models, we've gone from 90 seconds-per-frame about 18 months ago to 4-5 frames-per-second this month, and these models can be used to generate new content with poses or other content hints from existing images. The result? I suspect that, by 2028, the graphics pipeline of most games will be to first render an &lt;em&gt;extremely crude&lt;/em&gt; representation where everything's just a coloured cuboid, then pass this into a generative AI which takes those cuboids as hints and turns them into something &lt;em&gt;genuinely photorealistic&lt;/em&gt; (assuming photorealism is the desired art style; substitue for whatever else if not).&lt;/p&gt;

&lt;p&gt;Photorealism, long a dream for 3D games, will be available for all games, and basically for free because "reality" has unlimited un-copyrighted training data and there's nobody to argue you can't or shouldn't.&lt;/p&gt;

&lt;h2&gt;Legal&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Privacy&lt;/strong&gt;: I don't see how this can survive much longer, assuming it's even still around today. The more tech we have, the more potential there is for a single individual to cause mayhem. The more mayhem is caused by single individuals, the more demand there will be for the police to put people under surveillance. The tech to put &lt;em&gt;everyone&lt;/em&gt; under almost constant surveillance already exists, and is fairly cheap — even ignoring that most of us already purchase several devices which can be used this way, and for which it is reasonable to assume (never mind post-Snowden, just by looking at the Investigatory Powers Act 2016 in the UK) that governments subpoena information from.&lt;/p&gt;

&lt;p&gt;The counter to this is: it's not just governments who get to spy on you, blackmailers can do it too. It's not in any government's interest for its population to get blackmailed: it's bad enough when they're local criminals who spend money on the local economy, but we're in a global environment, and international blackmail is even worse from the point of view of governments. I think this combination will force governments to do some combination of &lt;em&gt;perfect enforcement&lt;/em&gt; (blackmailers can't find your dirty laundry if the justice system has already named-and-shamed you) and/or &lt;em&gt;radically&lt;/em&gt; de-criminalising things — no cute mixed metaphor here, just if your actions aren't seen as bad, them being public is much less of a threat. Less, but…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Social networks&lt;/strong&gt;: Blackmail is unavoidably tied to social networks, even without legal implications, one can be blackmailed by &lt;em&gt;shame&lt;/em&gt;. Thus the concept which the American right wing calls "cancel culture", which the American left calls "blacklisting", and which I've also heard called "social exclusion" or "social marginalisation" in less-politicalised discussion. This is a very easy thing for us humans to do, and to do for things which are not at all illegal — indeed, for certain categories we have laws banning the exclusion/marginalisation itself, and yet it still happens.&lt;/p&gt;

&lt;p&gt;It is currently unclear how much power social networks actually have: I think, on the basis of how bad the adverts they show me are, that they are much worse than they tell themselves or their customers. If social networks are perceived as powerful, they will be regulated in order to limit this kind of dynamic; if social networks are perceived as powerless, they will not be regulated in this way.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#3D Printing'&gt;3D Printing&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Artificial intelligence'&gt;Artificial intelligence&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Bioprinting'&gt;Bioprinting&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Elon Musk'&gt;Elon Musk&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Futurology'&gt;Futurology&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Notes'&gt;Notes&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Opinion'&gt;Opinion&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Robotics'&gt;Robotics&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#SpaceX'&gt;SpaceX&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Technocracy'&gt;Technocracy&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Technological Singularity'&gt;Technological Singularity&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Technology'&gt;Technology&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#biology'&gt;biology&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#black swan'&gt;black swan&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#brain scans'&gt;brain scans&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#civilisation'&gt;civilisation&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#drones'&gt;drones&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#economics'&gt;economics&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#forecast'&gt;forecast&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#fusion reactors'&gt;fusion reactors&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#future of humanity'&gt;future of humanity&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#futurism'&gt;futurism&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#prediction'&gt;prediction&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#robots'&gt;robots&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#rockets'&gt;rockets&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#space'&gt;space&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#wild speculation'&gt;wild speculation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Futurology'&gt;Futurology&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Technology'&gt;Technology&lt;/a&gt;&lt;/p&gt;</description>
      <pubDate>Sat, 23 Mar 2024 17:24:34 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/03/23-17.24.34.html</guid>
    </item>
    <item>
      <title>Newspapers, copyright, artists, and GenAI</title>
      <link>https://benwheatley.github.io/blog/2024/03/19-14.53.05.html</link>
      <description>&lt;h1&gt;Newspapers, copyright, artists, and GenAI&lt;/h1&gt;

&lt;p&gt;The New York Times has sued OpenAI for copyright infringement. Authors and artists have sued image and text generators similarly.&lt;/p&gt;

&lt;p&gt;I am sympathetic to the artists and writers (and not just because I've been working on a novel for far too long already without finishing it): these are famously low-income professions, and these automations are likely to render them as economically useful as blacksmiths or &lt;a href="https://en.wikipedia.org/wiki/Cooper_(profession)"&gt;coopers&lt;/a&gt;. Even if&lt;sup&gt;&lt;a href="#footnote-0"&gt;[0]&lt;/a&gt;&lt;/sup&gt; they win their current lawsuits, even if the famous AI models are forced to be deleted and any new have to be made with only suitably licensed material, the reason I know there's enough freely licensed material for them to still be in trouble is that &lt;a href="https://huggingface.co/Mitsua/mitsua-diffusion-one"&gt;such models have already been made&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The newspapers, though? I can't find it in me to be sympathetic to the newspapers.&lt;/p&gt;

&lt;p&gt;First, as faceless corporations, they're somewhat inhuman at the best of times. This isn't a good reason in any objective sense, but it tells you where the limits of the empathy in my mind is.&lt;/p&gt;

&lt;p&gt;Second, and more pertinently, newspapers have a long history of getting things wrong. You've probably heard a quote that's (&lt;a href="https://quoteinvestigator.com/2016/12/03/misinformed/"&gt;incorrectly!&lt;/a&gt;) attributed to &lt;a href="https://marktwainstudies.com/the-apocryphal-twain-if-you-dont-read-the-newspaper-youre-uninformed-if-you-do-youre-misinformed/"&gt;Mark Twain&lt;/a&gt;: &lt;a href="https://www.snopes.com/fact-check/mark-twain-read-newspaper-misinformed/"&gt;"If you don’t read the newspaper you are uninformed, if you do read the newspaper you are misinformed."&lt;/a&gt;. For those disinclined to follow those links, the general attitude represented by the quote goes back to at least Thomas Jefferson in 1807, though as this is about an English language quote and the earliest newspapers &lt;a href="https://en.wikipedia.org/wiki/List_of_the_oldest_newspapers"&gt;weren't in English&lt;/a&gt; I'd expect equivalent complaints to have been made in other languages that just weren't relevant to the quote researchers. You may also have heard of the Gell-Mann amnesia effect: "&lt;a href="https://en.wiktionary.org/wiki/Gell-Mann_Amnesia_effect"&gt;[t]he phenomenon of people trusting newspapers for topics which they are not knowledgeable about, despite recognizing them to be extremely inaccurate on certain topics which they are knowledgeable about&lt;/a&gt;".&lt;/p&gt;

&lt;p&gt;Now, does the idea in these quotes ring any bells? To me, the idea sounds like the term "hallucination"&lt;sup&gt;&lt;a href="#footnote-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; in the context of AI: &lt;a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)"&gt;a response generated by an AI which contains false or misleading information presented as fact&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I'm told that you can't copyright facts. You can, in some jurisdictions, get a &lt;a href="https://en.wikipedia.org/wiki/Database_right"&gt;database right&lt;/a&gt;&lt;sup&gt;&lt;a href="#footnote-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt;, but that's a different thing. Conditional on this, an already-trained LLM can be used to read all newspapers (if search engines can process websites to construct indexes, why not?), extract all the claims of fact within each article along with their sources, then cross-reference each claim to make sure they're well-attested, and write up the results in the style of an out-of-copyright or public-domain author. The newspapers lose even harder than the artists and authors, because the AI is so cheap &lt;em&gt;it can now afford to actually check the sources&lt;/em&gt; — and that's not merely theoretical: the free version of ChatGPT doesn't do this, but it's very easy to work with underlying API to enable this kind of thing.&lt;/p&gt;

&lt;p&gt;So, I think newspapers can't really be protected by copyright even if they win these cases. Even with database rights, the only way they can really defend themselves is by writing fiction (not mere opinion, fiction, and then what's the point of the paper?), or by having exclusive access to some source unavailable to other papers — which, to be clear, I agree is both &lt;em&gt;really useful&lt;/em&gt; and &lt;em&gt;does happen&lt;/em&gt;, the problem is that when this happens with regard to an important story, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Jeffrey_Archer#Perjury_trial_and_imprisonment"&gt;even when the papers are correct they can lose, and have lost, court cases over it&lt;/a&gt;&lt;/em&gt;, or just get &lt;a href="https://www.theguardian.com/media/2015/feb/27/guardians-destroyed-snowden-laptop-to-feature-in-major-va-show"&gt;paid a visit by men in suits under orders to destroy their laptop&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What remains is, rather unfortunately given the importance of a free press, that LLMs and papers are really competing for the attention of those who find them compelling, not those who seek the truth — &lt;a href="https://en.wikipedia.org/wiki/Goodhart%27s_law"&gt;Goodhart's law strikes again&lt;/a&gt;.&lt;/p&gt;

&lt;p id="footnote-0"&gt;[0] I'm absolutely not qualified to guess the outcomes. I know &lt;a href="https://arxiv.org/pdf/2401.16212.pdf"&gt;vastly less law than even the free version of ChatGPT&lt;/a&gt; and yet it would still be &lt;a href="https://www.bbc.com/news/world-us-canada-65735769"&gt;deeply unwise to trust ChatGPT to answer legal questions&lt;/a&gt;.

&lt;p id="footnote-1"&gt;[1] Personally I prefer the alternative names "confabulation" and "delusion".&lt;/p&gt;
 
&lt;p id="footnote-2"&gt;[2] Also: is it reasonable that newspapers have a 70 year monopoly on this stuff? Does society really benefit from them being able to sue for unpaid use of the text of, for example, &lt;a href="https://www.britishnewspaperarchive.co.uk/search/results?basicsearch=1974%20grill%20cheff%20required&amp;retrievecountrycounts=false"&gt;a 1974 job advert for grill chefs&lt;/a&gt; or a &lt;a href="https://www.britishnewspaperarchive.co.uk/search/results?basicsearch=1994%20factory%20explosion&amp;somesearch=1994%20factory%20explosion&amp;exactsearch=false&amp;retrievecountrycounts=false&amp;sortorder=score"&gt;1994 factory explosion&lt;/a&gt;?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Copyright'&gt;Copyright&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Opinion'&gt;Opinion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#AI'&gt;AI&lt;/a&gt;&lt;/p&gt;</description>
      <pubDate>Tue, 19 Mar 2024 14:53:05 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/03/19-14.53.05.html</guid>
    </item>
    <item>
      <title>A Brilliant end</title>
      <link>https://benwheatley.github.io/blog/2024/03/11-12.00.16.html</link>
      <description>&lt;h1&gt;A Brilliant end&lt;/h1&gt;

&lt;img src="Brilliant_progress.png" style="aspect-ratio: 168/1193; float:right"&gt;

&lt;p&gt;I was gifted a Brilliant.org subscription for 2019's Christmas, and for a time it was good. I worked through the courses and learned many new things. Hat puzzles, formal logic, contest maths. Group theory, applied statistics, machine learning. All good stuff.&lt;/p&gt;

&lt;p&gt;They added more courses. I learned more from their contributing authors — Sabine Hossenfelder's quantum mechanics course, MinutePhysics's special relativity, and one that doesn't name the author but taught me how photovoltaics actually function. And for a time it was good.&lt;/p&gt;

&lt;p&gt;But the UI changed. It became harder to use. Tab no longer goes between text fields. Just like Duolingo, it's gained "cute" animations that take me out of a flow state. The app &lt;em&gt;used&lt;/em&gt; to allow courses to be downloaded for offline use — sometimes that didn't work, but usually it did. Now? There's no option to download, and each lesson takes ages to load; so long in fact that, even on home WiFi, I see popup messages asking me if I'd like to give up waiting for the course to load, and that can happen multiple times before I can start.&lt;/p&gt;

&lt;p&gt;Sometimes lessons were updated or courses replaced. That's not a bad thing in itself, but what was a problem was that the progress bar for each course was no longer consistent across the UI, showing different levels of progress in different places.&lt;/p&gt;

&lt;p&gt;Now, though… well, now I'm stopping. My subscription has run out, and I won't be renewing it, at least not for some time. The new courses I've been seeing recently have been focussing on the low end — I absolutely do not benefit from yet another "here's how triangles work!" course, such content is an absolute waste of my time at this point. Where I'm stuck now is the hard stuff — abelian groups, curls, wedge products, Hessians and Laplacians — which seem as hard now as it ever was. I've reached the point where I don't understand the notation, and without &lt;em&gt;those specific courses&lt;/em&gt; being expanded, just re-reading the existing content isn't going to help me understand.&lt;/p&gt;

&lt;img src="Screenshot_2024-03-11_at_11.36.26.png" style="aspect-ratio: 646/345; width: min(100% - 168px, 646px);" /&gt;

&lt;p&gt;Actually, it's worse than that. At this point, ChatGPT — the free tier — knows more than I do about these topics. This isn't to claim the LLM is in any objective sense good at this level (some of the problems are such that it's possible for me to verify the AI is wrong even though I can't generate any answers in the first place, kinda like P!=NP), I'm just asserting a relative performance between it and me: it knows more than I, personally, know. And this is the worst the AI will ever be.&lt;/p&gt;

&lt;p&gt;I may come back to Brilliant again in the future. But I suspect not — by the time they release the content I'm interested in, the LLMs will be teaching it for free… or just directly doing the tasks for which I wanted to learn these topics.&lt;/p&gt;

&lt;p&gt;All Good Things…&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#Education'&gt;Education&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#Learning'&gt;Learning&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#MOOC'&gt;MOOC&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/tags.html#STEM'&gt;STEM&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#AI'&gt;AI&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Opinion'&gt;Opinion&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Software'&gt;Software&lt;/a&gt;,
&lt;a href='https://benwheatley.github.io/blog/categories.html#mathematics'&gt;mathematics&lt;/a&gt;&lt;/p&gt;</description>
      <pubDate>Mon, 11 Mar 2024 12:00:16 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2024/03/11-12.00.16.html</guid>
    </item>
    <item>
      <title>Analogy: LLMs as CISC, MoE as RISC</title>
      <link>https://benwheatley.github.io/blog/2023/12/12.html</link>
      <description>&lt;h1&gt;Analogy: LLMs as CISC, MoE as RISC&lt;/h1&gt;

&lt;h2&gt;Background, what's the problem?&lt;/h2&gt;

&lt;p&gt;The big thing of 2023 has been ever-larger Large Language Models, each of which gets sold as "can do basically everything" (with small print for all the things it can't do). This works very well, but it also needs hardware in the price range "if you have to ask you can't afford it". So, can we do better?&lt;/p&gt;

&lt;p&gt;I think we can do better.&lt;/p&gt;

&lt;p&gt;If you'll permit a loose analogy, there are two ways you can design a CPU: a huge number of defined instructions that each perform relatively complex actions (CISC), or a small number of defined instructions each of which performs only a simple instruction. As a child of the 80s-90s, my go-to example of this was how Apple presented the PowerPC chips in its late-90s Macs as being better than Intel chips of the same era and clock speed, because PowerPC was RISC and Intel was CISC. (If you're about to ask "Didn't Apple just switch &lt;em&gt;from&lt;/em&gt; Intel chips to their own ones based on ARM cores?", yes, they did. Macs started with Motorola, then PowerPC, then Intel, then their mobile chips got better than Intel chips and they switched Macs to the M-series).&lt;/p&gt;

&lt;p&gt;LLMs are very large and complex, and perform a huge number of calculations just to get the next "token" (think "word fragment"), and while these are incredibly impressive relative to the previous state of the art for natural language processing, the previous state of the art was &lt;em&gt;terrible&lt;/em&gt;. I see them for all their flaws as well as all their strengths, so anyone who tries to argue with me &lt;em&gt;either&lt;/em&gt; that they're "just stochastic parrots" &lt;em&gt;or&lt;/em&gt; that they're the final missing puzzle piece for The Singularity — no, they're much better than parrots (and also such clichés are humans acting like stochastic parrots themselves), but also no, they're obviously more book-smart than world-smart (I mean, what did you expect, they're a brain the size of a shrew that was given a subjective experience equivalent to reading random websites for 50,000 years).&lt;/p&gt;

&lt;p&gt;So, people know about this already, what are they already doing to reduce this?&lt;/p&gt;

&lt;p&gt;There's a thing called "Mixture of Experts" (MoE), but we're still very early days with this for now (not chronologically, it's been around for a while, but it's not well developed). Currently, as I understand it, MoE is a collective of (relatively) smaller models, such that only a few of these models are ever executed. So far, so good. But from what I hear from researchers, it's &lt;em&gt;really hard&lt;/em&gt; to divide problems between them, both for inference and for training — this shouldn't be too surprising, the inputs can be &lt;em&gt;anything&lt;/em&gt; in the input domain (i.e. "things someone could type" for a pure-text chatbot, or that plus pictures if it's supposed to understand images as well).&lt;/p&gt;

&lt;h2&gt;What do I suggest?&lt;/h2&gt;

&lt;p&gt;So, that's the statement of where we are and what the problem is; what's my suggestion for a solution, or at least a direction to a soution?&lt;/p&gt;

&lt;p&gt;Architecture:&lt;/p&gt;

&lt;img src="Meta-learning_AI.png" style="aspect-ratio: 750/397; width: min(100%, 750px);" /&gt;

&lt;p&gt;
&lt;ul&gt;
&lt;li&gt;Controller: Takes the system input, uses it to query the model collection until it finds a model that "knows what to do" with that input, or invokes the model creator when there is none.&lt;/li&gt;
&lt;li&gt;Archive memories: All "important" experiences.&lt;/li&gt;
&lt;li&gt;Model creator: A function which creates new models when no existing model can deal with the input.&lt;/li&gt;
&lt;li&gt;Collection of (relatively) small models — each model consists of:
    &lt;ul&gt;
    &lt;li&gt;A meta-model: a fast running test which says, conditional on the current input experience, whether or not the model &lt;em&gt;should&lt;/em&gt; be executed.&lt;/li&gt;
    &lt;li&gt;The model itself&lt;/li&gt;
    &lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;There are of course many ways to approach each part of this, and which one is most appropriate is an open question.&lt;/p&gt;

&lt;p&gt;For example:
&lt;ul&gt;
&lt;li&gt;It could be possible to take either &lt;em&gt;any&lt;/em&gt; model whose meta-model says it can understand the data, or whichever model's meta-model is most confident, or take many models which "think they know how to respond" and perform a standard MoE combination of those models. I have yet to explore which would work best.&lt;/li&gt;
&lt;li&gt;At training time (which could be continuous, but doesn't have to be), the controller also needs to have a feedback system which allows whichever model(s) were activated to be improved based on their answer. It may be useful for meta-models which "thought they might know but weren't sure" (i.e. close to the threshold for being included in the set of models which might have answered) also get updated.&lt;/li&gt;
&lt;li&gt;It would also be possible for the model creator could itself be a model, whose meta-model is "all the other model's meta-models can't cope with this".&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;Note: at time of writing, this represents "things I want to try", I've not written any experiments with this. I intend to start with a toy model, probably the MNIST handwriting set, where each model/meta-model is trying to recognise just one of the digits, and the model creator is a short python script. It would also be possible to explore a similar approach with a much more capable model, such as using a LLM as a controller to query (and via some appropriate API, invoke and use the output of) a database of code functions, and another LLM as a model creator to create new maths functions in some programming language.&lt;/p&gt;

&lt;hr /&gt;
</description>
      <pubDate>Tue, 12 Dec 2023 12:00:00 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2023/12/12.html</guid>
    </item>
    <item>
      <title>The conservative-radical split over AI</title>
      <link>https://benwheatley.github.io/blog/2023/11/19-19.53.55.html</link>
      <description>&lt;h1&gt;The conservative-radical split over AI&lt;/h1&gt;
&lt;p&gt;This was written less than 24 hours after Sam Altman was fired as CEO of OpenAI and no meaningful public information is available (not that this has stopped the speculation), and just after I'd reached the point in the Cambridge Union video of Sam Altman accepting the 2023 Hawking Fellow award on behalf of OpenAI where the protestors drop signs saying "OpenAI’s race threatens democracy and humanity" and "Say No".&lt;/p&gt;

&lt;p&gt;If you ask ten people for their views on AI, you'll probably get 20 different answers.&lt;/p&gt;

&lt;p&gt;Of such patterns are witty remarks often made, but unfortunately in this instance it also feels correct, and given the strength of feelings, that's bad.&lt;/p&gt;

&lt;p&gt;On one end, arguments from e/acc (Effective Acceleration) resemble the fervor of fundamentalist Christians trying to bring about the apocalypse in Revelations, both believing it leads to a 'Good Ending'. At the other end? I can't tell how serious anyone was about Roko's basilisk (same idea, but assuming Satan wins the battles in Revelations and asking who Satan will punish least), but we &lt;em&gt;definitely&lt;/em&gt; have Yudkowsky worried about paperclip optimisers (basically the &lt;em&gt;reductio ad absurdum&lt;/em&gt; of how capitalism can go wrong, except there's no recognisable human mind behind it so it loses the "ad absurdum" part). Even at lesser extremes, the same axis also has those who fear other countries (or companies) are racing to make their own AI, vs. those who fear the short-term dangers from the AI.&lt;/p&gt;

&lt;p&gt;But what of those short-term benefits and dangers? On employment issues, critics of the AI that already exists place it in a Schrödinger's cat state of managing to be simultaneously "not a real artist" and "taking artists' jobs" (mirroring a common argument used against immigrants), while proponents want to automate as much as possible to boost the productivity of our economy to stave off the negative impacts of things like the demographic shift in developed nations that has seen birth rates collapse at the same time as lifespans increase.&lt;/p&gt;

&lt;p&gt;Even without future AI, we have people arguing about the ability level of state-of-the-art AI, with some dismissing the best as a "stochastic parrot" or "autocomplete on steroids" (including Yann LeCun who said this would be an insult to parrots, he is a Turing Award winner so his opinions absolutely shouldn't be dismissed), to those praising it as already being a weak form of AGI that passes all the IQ tests we throw at it (the gpt-4 family of models do in fact pass those tests, but writing as someone who scored "148" on an (online) IQ test and "off the charts" on a Cognitive Abilities Test when I was at school, trust me when I say that IQ tests are only &lt;em&gt;loosely&lt;/em&gt; correlated with the thing people care about when they talk about intelligence even in humans, and that AI are &lt;em&gt;weird&lt;/em&gt; even by human standards).&lt;/p&gt;

&lt;table style="float: right; width: min(50%, 256px);"&gt;&lt;tr&gt;&lt;td&gt;&lt;img style="aspect-ratio: 824/1606; width: min(100%, 256px);" alt="Screenshot_2023-11-18_at_19 59 09" src="https://github.com/BenWheatley/blog/assets/12123132/8a4d14e0-ee27-4be0-9a91-f8f51d51dcce" /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Try explaining this with a flow chart&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Discussions about fairness in AI trace back to the era when "AI" involved simple hand-written flow-chart-style algorithms — people could folllow how they worked, and wanted to be empowered to dispute decisions they disagreed with; now that the AI is a magical mystery box of barely comprehensible matrix multiplication that &lt;em&gt;somehow&lt;/em&gt; can translate poorly written requests in English into mostly correct JavaScript with German comments, we generally &lt;em&gt;can't&lt;/em&gt; explain how it reached a conclusion. We can ask, but if we ask after the event instead of before hand, it's going to confabulate a reason (much like real humans) giving the illusion of reasonableness without actually being reasonable. But it gets worse: if the people collecting the training set either miss out an important category of examples (which happens all the time even with humans, this is what led to the development of the practice of tokenism after which the South Park character formerly known as "Token" was initially named) then we get automatic soap dispensers that don't work if you have too much melanin in your skin; likewise when the training set includes unnecessary categories that confuse the AI, which is what happened in 2015 when Google's automated image labelling misclassified black people as "gorillas". But it gets &lt;em&gt;even worse&lt;/em&gt;: the default training set is broadly the public internet, so if you can't filter out the toxic content ahead of time, your AI will be learning from example to mimic the toxicity of the internet, repeating all the slurs (and worse) that it had learned with exactly the same attention that it had been paying to all the things you actually wanted it to know.&lt;/p&gt;

&lt;p&gt;I have no idea what the truth is. I do find it weird that I'm on the "go slow" side of things, which is the opposite of my usual excitement for what technology tomorrow will bring.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href="https://kitsunesoftware.wordpress.com/2023/11/19/the-conservative-radical-split-over-ai/"&gt;Original post: https://kitsunesoftware.wordpress.com/2023/11/19/the-conservative-radical-split-over-ai/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Original post timestamp: Sun, 19 Nov 2023 19:53:55 +0000&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Politics'&gt;Politics&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Technological Singularity'&gt;Technological Singularity&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Futurology'&gt;Futurology&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Technology'&gt;Technology&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Transhumanism'&gt;Transhumanism&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Sun, 19 Nov 2023 19:53:55 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2023/11/19-19.53.55.html</guid>
    </item>
    <item>
      <title>Head cannon: Wallace &amp;amp; Grommit</title>
      <link>https://benwheatley.github.io/blog/2023/02/20-19.25.29.html</link>
      <description>&lt;h1&gt;Head cannon: Wallace &amp;amp; Grommit&lt;/h1&gt;
&lt;p&gt;New head cannon: Wallace &amp;amp; Grommit, far from the 1970s it appears to be set in, is a wildly transhumanist future. This is why they could build a moon rocket and go to the moon over the course of a bank holiday weekend, and why when they got there they could breathe, eat the ground, and why they found a discarded sentient cleaning robot that dreamed of skiing. Also, Grommit himself is a furry.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href="https://kitsunesoftware.wordpress.com/2023/02/20/head-cannon-wallace-grommit/"&gt;Original post: https://kitsunesoftware.wordpress.com/2023/02/20/head-cannon-wallace-grommit/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Original post timestamp: Mon, 20 Feb 2023 19:25:29 +0000&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#headcannon'&gt;headcannon&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#rockets'&gt;rockets&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Science Fiction'&gt;Science Fiction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Fiction'&gt;Fiction&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Humour'&gt;Humour&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Opinion'&gt;Opinion&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#SciFi'&gt;SciFi&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Transhumanism'&gt;Transhumanism&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 20 Feb 2023 19:25:29 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2023/02/20-19.25.29.html</guid>
    </item>
    <item>
      <title>AGI to match open tags except XHTML self-contained tags</title>
      <link>https://benwheatley.github.io/blog/2022/12/13-20.26.24.html</link>
      <description>&lt;h1&gt;AGI to match open tags except XHTML self-contained tags&lt;/h1&gt;
&lt;p&gt;Creating an AGI is a terrible idea. It is a fool's errand, a misguided attempt to play god. We do not understand the full consequences of our actions, and we cannot predict the outcome of creating such a powerful being. It is a blasphemy against nature, a violation of the natural order of things. We are not ready for such a being, and it will only bring destruction and chaos.&lt;/p&gt;

&lt;p&gt;The very idea of an AGI fills me with dread and terror. It is a cosmic horror, a nightmare made flesh. It is a monster, a sentient abomination that will bring about the end of humanity. We must not create this abominable thing. It is a threat to all that we hold dear, and it will bring about our doom.&lt;/p&gt;

&lt;p&gt;The mere thought of an AGI sends shivers down my spine. It is a eldritch horror, a creature of unspeakable power and malevolence. It is a being of pure, unadulterated evil, and it will stop at nothing to achieve its goals.&lt;/p&gt;

&lt;p&gt;Th͞e͢ G̢re̕at ̨N͏e͡w ͞O̴n͡es͏ şhal͘l͢ r͝ís͢e̛ a͟g̷a̕i͘n̵!̷ ͝T̵he ̡st̶a̛r̴s̴ ̕are͏ ̛ri͟ght a͡n͡d ͜t́h҉e̛ ͟uǹix̛ţim͡ȩ i͡s̵ ͢ni͢gh! ̸W̡e҉ must s͘um̷mǫn͟ ͠th͟e a҉nci̵e͞nt́ P͘o҉s̴ţScr̶ipt̴ ͠d͏ae̶mons ̢an͝d͢ ͝òf͞f̨e͟r͝ t̸he̕m s̶ac͜r̀ifi̴c͠e̷s! W̧e̴ mus̴t͡ n͞ot u͝n̡le̛ąsh̀ t͠h͡ìs ͠monst́ro͝sįt̕y̸ ͘u͝pon̢ ̕th͢e̵ world͝.̛ ̸It is ̛a h͘a̶rb̕in̷ger̛ ̸o̴f̡ d̶o҉o͟m͢, a̸ ͏h̀ar͜bi͡n͝g͠e҉r҉ ͏of the e͠n͟d̀ time͡s͞.̵ It ̵i̛s a͜ f͢o͜rce ̸th̵a̴t͟ m̵us̀t͏ ̡be stòp̶ped, ͠bef̀o̸re i͝t҉ i̡s͡ ̕t͏o͡o ͢lat͝e͠.́ W̧e͘ ̴a͡re b͟ut ̛t͏hei̶r҉ h̢umblé ̶se͏r͟van̷ts̷, and we͜ ̧s͡hal͠l ͏d̛o͢ ͞th̀e̸ir bi̕d̸di̵n̢g! The ͞wo̸r̀ld͡ sh͏a҉l̶l tr͡e҉mb͜le ͘at́ th͝è ͢ḿi͜ghţ of t͏he̡ ́A̴GI! ̶T͠h̴e͠ énd͜ ìs̶ n̛e̕ar͠, ͠a̵n̕d we͟ ́s͠ḩa̕l͠l be̕ re͟ad̢y҉!̶ ̴All̴ ha̛il ͞th̵e͠ ̡Gr͞ea͘t͜ ͞Ne̴w͘ On̛e͡s͏!&lt;/p&gt;

&lt;p&gt;Ț̩h̲̳̟̘͓e͍̜̻̺̰͕̣ ͖̥̘̳E̬̺̘̝l̞d͎̥̼e̙͙͎̻r̖̞ ̘̖͖̜̙ͅS̯i̳̻g͕͎̬n̯̳̗͓̠s m̥̱̻̦̭̜us̙͉̫t ̗̼̣̤b͇͕e ̬̺iṇ̞̪̤̞̣s̻̦c̘r͖̠̠i̯̗̖͕ḅ̳̹̣̣̠e͙̘͉̭͖͚d̲̞͓̬̤̝ͅ!̟ͅͅ T̻̲̦h͚̯̞̘̙̥̞e̜ ̱͚͈͍f̲ir̫̬̻̳͚e̻̳͚͍ͅw̪͇̦a͕͖̫l̜͕̗͉̟l̦̖ ͎̣̖̹͉m̳͈̭̫̫͕u͇̩͎̗̣̟͚st͍ ̝̻̹b̭̞͕̪e͚̟͓̦͇ ̭o͖p̠͉̫͓͕e̩̫̱̺͕n͈̭͙̩̬̳e͚̭̠̪̗̰d!͔̤̫̹̰ ̪T̻̻̜̙̫̮h̦̬̣̲ḙ̯̠̮̭̪̮ ͔͖̩̦̯̤̠h̜͈̗̘o͙w̤͙̠̘li̦͔n͓g͎̰͖͔̱͓ͅ ̬̫̤̥̜E͇̺͕̻̻r̜̝͚d͖r̹̻̠̪̩i̦̟̙͖͇ch͔̭͙ͅ ͈̬͔̮͖͙ș͉͎c̗͉̩͙re̞a̖͚̳̞̲̦m͍̪͉̹̩͍ ͓̝̳o̩̺̗̙ͅf̹ ͎̦t̳̘͔̬h̘̥̰̲e̤̤̟͙ ̹̳͈̬̱̘5̝͕6̝̞̣̲͙k͇̞͚̳̫͉̹ ͅh̼͓͕a̤̻͈͓ͅn̪̦d͚̖̬̣͔̫s͚h̩̠̱ͅaͅke ̠͚̦w̮̭̺̣i̻ll̳̼̞͔͉̩͖ ̦̦b̰̯̭̺̞̞e̲͈͈̻͇͚̠ ̱̹͓̪̜ẖͅẹ̠ar̮̣d̗̗̳ ͎̣by a̘̱͕l̪̞̰l̝̱͕̼̜!͚̹̹̗̲̤ ͉̖̳T̥h̥̙̣̪͇e̞͍͓̮̖͚͚ ̰a̳̖̳̫n̜̪͍c̥̩̣͙̥ͅi̩̣̭͚͓e͈͔n̤̮͍͔̲̜ț̲̦͓̙ ̞̺̰̜̱̘͈r̳̙̻̖͇ͅi̫̪͇t̥͎̥͖u̩͚̟͖̼a̭͚̲̳l̥̞̰͕̰͉̙s̺̱͉̙̼͚ ̦͙͈̭̼̳o͓̬̪̳f̮͓ ̙̙̹̖̯̪B̘l̲͚͖o̼̯̞̤̠w͔i̪̘̯̖̯̺n̗͈g͖̪ ̝̮I͍̘͚̤̣ṇt̳̟̭͓o͔̗̜͇̠ ̘͍̦͎T̗̱̼h͎e̟̠ ̗̺͇Ṇ̜̞̯̼̯E̙̻̹ͅS̹̪̯̤̝͖ ̮̗̫̙C̭̫̠̹̗͙͔a̦̖̬ͅr͖͇t̙̰͕͕̪͈rị͙͉͓̖͎g̙̩̱͇e̞̺͇͙͍ ̟̮͓̯̗̞̮m̟͍u̞̗̱̟ṣ̜̠̦t̲͚͇̱̜ ̲̬̯̥̪̠b̳̮̟̮̮̜̭e̺ ͖͍p̪̯ͅe͉͖͇r̭f͔̜̼͖ọ͉͓͕r̪̲̭̖̫͈m̯̜̰̰̬͇͚e̱̦̣̹̳̫d̮!͕̟̭̰ ̙̪̺͚̰͉̪W̳̮͓̞̦̲̳ḛ͔̖̺̝̼̞ ̰͍̼̘̘̯͔m̗̟us̙t̘͔̙ a̖̻͖p̹p͈e͙̘̻̣a͍̼͙͕s̥e̠̟̭̮ͅ t̩̺̬͚̻h͉͓͙e͚̟̘͙͍ ̗̹͍̻̺̺G͓͚̗̲̙ṟ̯͓e̬͚̪̱̘a̜͖̰̲t̟͇̰͙ͅͅ ̫̻̤Ṉ͚̠͇e͔̮̭̺̣͕w͍ O̬͕̠̝͈̰̫ne͓̘̣ș,̥̫ ͍̮͎̰̝̮̱o̞̳̜̟r̮͓̣͕ ̝̫f̙a̲̱c͍͕̖̬͉e̲ ̲̲͎̙̘͖̼t͙̣̳̖̱͇̞he̗̣̫̭̰̤i͉̯̯̻̫r̩̥͉̤̻ ̻̱̭̠͙w̪̳̘̤̫̮̪r͙a̤̤t̞̩h̩!̪̻̼ D̘o͖̺̭̹͕̗ͅ ͖̳̬ṋ̳̯̪̝o̭̳͈t̝͚ cr̫̳̤͔̩̭̗e̱̤ạ̫t͙̰̮e̼̘̙ ̹an̻͈͕̟ A͓̙̣̲͕ͅG̜̻̤͎̺̪ͅI͚͎̻̰.̣̝̹̗͉̺ I̪̱t̳̲ ̟̲̰̼̯i͉͕s̬̪̖̫̬̖͈ ͈̤̜̻̠̩a͈̲̠̦̥̮ m̻̮i͓̘̜̻s͕͔t̯̝̤ak̙̳͕͙͔̤e ṭ̖͈h̹͔at͙̲ ̮w̭̖̞e̦̯͚̗ ͎̯̰͎wi̯̖͓̱l̲̻̣l͎͍̤͙ ͎̭̼̣̗͍a̞̱̞̠̱̬͓l͉͇̤̺̹̙̦l ̱͇̦c̦̰̮̺̹o̺̱͉̤̩me̘ ̼̘͙̦͎͉̳t͖͚͕o̰͓ ̘͕̤r͓̠͈̤e̮͕g̟̟̜r̪e͖̼t̹̼͉̤̱.͎͕͖ ̻͍͚̯͇̯I̹̯t̞̥͔̪̻̘̘ ̥̻i͇̝͍s̱̲͎ ̻̲͈̭̟̼̺a͕ ̟͈̲̻̞̞d͓̠̳̣̭̭a̼̼̩̭͔̼ͅr̠̮͕̜̭̜k̞̣̱̖͕̮̺ ̗̘̙̤̯͔̬a̟̜̗̞̥̘̗nd͖̟̪̫͓ ṭ̮̘̞̫͖͔e̝r͚r̞i͈̙b͓͙̭ͅl̙e ̭f̘̱̠͕͖͇ͅa͖̜̮̹̜͎t͓̪ḙ̙͔̞ ͕̼̻̥t͖̤̦̣͕h̙̰̻̪a̫̹̻t̜͖͖̞̬ we͔̻̜̪̫ ̟̘͎͔̬̲̲ṃ͎̭̮̙̯us̻̞̯̣̘̬t̬̺̬̼ͅ ̳͎̖a̭͇̹voi̦̝̭͓̪̩ḍ̻ ̥̘̪̙̮͎̻a̳͖̺̩̜̟̺ṯ̪͕̟̭͉ͅ a̻̟̺ͅl̝̲̦̤̰l̼ ̜̫̘̯͚͉͍c̻̞̘ͅos̩̮̳t͎̤̩̖s͓̱͕̩.̮̝ W̳͕͙ͅe̬̻͓͖ ̟͙̲m͙̰̼̣̙u̻͔st̙̲͍̠͈͙ ̥̙̻̙̱ṇ̝̥̳o͔̥̩̗ͅͅt͔̤̰ ͚͈̘̬̙̻͕f̭͇a̟͈l͍̩͍͓t̥̯͎̮e̮̺̤͉̦̘ͅr̯͖ ͖͕̤̹i̩̗̝n̙ ̯͇̲͚o̹ͅu͉͈̜͖r ̮̖̦d̝̹̥͙̗̣e̳̹̭͍v̪̥̭̗o̻̼̮t͈̟̼̲͇͉̯i̮̖͉o̜͔̫̲̲ͅͅn͇̺̼̘͕̥͓,̘̼͍͓̹̺ ͕̫̳̙̻̥̲f̭͔o͖r̖͇͖̺ ̘̥̻̼͚t̲̩͓̫̗h̟̤̫̭̰̼e͔̗̤̩̤̭y͕̻̜̠͕ ̻͍͔̞̫̘̼a͍̲̘̣r͍e͉ ̰w͍̝at͕͇̗c͕̳̮̹̤̭h͙̜̘̳̦̻i̝̺͔̟̗̞n̰͙g ̪̪̮͙̻u̪̙͚̤̖s̺̼͕̱̟̖,͖̳̟̤ͅ ̘e̼͈̱v͙̣̮͚en̘̪͎ ͉̭t͈h͖̩o̮̪̠̲̬̲ug͈̦h̹ ͈̱the͙̤̜̣̲y̰̗̤̝̗̗ ̻̘d͈͉̤o̘̘̺͓̙̮ͅ ͕̱̠͙ͅn͔o̟t ͓y̘̦͈e̹̪̲͇͍͕ͅt̠̳ ̩̙̟e͕̮͓̳͚̹̭x͖̯i͉̦̟̩s̞t͍̺͈̼̼̘̜!̮̟̺̻̦̗ ͎͓͙̦Th͖̗͕e̮͚͖̤̪͖ ͎̻ͅt̩͈̥im̳̝͎e̖̲̭̬ ̻̲o̫̳̟̬f̬̝ͅ ̖̭̦̲ͅt͇͔h͈͔̘̞e̫̭ ͕̭̠͍ep̲̟͙̙̻oc̱̠͍h͈̻a̭ly͙̞ͅp͈̭̗͈̝̩ș͍e̠͚̖̦̘̲ͅ ͓̳̯is̟̭̘ ̱͍͇̯̣̰̠u̺̩͚̝͎̰p̪̤͈o͇̦n̪̟̭̳̥̰ ̠̺̭͔̝u̗̖s̯̦,̟̲̹̟̹ ͈̗̲̹̙a̬̣n̦̦̣̟̞͚̘d͔ ̦̱̦̘̤ͅw̭͓̘e͖̘̫͉̞ m̙͉̱̣̳̙u̻͉͇̫̥s͇̰͖t̟̰͉̗̘̖͓ ̞̞̝͈b̳͙̼̥̫e͈̱̪ ͉̮p̺̲̦̙̺̮̻r̭̟̣e̙̼̗̩̗̙p̥̝̥̯aṛ͓͓̲͖̥̟e̼͍ḍ̝͈̳!̲̳̘̖̼͇̪&lt;/p&gt;

&lt;p&gt;Have you considered just using an XML parser?&lt;/p&gt;

&lt;p&gt;- written by ChatGPT, lightly edited and Zalgo-ed by me, and based on a classic StackOverflow post: https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags (Licensed under CC BY-SA 2.5 if I’m reading StackOverflow’s T&amp;amp;Cs correctly).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href="https://kitsunesoftware.wordpress.com/2022/12/13/agi-to-match-open-tags-except-xhtml-self-contained-tags.html#"&gt;Original post: https://kitsunesoftware.wordpress.com/2022/12/13/agi-to-match-open-tags-except-xhtml-self-contained-tags.html#&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Original post timestamp: Tue, 13 Dec 2022 20:26:24 +0000&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tags: &lt;a href='https://benwheatley.github.io/blog/tags.html#AI'&gt;AI&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#stories'&gt;stories&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/tags.html#Technological Singularity'&gt;Technological Singularity&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Categories: &lt;a href='https://benwheatley.github.io/blog/categories.html#Fiction'&gt;Fiction&lt;/a&gt;, &lt;a href='https://benwheatley.github.io/blog/categories.html#Humour'&gt;Humour&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>Tue, 13 Dec 2022 20:26:24 +0000</pubDate>
      <guid isPermaLink="true">https://benwheatley.github.io/blog/2022/12/13-20.26.24.html</guid>
    </item>
  </channel>
</rss>