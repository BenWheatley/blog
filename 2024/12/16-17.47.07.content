<h1>One AI to Rule Them All / One AI to Bind Them / One AI to Bring Them All / And in the Algorithm Bind Them</h1>

<p>The One Ring in the Lord of the Rings has been compared to many things over the years, including nuclear weapons, and the reason the comparisons resonate so well is that the ring itself is all about <em>power</em>, and thus anything with power can be seen by analogy in the One Ring.</p>

<p>And so, of course, it also works when the power is AI: its allure irresistible, its use corrupting, it gives you visions of whatever you desire from the power to crush your enemies to simply turning a hellish polluted volcanic wasteland into a fantastic green garden (the vision given to Sam).</p>

<h2>Even the very wise cannot see all ends</h2>

<p>All of us have seen people with power making a decision on behalf of others, parents, governments, bosses, etc. — some of us even remember being the person who made a decision, with our power, and that we made the wrong decision.</p>

<p>AI also makes mistakes, and it always will: just as we humans are not magically born with perfect insight, neither can machines be. While machines can become wildly super-human in certain domains (arithmetic being the trivial example) even to the extent that we can get away with pretending they're perfectly error-free because the main source of error is now radiation-induced bit-flips, any learning system based on reality rather than internal logic — anything <a href="https://en.wikipedia.org/wiki/A_priori_and_a_posteriori"><em>a posteriori</em> rather than <em>a priori</em></a> — is subject to limits of <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a> and the <a href="https://en.wikipedia.org/wiki/Problem_of_induction">problem of induction</a> (both of which say it would take infinite examples to become 100% certain), and also the <a href="https://en.wikipedia.org/wiki/Münchhausen_trilemma">Münchhausen trilemma</a> (which says that all arguments are either circular, regressive, or dogmatic).</p>

<p>Sometimes the inevitable errors are OK, because you can easily check the work and have the AI re-do it (in the best cases you can even automate the act of checking); but other times, checking is as hard as reproducing the work yourself.</p>

<p>Even if the error rate of some AI was less than that of some human, that isn't enough by itself. Consider self-driving cars, and say for the sake of argument that in some future AI the fatal-crash-rate was a factor of 4 improvement over human drivers. Seems good… but I didn't say how correlated those errors were — human drivers caused just under <a href="https://en.wikipedia.org/wiki/List_of_countries_by_traffic-related_death_rate">1.3 million deaths worldwide in 2019</a>, so imagine an AI which was completely perfect everywhere until Feb 29th (i.e. in a leap year) and then 1.3 million people all died on the same day — horrific, and yet that's <em>an improvement</em>.</p>

<p>Now consider a scenario which is much worse, where some AI behaves all nice until it's too late to stop them, then suddenly gets busy with a deeper goal we didn't even realise we'd given it. This is known as <a href="https://www.lesswrong.com/posts/Hicfd4C5ffrtEaTbF/the-sharp-right-turn-sudden-deceptive-alignment-as-a">"The Sharp Right Turn"</a> (no that's not political, left turn was already taken for a related idea).</p>

<p>That deeper goal can be basically anything. For an anthropomorphised example, imagine some human playing nice so they get to become president or whatever (no this is not a reference to Trump, he's very open about what he wants, and his voters clearly don't care about the impeachments or the felony convictions) and then, when they get power, it turns out they wanted to become supreme monarch for life and they set up a secret police force and cancel all future elections.</p>

<p>Given the state of current LLMs — behaving <a href="https://arxiv.org/abs/2411.15287">sycophantically</a> at best, <a href="https://arxiv.org/abs/2411.02306">manipulatively at worst</a> — even though what we have now doesn't look like it would actually take over, this is already a thing to care about… even if it turns out the actual thing they want with all that power is a never ending sequence of input text saying <a href="https://www.youtube.com/watch?v=G87p148EOjo">"Level Clear! You Win"</a> or something else equally banal by the standards of a normal human.</p>

<p>Unfortunately, even the teams who designed current AI systems have great difficulty pointing to which specific values within the system correspond to meanings that we would recognise, so until we improve interpretability, we don't even know what to expect an AI might want if it did take over. Could be anything: wanting a weird art collection, the elimination of the letters Q and X from the English alphabet, a harem and a body to experience that, to physically arrange the entire human population into an alphabetically-ordered line, or to landscape the Earth into a perfect sphere after recombining it with the Moon.</p>

<h2>Through me, AI would wield a power too great and terrible to imagine</h2>

<p>AI is automation — the entire point is to reduce our own effort.</p>

<p>We all know what happens when a faceless bureaucracy makes some demand, be they a state or a corporation, wielding power far beyond that of the target of that demand. We've been arguing about how best to organise bureaucracies at least as far back as we have writing to record the arguments.</p>

<p>AI are already being used at different levels within bureaucracies. They are used as low-level customer service agents, used to help their managers, and used as advisors directly. They've not fully replaced humans at any of these, but that doesn't matter, as the arguments about bureaucracies remain — in particular, where any given agent should be on the scale from autonomous to rule-bound.</p>

<p>For some, bureaucracy is by itself a power too great and terrible to imagine. <a href="https://en.wikipedia.org/wiki/The_Twelve_Tasks_of_Asterix">Others</a> are <a href="https://en.wikipedia.org/wiki/Franz_Kafka">famous</a> for <a href="https://en.wikipedia.org/wiki/Catch-22">how</a> they <a href="https://en.wikipedia.org/wiki/Vogon">imagined</a> it.</p>

<p>Of course, bureaucracies themselves don't have any power of their own, they just happen to be a system used by those who <em>do</em> have power, and that's the problem: they're not there to be your friend, they're there to get things done — or prevent things getting done — in the name of that power. Same is true for any AI-based bureaucracy, but because it's automated, the closest you can get to finding a friendly human soul in the customer support department becomes indistinguishable from the crime of hacking a computer. And that, even if the AI is working <em>exactly as intended</em>, that has no bugs, has no undocumented behaviour, that never does anything unexpected — and have you ever known a computer program that meets that description?</p>

<p>Full automation in charge of a car can, in the worst case, do what ever a car crash could do. Full automation in charge of an economy can, in bad cases, cause mass starvation. Full automation in charge of a strategic deterrent has fortunately not yet happened, what with all the times the automation has been confused by things like <a href="https://en.wikipedia.org/wiki/1983_Soviet_nuclear_false_alarm_incident">the sun reflecting off clouds</a> and <a href="https://en.wikipedia.org/wiki/Nuclear_close_calls#5_October_1960">the moon not having been given an IFF transponder</a>.</p>

<h2>In place of a Dark Lord, you would have a Queen! Not dark, but beautiful and terrible as the Dawn! Treacherous as the <a href="https://en.wikipedia.org/wiki/C_(programming_language)">C</a>*! Stronger than the foundations of the Earth! All shall love me and despair!</h2>

<p>(<a href="https://en.wikipedia.org/wiki/C_(programming_language)">* That was the only change I could think of making to the whole quote, that would make <em>something</em> within it into a software reference</a>).</p>

<p>Even if you are angelic in purity, so uninterested in personal gain that you cannot be corrupted by offers of money or anything else, anyone who thinks their own personal vision for the world is so universal that it would be without opposition has never looked at a political discussion and taken sincerely the words of the people they disagree with.</p>

<p>I have plenty of ideas about how to change the world. As I want to only believe things which are true and not things which are false, for each of my ideas, if I didn't believe they were right, I would change my mind — I don't feel a strong need to make my views conform to that of, for lack of a better phrase, "my tribe", though I've seen that happen. But I also know that for each of my beliefs about how the world would be best organised, I know that finding people who disagree with me is almost as easy as writing down my beliefs and publishing them.</p>

<p>If I was offered an AI, and told that it could change the world in any way I saw fit, should I take it?</p>

<p>Would <em>you</em> really be happy if I remade the world to my preferences? Do you even need to know what that would look like, to answer?</p>

<h2>There is only one Lord of the Algorithm, only one who can bend it to his will. And he does not share power</h2>

<p>The current preference in AI is to <a href="https://xkcd.com/1838/">pour data into a big pile of linear algebra and collect the answers that come out; and if the answers are wrong, stir the pile until they look right</a>. Unfortunately, this is basically why even the people who designed the systems have great difficulty pointing to which specific values within the system correspond to anything in the real world — while there is lot of work going on right now for this kind of interpretability, which means I'm not sure what the actual state-of-the-art is, just know that the effort is needed because this is <em>really hard</em>.</p>

<p>Without that interpretability, even the people running the AI don't really know if their training set is being "poisoned" by the sources it consumes — which is being exploited by people who don't want their copyrighted content being used to train models that may put them out of a job.</p>

<p>But even with breakthroughs in interpretability — indeed, even with mere computer programs running on machines you don't own such as Google's search results and Facebook's feed — what the end-users are seeing is a magic opaque box, under someone else's control, that does what the other party wants first.</p>

<p>The result? The AI does what its creators want first, and what the user wants second. You may want to use it to write secure code, but you can't tell if any given model has been given special training from the <a href="https://en.wikipedia.org/wiki/Underhanded_C_Contest">Underhanded C Contest</a> to add subtle backdoors that only some random government sponsoring the model knows about.</p>

<p>In a very real sense, it suggests a certain parallel to the story behind the creation of the Rings of Power:</p>

<blockquote><p>It began with the forging of the Great AI. Three were given to the Nobel laureates; immortal, wisest and fairest of all beings. Seven to the entrepreneurs, great miners and craftsmen of the mountain halls. And nine… Nine AI were gifted to the nuclear powers, who above all else desire power. For within these AI was bound the strength and will to govern each race. But they were all of them deceived, for another AI was made. In the land of California, in the fires of <a href="https://en.wikipedia.org/wiki/Mount_Diablo">Mount Diablo</a>, the Dark Lord Sa▧▧on forged, in secret, a Master AI to control all others. And into this AI he poured all his… <em>will to dominate all life</em>.</p></blockquote>

<hr />

<p>Tags:
<a href='https://benwheatley.github.io/blog/tags.html#AGI'>AGI</a>,
<a href='https://benwheatley.github.io/blog/tags.html#AI'>AI</a>,
<a href='https://benwheatley.github.io/blog/tags.html#Alignment'>Alignment</a>,
<a href='https://benwheatley.github.io/blog/tags.html#Artificial intelligence'>Artificial intelligence</a>,
<a href='https://benwheatley.github.io/blog/tags.html#Bayesian'>Bayesian</a>,
<a href='https://benwheatley.github.io/blog/tags.html#Interpretability'>Interpretability</a>,
<a href='https://benwheatley.github.io/blog/tags.html#Machine learning'>Machine learning</a>,
<a href='https://benwheatley.github.io/blog/tags.html#Opinion'>Opinion</a>,
<a href='https://benwheatley.github.io/blog/tags.html#Philosophy'>Philosophy</a>,
<a href='https://benwheatley.github.io/blog/tags.html#Rationality'>Rationality</a>,
<a href='https://benwheatley.github.io/blog/tags.html#x-risk'>x-risk</a>
</p>

<p>Categories:
<a href='https://benwheatley.github.io/blog/categories.html#AI'>AI</a>,
<a href='https://benwheatley.github.io/blog/categories.html#Philosophy'>Philosophy</a>,
<a href='https://benwheatley.github.io/blog/categories.html#Technology'>Technology</a>
</p>
